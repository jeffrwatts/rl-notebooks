{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import dqn_agent\n",
    "#from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v0'  # @param\n",
    "num_iterations = 20000  # @param\n",
    "\n",
    "initial_collect_steps = 1000  # @param\n",
    "collect_steps_per_iteration = 1  # @param\n",
    "replay_buffer_capacity = 100000  # @param\n",
    "\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "batch_size = 64  # @param\n",
    "learning_rate = 1e-3  # @param\n",
    "log_interval = 200  # @param\n",
    "\n",
    "num_eval_episodes = 10  # @param\n",
    "eval_interval = 1000  # @param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = suite_gym.load(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "BoundedArraySpec(shape=(4,), dtype=dtype('float32'), name=None, minimum=[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], maximum=[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38])\n",
      "Action Spec:\n",
      "BoundedArraySpec(shape=(), dtype=dtype('int64'), name=None, minimum=0, maximum=1)\n"
     ]
    }
   ],
   "source": [
    "print('Observation Spec:')\n",
    "print(env.time_step_spec().observation)\n",
    "print('Action Spec:')\n",
    "print(env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_net = q_network.QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collect policy is e-greedy\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.compat.v2.Variable(0)\n",
    "\n",
    "tf_agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=dqn_agent.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = tf_agent.policy\n",
    "collect_policy = tf_agent.collect_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=tf_agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    replay_buffer.add_batch(traj)\n",
    "\n",
    "for _ in range(initial_collect_steps):\n",
    "    collect_step(train_env, random_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, sample_batch_size=batch_size, num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0708 09:04:39.997025 4561159616 deprecation.py:323] From /Users/jwatts/TensorflowProjects/lib/python3.7/site-packages/tf_agents/policies/epsilon_greedy_policy.py:96: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "td_errors_loss_fn=<function element_wise_squared_loss at 0x141512ea0>\n",
      "_n_step_update=1\n",
      "no state spec, map structure\n",
      "multi_dim_actions=False\n",
      "reward_scale_factor=1.0; gamma=1.0\n",
      "td_errors_loss_fn=<function element_wise_squared_loss at 0x141512ea0>\n",
      "_n_step_update=1\n",
      "no state spec, map structure\n",
      "multi_dim_actions=False\n",
      "reward_scale_factor=1.0; gamma=1.0\n",
      "step = 200: loss = 12.647035598754883\n",
      "step = 400: loss = 31.06681251525879\n",
      "step = 600: loss = 17.79585838317871\n",
      "step = 800: loss = 29.510196685791016\n",
      "step = 1000: loss = 71.55635833740234\n",
      "step = 1000: Average Return = 44.70000076293945\n",
      "step = 1200: loss = 34.57028579711914\n",
      "step = 1400: loss = 29.073408126831055\n",
      "step = 1600: loss = 57.44021224975586\n",
      "step = 1800: loss = 19.16401481628418\n",
      "step = 2000: loss = 67.837646484375\n",
      "step = 2000: Average Return = 26.600000381469727\n",
      "step = 2200: loss = 21.267086029052734\n",
      "step = 2400: loss = 60.337196350097656\n",
      "step = 2600: loss = 55.717159271240234\n",
      "step = 2800: loss = 18.117290496826172\n",
      "step = 3000: loss = 51.93886184692383\n",
      "step = 3000: Average Return = 54.900001525878906\n",
      "step = 3200: loss = 18.545806884765625\n",
      "step = 3400: loss = 8.119680404663086\n",
      "step = 3600: loss = 12.565216064453125\n",
      "step = 3800: loss = 7.895153522491455\n",
      "step = 4000: loss = 13.146234512329102\n",
      "step = 4000: Average Return = 31.100000381469727\n",
      "step = 4200: loss = 12.886043548583984\n",
      "step = 4400: loss = 102.09980773925781\n",
      "step = 4600: loss = 23.4152889251709\n",
      "step = 4800: loss = 132.20497131347656\n",
      "step = 5000: loss = 11.407313346862793\n",
      "step = 5000: Average Return = 124.30000305175781\n",
      "step = 5200: loss = 128.555419921875\n",
      "step = 5400: loss = 158.9695281982422\n",
      "step = 5600: loss = 4.897219181060791\n",
      "step = 5800: loss = 6.436513900756836\n",
      "step = 6000: loss = 43.385074615478516\n",
      "step = 6000: Average Return = 163.8000030517578\n",
      "step = 6200: loss = 6.712560653686523\n",
      "step = 6400: loss = 44.70085144042969\n",
      "step = 6600: loss = 22.090898513793945\n",
      "step = 6800: loss = 26.361257553100586\n",
      "step = 7000: loss = 161.6080322265625\n",
      "step = 7000: Average Return = 155.0\n",
      "step = 7200: loss = 11.085664749145508\n",
      "step = 7400: loss = 34.85843276977539\n",
      "step = 7600: loss = 125.97148895263672\n",
      "step = 7800: loss = 146.18902587890625\n",
      "step = 8000: loss = 5.495702743530273\n",
      "step = 8000: Average Return = 147.39999389648438\n",
      "step = 8200: loss = 25.457468032836914\n",
      "step = 8400: loss = 261.4249267578125\n",
      "step = 8600: loss = 197.98165893554688\n",
      "step = 8800: loss = 242.55661010742188\n",
      "step = 9000: loss = 11.288138389587402\n",
      "step = 9000: Average Return = 125.69999694824219\n",
      "step = 9200: loss = 12.017148971557617\n",
      "step = 9400: loss = 37.672027587890625\n",
      "step = 9600: loss = 122.1511459350586\n",
      "step = 9800: loss = 43.30611801147461\n",
      "step = 10000: loss = 250.2428741455078\n",
      "step = 10000: Average Return = 166.10000610351562\n",
      "step = 10200: loss = 205.5945281982422\n",
      "step = 10400: loss = 81.44689178466797\n",
      "step = 10600: loss = 9.910019874572754\n",
      "step = 10800: loss = 42.14085388183594\n",
      "step = 11000: loss = 5.356292724609375\n",
      "step = 11000: Average Return = 184.5\n",
      "step = 11200: loss = 62.09098434448242\n",
      "step = 11400: loss = 289.0035095214844\n",
      "step = 11600: loss = 277.79595947265625\n",
      "step = 11800: loss = 16.756237030029297\n",
      "step = 12000: loss = 392.76324462890625\n",
      "step = 12000: Average Return = 197.6999969482422\n",
      "step = 12200: loss = 137.08926391601562\n",
      "step = 12400: loss = 12.889716148376465\n",
      "step = 12600: loss = 8.651581764221191\n",
      "step = 12800: loss = 60.036991119384766\n",
      "step = 13000: loss = 14.287984848022461\n",
      "step = 13000: Average Return = 176.0\n",
      "step = 13200: loss = 7.969167709350586\n",
      "step = 13400: loss = 21.49718475341797\n",
      "step = 13600: loss = 25.41521453857422\n",
      "step = 13800: loss = 681.6915283203125\n",
      "step = 14000: loss = 163.87078857421875\n",
      "step = 14000: Average Return = 200.0\n",
      "step = 14200: loss = 12.524378776550293\n",
      "step = 14400: loss = 67.6680908203125\n",
      "step = 14600: loss = 383.69207763671875\n",
      "step = 14800: loss = 7.6310906410217285\n",
      "step = 15000: loss = 595.8534545898438\n",
      "step = 15000: Average Return = 199.10000610351562\n",
      "step = 15200: loss = 8.978577613830566\n",
      "step = 15400: loss = 16.664813995361328\n",
      "step = 15600: loss = 208.9370880126953\n",
      "step = 15800: loss = 79.09851837158203\n",
      "step = 16000: loss = 84.433349609375\n",
      "step = 16000: Average Return = 198.8000030517578\n",
      "step = 16200: loss = 126.70572662353516\n",
      "step = 16400: loss = 15.677742004394531\n",
      "step = 16600: loss = 498.68182373046875\n",
      "step = 16800: loss = 16.367473602294922\n",
      "step = 17000: loss = 101.64053344726562\n",
      "step = 17000: Average Return = 200.0\n",
      "step = 17200: loss = 19.692203521728516\n",
      "step = 17400: loss = 555.4901733398438\n",
      "step = 17600: loss = 10.10008430480957\n",
      "step = 17800: loss = 394.9402770996094\n",
      "step = 18000: loss = 193.22096252441406\n",
      "step = 18000: Average Return = 200.0\n",
      "step = 18200: loss = 33.82866668701172\n",
      "step = 18400: loss = 11.78503704071045\n",
      "step = 18600: loss = 28.143360137939453\n",
      "step = 18800: loss = 18.180519104003906\n",
      "step = 19000: loss = 157.24539184570312\n",
      "step = 19000: Average Return = 200.0\n",
      "step = 19200: loss = 101.61576080322266\n",
      "step = 19400: loss = 851.7750854492188\n",
      "step = 19600: loss = 258.4828186035156\n",
      "step = 19800: loss = 84.20079040527344\n",
      "step = 20000: loss = 10.83415412902832\n",
      "step = 20000: Average Return = 200.0\n"
     ]
    }
   ],
   "source": [
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "tf_agent.train = common.function(tf_agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "tf_agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    for _ in range(collect_steps_per_iteration):\n",
    "        collect_step(train_env, tf_agent.collect_policy)\n",
    "\n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = tf_agent.train(experience)\n",
    "\n",
    "    step = tf_agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss.loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17.930000400543214, 250)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(steps, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Step')\n",
    "plt.ylim(top=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = eval_env.reset()\n",
    "while not time_step.is_last():\n",
    "    action_step = tf_agent.policy.action(time_step)\n",
    "    time_step = eval_env.step(action_step.action)\n",
    "    eval_py_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _experience_to_transitions(experience):\n",
    "    transitions = trajectory.to_transition(experience)\n",
    "\n",
    "    transitions = tf.nest.map_structure(lambda x: tf.squeeze(x, [1]), transitions)\n",
    "\n",
    "    time_steps, policy_steps, next_time_steps = transitions\n",
    "    actions = policy_steps.action\n",
    "    return time_steps, actions, next_time_steps\n",
    "\n",
    "def element_wise_squared_loss(x, y):\n",
    "    return tf.compat.v1.losses.mean_squared_error(x, y, reduction=tf.compat.v1.losses.Reduction.NONE)\n",
    "\n",
    "def element_wise_huber_loss(x, y):\n",
    "    return tf.compat.v1.losses.huber_loss(x, y, reduction=tf.compat.v1.losses.Reduction.NONE)\n",
    "\n",
    "def compute_td_targets(next_q_values, rewards, discounts):\n",
    "    return tf.stop_gradient(rewards + discounts * next_q_values)\n",
    "\n",
    "def dqn_compute_next_q_values(self, next_time_steps):\n",
    "    next_target_q_values, _ = self._target_q_network(next_time_steps.observation, next_time_steps.step_type)\n",
    "    return tf.reduce_max(input_tensor=next_target_q_values, axis=-1)\n",
    "\n",
    "def ddqn_compute_next_q_values(self, next_time_steps):\n",
    "    next_q_values, _ = self._q_network(next_time_steps.observation,\n",
    "                                       next_time_steps.step_type)\n",
    "    best_next_actions = tf.cast(\n",
    "        tf.argmax(input=next_q_values, axis=-1), dtype=tf.int32)\n",
    "    \n",
    "    next_target_q_values, _ = self._target_q_network(\n",
    "        next_time_steps.observation, next_time_steps.step_type)\n",
    "    \n",
    "    multi_dim_actions = best_next_actions.shape.ndims > 1\n",
    "    \n",
    "    return common.index_with_actions(\n",
    "        next_target_q_values,\n",
    "        best_next_actions,\n",
    "        multi_dim_actions=multi_dim_actions)\n",
    "\n",
    "def _loss(self,\n",
    "          experience,\n",
    "          td_errors_loss_fn=element_wise_squared_loss,\n",
    "          gamma=1.0,\n",
    "          reward_scale_factor=1.0):\n",
    "    \n",
    "    time_steps, actions, next_time_steps = self._experience_to_transitions(experience)\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "        q_values, _ = self._q_network(time_steps.observation,\n",
    "                                      time_steps.step_type)\n",
    "\n",
    "        q_values = common.index_with_actions(q_values,\n",
    "                                             tf.cast(actions, dtype=tf.int32),\n",
    "                                             multi_dim_actions=False)\n",
    "\n",
    "        next_q_values = dqn_compute_next_q_values(self, next_time_steps)\n",
    "\n",
    "        td_targets = compute_td_targets(next_q_values,\n",
    "                                        rewards=reward_scale_factor * next_time_steps.reward,\n",
    "                                        discounts=gamma * next_time_steps.discount)\n",
    "\n",
    "        valid_mask = tf.cast(~time_steps.is_last(), tf.float32)\n",
    "        td_error = valid_mask * (td_targets - q_values)\n",
    "\n",
    "        td_loss = valid_mask * td_errors_loss_fn(td_targets, q_values)\n",
    "\n",
    "        loss = tf.reduce_mean(input_tensor=td_loss)\n",
    "        \n",
    "        return loss, td_loss, td_error\n",
    "    \n",
    "def _train(self, experience):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, td_loss, td_error = _loss(self, experience)\n",
    "        \n",
    "    variables_to_train = self._q_network.trainable_weights\n",
    "\n",
    "    grads = tape.gradient(loss, variables_to_train)\n",
    "    # Tuple is used for py3, where zip is a generator producing values once.\n",
    "    grads_and_vars = tuple(zip(grads, variables_to_train))\n",
    "    \n",
    "    self._optimizer.apply_gradients(grads_and_vars,\n",
    "                                    global_step=self.train_step_counter)\n",
    "\n",
    "    self._update_target()\n",
    "\n",
    "    return loss, td_loss, td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no state spec, map structure\n"
     ]
    }
   ],
   "source": [
    "experience, unused_info = next(iterator)\n",
    "\n",
    "loss, td_loss, td_error = _train(tf_agent, experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(35.04481, shape=(), dtype=float32)\n",
      "tf.Tensor(\n",
      "[3.78888935e-01 1.33157730e+01 2.27957654e+00 1.62405348e+00\n",
      " 2.17294312e+00 1.19236507e-01 4.52400742e+01 1.64778814e-01\n",
      " 1.23210136e+02 1.70909882e+01 2.32551336e+00 1.07562757e+00\n",
      " 3.55849862e+00 6.36713207e-01 2.63366795e+01 4.16435577e+02\n",
      " 2.92448831e+00 4.71523970e-01 4.45435667e+00 4.02125816e+01\n",
      " 6.46726532e+01 7.21317649e-01 3.49790907e+00 2.71748657e+01\n",
      " 2.85208679e+02 1.63425887e+00 5.28759289e+00 1.09741735e+00\n",
      " 2.94753761e+01 2.32595444e+01 2.98623681e+00 1.94531322e+00\n",
      " 3.25009232e+01 3.49029907e+02 4.14260626e+00 1.89885883e+01\n",
      " 2.92070836e-01 3.74208999e+00 4.57901993e+01 4.08018708e-01\n",
      " 1.22401869e+00 6.40908539e-01 3.20336580e+00 1.54813245e-01\n",
      " 7.37054825e+01 5.41661406e+00 4.30896915e-02 1.51563084e+00\n",
      " 1.49667978e+00 3.71509018e+01 1.22739375e-01 6.88067436e-01\n",
      " 3.94295692e-01 5.94698524e+01 9.09104943e-02 2.00414968e+00\n",
      " 3.34235573e+01 1.12032270e+01 4.58005476e+00 2.32201163e-03\n",
      " 3.40268951e+02 2.18348242e-02 5.42197075e+01 5.94311714e+00], shape=(64,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[-0.61553955 -3.6490784   1.5098267  -1.2743835  -1.4740906  -0.3453064\n",
      "  6.726074   -0.40592957 11.100006    4.1341248   1.5249634   1.0371246\n",
      "  1.8863983   0.7979431   5.1319275  20.406754    1.7101135   0.686676\n",
      " -2.1105347  -6.341339    8.041931    0.8493042   1.8702698  -5.2129517\n",
      " 16.888123    1.2783813  -2.2994766   1.0475769  -5.429123    4.822815\n",
      " -1.7280731  -1.3947449  -5.7009583  18.682343    2.0353394  -4.3575897\n",
      " -0.5404358   1.9344482   6.7668457   0.6387634   1.1063538   0.8005676\n",
      "  1.7897949   0.39346313  8.58519     2.327362    0.20758057  1.2311096\n",
      " -1.2233887   6.095154    0.3503418  -0.8294983   0.6279297  -7.71167\n",
      " -0.30151367 -1.4156799   5.781311   -3.3471222   2.1401062   0.04818726\n",
      " 18.44638     0.14776611 -7.3634033   2.437851  ], shape=(64,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(loss)\n",
    "print(td_loss)\n",
    "print(td_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
