{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'  # @param\n",
    "num_iterations = 20000  # @param\n",
    "\n",
    "initial_collect_steps = 1000  # @param\n",
    "collect_steps_per_iteration = 1  # @param\n",
    "replay_buffer_capacity = 100000  # @param\n",
    "\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "batch_size = 64  # @param\n",
    "learning_rate = 1e-3  # @param\n",
    "log_interval = 200  # @param\n",
    "\n",
    "num_eval_episodes = 10  # @param\n",
    "eval_interval = 1000  # @param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = suite_gym.load(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.specs import tensor_spec\n",
    "\n",
    "class DDQN(tf.keras.Model):\n",
    "    def __init__(self, input_tensor_spec, state_spec, name=\"DDQN\"):\n",
    "        super(DDQN, self).__init__()\n",
    "        \n",
    "        self._input_tensor_spec = input_tensor_spec\n",
    "        self._state_spec = ()\n",
    "        self.init_state_spec = state_spec\n",
    "        \n",
    "        hidden_units = 32\n",
    "        action_spec = tf.nest.flatten(state_spec)[0]\n",
    "        num_actions = action_spec.maximum - action_spec.minimum + 1\n",
    "        \n",
    "        # Shared layers\n",
    "        self._shared1 = tf.keras.layers.Dense(hidden_units, \n",
    "                         activation='relu', \n",
    "                         kernel_initializer='he_uniform')\n",
    "     \n",
    "        self._shared2 = tf.keras.layers.Dense(hidden_units, \n",
    "                         activation='relu', \n",
    "                         kernel_initializer='he_uniform')\n",
    "\n",
    "        # Learn advantage\n",
    "        self._adv1 = tf.keras.layers.Dense(hidden_units, \n",
    "                         activation='relu', \n",
    "                         kernel_initializer='he_uniform')\n",
    "        self._adv2 = tf.keras.layers.Dense(num_actions, \n",
    "                         activation='linear', \n",
    "                         kernel_initializer='he_uniform')\n",
    "        self._adv3 = tf.keras.layers.Lambda(lambda a: a - tf.keras.backend.mean(a, axis=1, keepdims=True), \n",
    "                                            output_shape=(num_actions,))\n",
    "        \n",
    "        # Learn value\n",
    "        self._value1 = tf.keras.layers.Dense(hidden_units, \n",
    "                         activation='relu', \n",
    "                         kernel_initializer='he_uniform')\n",
    "        self._value2 = tf.keras.layers.Dense(1, \n",
    "                         activation='linear', \n",
    "                         kernel_initializer='he_uniform')\n",
    "        \n",
    "        # Combine into Q\n",
    "        self._q_out = tf.keras.layers.Add()\n",
    "\n",
    "    def call(self, observation, step_type=None, network_state=()):\n",
    "        shared = self._shared1(observation)\n",
    "        shared = self._shared2(shared)\n",
    "        adv = self._adv1(shared)\n",
    "        adv = self._adv2(adv)\n",
    "        adv = self._adv3(adv)\n",
    "        value = self._value1(shared)\n",
    "        value = self._value2(shared)\n",
    "        q_out = self._q_out([adv, value])\n",
    "        return q_out, network_state\n",
    "    \n",
    "    def copy(self, name):\n",
    "        return type(self)(self._input_tensor_spec, self.init_state_spec, name)\n",
    "    \n",
    "    @property\n",
    "    def state_spec(self):\n",
    "        return self._state_spec\n",
    "\n",
    "    def _build(self):\n",
    "        if not self.built and self.input_tensor_spec is not None:\n",
    "            random_input = tensor_spec.sample_spec_nest(self.input_tensor_spec, outer_dims=(1,))\n",
    "            #step_type = tf.expand_dims(time_step.StepType.FIRST, 0)\n",
    "            self.__call__(random_input, None, None)\n",
    "\n",
    "    @property\n",
    "    def input_tensor_spec(self):\n",
    "        return self._input_tensor_spec\n",
    "\n",
    "    @property\n",
    "    def variables(self):\n",
    "        try:\n",
    "            self._build()\n",
    "        except ValueError as e:\n",
    "            traceback = sys.exc_info()[2]\n",
    "            six.reraise(\n",
    "                ValueError, \"Failed to call build on the network when accessing \"\n",
    "                \"variables. Message: {!r}.\".format(e), traceback)\n",
    "        return self.weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_net = DDQN(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from tf_agents.policies import epsilon_greedy_policy\n",
    "from tf_agents.policies import greedy_policy\n",
    "from tf_agents.policies import q_policy\n",
    "from tf_agents.utils import nest_utils\n",
    "\n",
    "LossInfo = collections.namedtuple(\"LossInfo\", (\"loss\", \"extra\"))\n",
    "\n",
    "def element_wise_squared_loss(x, y):\n",
    "    return tf.compat.v1.losses.mean_squared_error(\n",
    "      x, y, reduction=tf.compat.v1.losses.Reduction.NONE)\n",
    "\n",
    "\n",
    "def element_wise_huber_loss(x, y):\n",
    "    return tf.compat.v1.losses.huber_loss(\n",
    "      x, y, reduction=tf.compat.v1.losses.Reduction.NONE)\n",
    "\n",
    "\n",
    "def compute_td_targets(next_q_values, rewards, discounts):\n",
    "    return tf.stop_gradient(rewards + discounts * next_q_values)\n",
    "\n",
    "\n",
    "class DqnLossInfo(collections.namedtuple('DqnLossInfo',\n",
    "                                         ('td_loss', 'td_error'))):\n",
    "    pass\n",
    "\n",
    "class MyDqnAgent:\n",
    "    def __init__(\n",
    "      self,\n",
    "      time_step_spec,\n",
    "      action_spec,\n",
    "      q_network,\n",
    "      optimizer,\n",
    "      epsilon_greedy=0.1,\n",
    "      n_step_update=1,\n",
    "      boltzmann_temperature=None,\n",
    "      emit_log_probability=False,\n",
    "      # Params for target network updates\n",
    "      target_update_tau=1.0,\n",
    "      target_update_period=1,\n",
    "      # Params for training.\n",
    "      td_errors_loss_fn=None,\n",
    "      gamma=1.0,\n",
    "      reward_scale_factor=1.0,\n",
    "      gradient_clipping=None,\n",
    "      # Params for debugging\n",
    "      debug_summaries=False,\n",
    "      summarize_grads_and_vars=False,\n",
    "      train_step_counter=None,\n",
    "      name=None):\n",
    "        tf.Module.__init__(self, name=name)\n",
    "\n",
    "        flat_action_spec = tf.nest.flatten(action_spec)\n",
    "        self._num_actions = [\n",
    "            spec.maximum - spec.minimum + 1 for spec in flat_action_spec\n",
    "        ]\n",
    "\n",
    "        # TODO(oars): Get DQN working with more than one dim in the actions.\n",
    "        if len(flat_action_spec) > 1 or flat_action_spec[0].shape.ndims > 1:\n",
    "            raise ValueError('Only one dimensional actions are supported now.')\n",
    "\n",
    "        if not all(spec.minimum == 0 for spec in flat_action_spec):\n",
    "            raise ValueError(\n",
    "              'Action specs should have minimum of 0, but saw: {0}'.format(\n",
    "                  [spec.minimum for spec in flat_action_spec]))\n",
    "\n",
    "        if epsilon_greedy is not None and boltzmann_temperature is not None:\n",
    "            raise ValueError(\n",
    "              'Configured both epsilon_greedy value {} and temperature {}, '\n",
    "              'however only one of them can be used for exploration.'.format(\n",
    "                  epsilon_greedy, boltzmann_temperature))\n",
    "\n",
    "        self._time_step_spec = time_step_spec\n",
    "        self._action_spec = action_spec\n",
    "        self._q_network = q_network\n",
    "        self._target_q_network = self._q_network.copy(name='TargetQNetwork')\n",
    "        self._epsilon_greedy = epsilon_greedy\n",
    "        self._n_step_update = n_step_update\n",
    "        self._boltzmann_temperature = boltzmann_temperature\n",
    "        self._optimizer = optimizer\n",
    "        self._td_errors_loss_fn = td_errors_loss_fn or element_wise_huber_loss\n",
    "        self._gamma = gamma\n",
    "        self._reward_scale_factor = reward_scale_factor\n",
    "        self._gradient_clipping = gradient_clipping\n",
    "        self._update_target = self._get_target_updater(\n",
    "            target_update_tau, target_update_period)\n",
    "        self._train_step_counter = train_step_counter\n",
    "        self._summarize_grads_and_vars = None\n",
    "\n",
    "        policy = q_policy.QPolicy(\n",
    "            time_step_spec,\n",
    "            action_spec,\n",
    "            q_network=self._q_network,\n",
    "            emit_log_probability=emit_log_probability)\n",
    "\n",
    "        self._collect_policy = epsilon_greedy_policy.EpsilonGreedyPolicy(policy, epsilon=self._epsilon_greedy)\n",
    "        self._policy = greedy_policy.GreedyPolicy(policy)\n",
    "\n",
    "        if q_network.state_spec and n_step_update != 1:\n",
    "            raise NotImplementedError(\n",
    "              'DqnAgent does not currently support n-step updates with stateful '\n",
    "              'networks (i.e., RNNs), but n_step_update = {}'.format(n_step_update))\n",
    "\n",
    "        train_sequence_length = (\n",
    "            n_step_update + 1 if not q_network.state_spec else None)\n",
    "\n",
    "    def initialize(self):\n",
    "        common.soft_variables_update(\n",
    "            self._q_network.variables, self._target_q_network.variables, tau=1.0)\n",
    "\n",
    "    @property\n",
    "    def time_step_spec(self):\n",
    "        return self._time_step_spec\n",
    "    \n",
    "    @property\n",
    "    def action_spec(self):\n",
    "        return self._action_spec    \n",
    "\n",
    "    @property\n",
    "    def policy(self):\n",
    "        return self._policy\n",
    "\n",
    "    @property\n",
    "    def collect_policy(self):\n",
    "        return self._collect_policy\n",
    "    \n",
    "    @property\n",
    "    def collect_data_spec(self):\n",
    "        return self.collect_policy.trajectory_spec\n",
    "    \n",
    "    @property\n",
    "    def train_step_counter(self):\n",
    "        return self._train_step_counter\n",
    "        \n",
    "    def _get_target_updater(self, tau=1.0, period=1):\n",
    "        with tf.name_scope('update_targets'):\n",
    "\n",
    "            def update():\n",
    "                return common.soft_variables_update(\n",
    "                self._q_network.variables, self._target_q_network.variables, tau)\n",
    "\n",
    "            return common.Periodically(update, period, 'periodic_update_targets')\n",
    "\n",
    "    def _check_trajectory_dimensions(self, experience):\n",
    "        if not nest_utils.is_batched_nested_tensors(experience, self.collect_data_spec, num_outer_dims=2):\n",
    "            debug_str_1 = tf.nest.map_structure(lambda tp: tp.shape, experience)\n",
    "            debug_str_2 = tf.nest.map_structure(lambda spec: spec.shape,\n",
    "                                          self.collect_data_spec)\n",
    "            raise ValueError((debug_str_1, debug_str_2))\n",
    "            \n",
    "    def _experience_to_transitions(self, experience):\n",
    "        transitions = trajectory.to_transition(experience)\n",
    "\n",
    "        # Remove time dim if we are not using a recurrent network.\n",
    "        if not self._q_network.state_spec:\n",
    "            print(\"no state spec, map structure\")\n",
    "            transitions = tf.nest.map_structure(lambda x: tf.squeeze(x, [1]),\n",
    "                                          transitions)\n",
    "\n",
    "        time_steps, policy_steps, next_time_steps = transitions\n",
    "        actions = policy_steps.action\n",
    "        return time_steps, actions, next_time_steps\n",
    "\n",
    "    # Use @common.function in graph mode or for speeding up.\n",
    "    def train(self, experience, weights=None):\n",
    "        with tf.GradientTape() as tape:\n",
    "              loss_info = self._loss(\n",
    "                  experience,\n",
    "                  td_errors_loss_fn=self._td_errors_loss_fn,\n",
    "                  gamma=self._gamma,\n",
    "                  reward_scale_factor=self._reward_scale_factor,\n",
    "                  weights=weights)\n",
    "        tf.debugging.check_numerics(loss_info[0], 'Loss is inf or nan')\n",
    "        variables_to_train = self._q_network.trainable_weights\n",
    "        assert list(variables_to_train), \"No variables in the agent's q_network.\"\n",
    "        grads = tape.gradient(loss_info.loss, variables_to_train)\n",
    "        # Tuple is used for py3, where zip is a generator producing values once.\n",
    "        grads_and_vars = tuple(zip(grads, variables_to_train))\n",
    "        if self._gradient_clipping is not None:\n",
    "            print(\"gradient clipping enabled\")\n",
    "            grads_and_vars = eager_utils.clip_gradient_norms(grads_and_vars,\n",
    "                                                       self._gradient_clipping)\n",
    "\n",
    "        if self._summarize_grads_and_vars:\n",
    "            eager_utils.add_variables_summaries(grads_and_vars,\n",
    "                                          self.train_step_counter)\n",
    "            eager_utils.add_gradients_summaries(grads_and_vars,\n",
    "                                          self.train_step_counter)\n",
    "\n",
    "        self._optimizer.apply_gradients(grads_and_vars,\n",
    "                                    global_step=self.train_step_counter)\n",
    "\n",
    "        self._update_target()\n",
    "\n",
    "        return loss_info\n",
    "\n",
    "    def _loss(self,\n",
    "            experience,\n",
    "            td_errors_loss_fn=element_wise_huber_loss,\n",
    "            gamma=1.0,\n",
    "            reward_scale_factor=1.0,\n",
    "            weights=None):\n",
    "        # Check that `experience` includes two outer dimensions [B, T, ...]. This\n",
    "        # method requires a time dimension to compute the loss properly.\n",
    "        self._check_trajectory_dimensions(experience)\n",
    "\n",
    "        time_steps, actions, next_time_steps = self._experience_to_transitions(experience)\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            actions = tf.nest.flatten(actions)[0]\n",
    "            q_values, _ = self._q_network(time_steps.observation,\n",
    "                                    time_steps.step_type)\n",
    "\n",
    "            # Handle action_spec.shape=(), and shape=(1,) by using the\n",
    "            # multi_dim_actions param.\n",
    "            multi_dim_actions = tf.nest.flatten(self._action_spec)[0].shape.ndims > 0\n",
    "            print(\"multi_dim_actions={}\".format(multi_dim_actions))\n",
    "            q_values = common.index_with_actions(\n",
    "                q_values,\n",
    "                tf.cast(actions, dtype=tf.int32),\n",
    "                multi_dim_actions=multi_dim_actions)\n",
    "\n",
    "            next_q_values = self._compute_next_q_values(next_time_steps)\n",
    "\n",
    "            td_targets = compute_td_targets(\n",
    "                next_q_values,\n",
    "                rewards=reward_scale_factor * next_time_steps.reward,\n",
    "                discounts=gamma * next_time_steps.discount)\n",
    "\n",
    "            valid_mask = tf.cast(~time_steps.is_last(), tf.float32)\n",
    "            td_error = valid_mask * (td_targets - q_values)\n",
    "\n",
    "            td_loss = valid_mask * td_errors_loss_fn(td_targets, q_values)\n",
    "\n",
    "            if nest_utils.is_batched_nested_tensors(time_steps, self.time_step_spec, num_outer_dims=2):\n",
    "                # Do a sum over the time dimension.\n",
    "                print(\"is nested\")\n",
    "                td_loss = tf.reduce_sum(input_tensor=td_loss, axis=1)\n",
    "\n",
    "            if weights is not None:\n",
    "                print(\"has weights\")\n",
    "                td_loss *= weights\n",
    "\n",
    "            # Average across the elements of the batch.\n",
    "            # Note: We use an element wise loss above to ensure each element is always\n",
    "            #   weighted by 1/N where N is the batch size, even when some of the\n",
    "            #   weights are zero due to boundary transitions. Weighting by 1/K where K\n",
    "            #   is the actual number of non-zero weight would artificially increase\n",
    "            #   their contribution in the loss. Think about what would happen as\n",
    "            #   the number of boundary samples increases.\n",
    "            loss = tf.reduce_mean(input_tensor=td_loss)\n",
    "\n",
    "            return LossInfo(loss, DqnLossInfo(td_loss=td_loss,\n",
    "                                                 td_error=td_error))\n",
    "\n",
    "    #def _compute_next_q_values(self, next_time_steps):\n",
    "    #    next_target_q_values, _ = self._target_q_network(\n",
    "    #    next_time_steps.observation, next_time_steps.step_type)\n",
    "    #    # Reduce_max below assumes q_values are [BxF] or [BxTxF]\n",
    "    #    assert next_target_q_values.shape.ndims in [2, 3]\n",
    "    #    return tf.reduce_max(input_tensor=next_target_q_values, axis=-1)\n",
    "    \n",
    "    def _compute_next_q_values(self, next_time_steps):\n",
    "        next_q_values, _ = self._q_network(next_time_steps.observation,\n",
    "                                           next_time_steps.step_type)\n",
    "        best_next_actions = tf.cast(\n",
    "            tf.argmax(input=next_q_values, axis=-1), dtype=tf.int32)\n",
    "        next_target_q_values, _ = self._target_q_network(\n",
    "            next_time_steps.observation, next_time_steps.step_type)\n",
    "        multi_dim_actions = best_next_actions.shape.ndims > 1\n",
    "        return common.index_with_actions(\n",
    "            next_target_q_values,\n",
    "            best_next_actions,\n",
    "            multi_dim_actions=multi_dim_actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.compat.v2.Variable(0)\n",
    "\n",
    "dqn_agent = MyDqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "dqn_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = dqn_agent.policy\n",
    "collect_policy = dqn_agent.collect_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=dqn_agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    replay_buffer.add_batch(traj)\n",
    "\n",
    "for _ in range(initial_collect_steps):\n",
    "    collect_step(train_env, random_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, sample_batch_size=batch_size, num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0711 17:49:01.180028 4690060736 deprecation.py:323] From /Users/jwatts/TensorflowProjects/lib/python3.7/site-packages/tf_agents/policies/epsilon_greedy_policy.py:96: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no state spec, map structure\n",
      "multi_dim_actions=False\n",
      "no state spec, map structure\n",
      "multi_dim_actions=False\n",
      "step = 200: loss = 71.21623992919922\n",
      "step = 400: loss = 1169.346923828125\n",
      "step = 600: loss = 268.4332275390625\n",
      "step = 800: loss = 29.989715576171875\n",
      "step = 1000: loss = 33.46934509277344\n",
      "step = 1000: Average Return = 110.30000305175781\n",
      "step = 1200: loss = 64.45404815673828\n",
      "step = 1400: loss = 74.64080047607422\n",
      "step = 1600: loss = 191.63059997558594\n",
      "step = 1800: loss = 1219.96826171875\n",
      "step = 2000: loss = 292.3315734863281\n",
      "step = 2000: Average Return = 147.10000610351562\n",
      "step = 2200: loss = 413.4766540527344\n",
      "step = 2400: loss = 193.671630859375\n",
      "step = 2600: loss = 352.92236328125\n",
      "step = 2800: loss = 1234.370361328125\n",
      "step = 3000: loss = 223.91256713867188\n",
      "step = 3000: Average Return = 131.89999389648438\n",
      "step = 3200: loss = 287.515380859375\n",
      "step = 3400: loss = 675.6183471679688\n",
      "step = 3600: loss = 520.9953002929688\n",
      "step = 3800: loss = 1311.905517578125\n",
      "step = 4000: loss = 6256.0244140625\n",
      "step = 4000: Average Return = 477.6000061035156\n",
      "step = 4200: loss = 31362.6953125\n",
      "step = 4400: loss = 2317.165771484375\n",
      "step = 4600: loss = 2561.1513671875\n",
      "step = 4800: loss = 1309.88671875\n",
      "step = 5000: loss = 1388.178955078125\n",
      "step = 5000: Average Return = 196.8000030517578\n",
      "step = 5200: loss = 31820.583984375\n",
      "step = 5400: loss = 1761.3028564453125\n",
      "step = 5600: loss = 1804.9439697265625\n",
      "step = 5800: loss = 26305.294921875\n",
      "step = 6000: loss = 3949.99609375\n",
      "step = 6000: Average Return = 488.79998779296875\n",
      "step = 6200: loss = 189287.703125\n",
      "step = 6400: loss = 64073.90234375\n",
      "step = 6600: loss = 83570.890625\n",
      "step = 6800: loss = 9692.5810546875\n",
      "step = 7000: loss = 138283.25\n",
      "step = 7000: Average Return = 174.6999969482422\n",
      "step = 7200: loss = 96061.1328125\n",
      "step = 7400: loss = 393564.0625\n",
      "step = 7600: loss = 57059.63671875\n",
      "step = 7800: loss = 79015.359375\n",
      "step = 8000: loss = 440588.5625\n",
      "step = 8000: Average Return = 500.0\n",
      "step = 8200: loss = 156152.65625\n",
      "step = 8400: loss = 35778.0703125\n",
      "step = 8600: loss = 44256.43359375\n",
      "step = 8800: loss = 194468.46875\n",
      "step = 9000: loss = 739860.9375\n",
      "step = 9000: Average Return = 363.5\n",
      "step = 9200: loss = 247933.6875\n",
      "step = 9400: loss = 79591.7265625\n",
      "step = 9600: loss = 46944.9765625\n",
      "step = 9800: loss = 37663.546875\n",
      "step = 10000: loss = 3050594.0\n",
      "step = 10000: Average Return = 500.0\n",
      "step = 10200: loss = 79912.46875\n",
      "step = 10400: loss = 44253.21875\n",
      "step = 10600: loss = 1758256.75\n",
      "step = 10800: loss = 61396.265625\n",
      "step = 11000: loss = 56218.83203125\n",
      "step = 11000: Average Return = 500.0\n",
      "step = 11200: loss = 2446997.5\n",
      "step = 11400: loss = 1323480.25\n",
      "step = 11600: loss = 2256373.0\n",
      "step = 11800: loss = 62008.16015625\n",
      "step = 12000: loss = 49826.97265625\n",
      "step = 12000: Average Return = 489.6000061035156\n",
      "step = 12200: loss = 599152.875\n",
      "step = 12400: loss = 61329.671875\n",
      "step = 12600: loss = 187747.25\n",
      "step = 12800: loss = 196801.828125\n",
      "step = 13000: loss = 1811729.875\n",
      "step = 13000: Average Return = 500.0\n",
      "step = 13200: loss = 121712.203125\n",
      "step = 13400: loss = 6619054.5\n",
      "step = 13600: loss = 225656.390625\n",
      "step = 13800: loss = 70793.3359375\n",
      "step = 14000: loss = 152741.265625\n",
      "step = 14000: Average Return = 500.0\n",
      "step = 14200: loss = 416380.0625\n",
      "step = 14400: loss = 430545.9375\n",
      "step = 14600: loss = 67818.6171875\n",
      "step = 14800: loss = 167928.015625\n",
      "step = 15000: loss = 581286.125\n",
      "step = 15000: Average Return = 499.5\n",
      "step = 15200: loss = 2441153.5\n",
      "step = 15400: loss = 385544.5625\n",
      "step = 15600: loss = 338172.34375\n",
      "step = 15800: loss = 131162.4375\n",
      "step = 16000: loss = 515904.15625\n",
      "step = 16000: Average Return = 500.0\n",
      "step = 16200: loss = 451042.96875\n",
      "step = 16400: loss = 1569781.125\n",
      "step = 16600: loss = 520398.59375\n",
      "step = 16800: loss = 5800117.0\n",
      "step = 17000: loss = 651356.375\n",
      "step = 17000: Average Return = 487.6000061035156\n",
      "step = 17200: loss = 466409.5625\n",
      "step = 17400: loss = 313547.28125\n",
      "step = 17600: loss = 461238.0\n",
      "step = 17800: loss = 455184.25\n",
      "step = 18000: loss = 1053053.875\n",
      "step = 18000: Average Return = 486.1000061035156\n",
      "step = 18200: loss = 709581.9375\n",
      "step = 18400: loss = 387246.1875\n",
      "step = 18600: loss = 396113.1875\n",
      "step = 18800: loss = 859636.8125\n",
      "step = 19000: loss = 1103199.5\n",
      "step = 19000: Average Return = 500.0\n",
      "step = 19200: loss = 4761595.0\n",
      "step = 19400: loss = 482618.625\n",
      "step = 19600: loss = 929987.0\n",
      "step = 19800: loss = 1315484.875\n",
      "step = 20000: loss = 964268.5\n",
      "step = 20000: Average Return = 500.0\n"
     ]
    }
   ],
   "source": [
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "dqn_agent.train = common.function(dqn_agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "dqn_agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, dqn_agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    for _ in range(collect_steps_per_iteration):\n",
    "        collect_step(train_env, dqn_agent.collect_policy)\n",
    "\n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = dqn_agent.train(experience)\n",
    "\n",
    "    step = dqn_agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss.loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, dqn_agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-14.184999799728395, 550)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(steps, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Step')\n",
    "plt.ylim(top=550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = eval_env.reset()\n",
    "while not time_step.is_last():\n",
    "    action_step = dqn_agent.policy.action(time_step)\n",
    "    time_step = eval_env.step(action_step.action)\n",
    "    eval_py_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
