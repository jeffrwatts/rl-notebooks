{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import dqn_agent\n",
    "#from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "#from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'  # @param\n",
    "num_iterations = 20000  # @param\n",
    "\n",
    "initial_collect_steps = 1000  # @param\n",
    "collect_steps_per_iteration = 1  # @param\n",
    "replay_buffer_capacity = 100000  # @param\n",
    "\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "batch_size = 64  # @param\n",
    "learning_rate = 1e-3  # @param\n",
    "log_interval = 200  # @param\n",
    "\n",
    "num_eval_episodes = 10  # @param\n",
    "eval_interval = 1000  # @param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = suite_gym.load(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "BoundedArraySpec(shape=(4,), dtype=dtype('float32'), name=None, minimum=[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], maximum=[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38])\n",
      "Action Spec:\n",
      "BoundedArraySpec(shape=(), dtype=dtype('int64'), name=None, minimum=0, maximum=1)\n"
     ]
    }
   ],
   "source": [
    "print('Observation Spec:')\n",
    "print(env.time_step_spec().observation)\n",
    "print('Action Spec:')\n",
    "print(env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.specs import tensor_spec\n",
    "\n",
    "class DDQN(tf.keras.Model):\n",
    "    def __init__(self, input_tensor_spec, state_spec, name=\"DDQN\"):\n",
    "        super(DDQN, self).__init__()\n",
    "        \n",
    "        self._input_tensor_spec = input_tensor_spec\n",
    "        self._state_spec = ()\n",
    "        self.init_state_spec = state_spec\n",
    "        \n",
    "        hidden_units = 32\n",
    "        action_spec = tf.nest.flatten(state_spec)[0]\n",
    "        num_actions = action_spec.maximum - action_spec.minimum + 1\n",
    "        \n",
    "        # Shared layers\n",
    "        self._shared1 = tf.keras.layers.Dense(hidden_units, \n",
    "                         activation='relu', \n",
    "                         kernel_initializer='he_uniform')\n",
    "     \n",
    "        self._shared2 = tf.keras.layers.Dense(hidden_units, \n",
    "                         activation='relu', \n",
    "                         kernel_initializer='he_uniform')\n",
    "\n",
    "        # Learn advantage\n",
    "        self._adv1 = tf.keras.layers.Dense(hidden_units, \n",
    "                         activation='relu', \n",
    "                         kernel_initializer='he_uniform')\n",
    "        self._adv2 = tf.keras.layers.Dense(num_actions, \n",
    "                         activation='linear', \n",
    "                         kernel_initializer='he_uniform')\n",
    "        self._adv3 = tf.keras.layers.Lambda(lambda a: a - tf.keras.backend.mean(a, axis=1, keepdims=True), \n",
    "                                            output_shape=(num_actions,))\n",
    "        \n",
    "        # Learn value\n",
    "        self._value1 = tf.keras.layers.Dense(hidden_units, \n",
    "                         activation='relu', \n",
    "                         kernel_initializer='he_uniform')\n",
    "        self._value2 = tf.keras.layers.Dense(1, \n",
    "                         activation='linear', \n",
    "                         kernel_initializer='he_uniform')\n",
    "        \n",
    "        # Combine into Q\n",
    "        self._q_out = tf.keras.layers.Add()\n",
    "\n",
    "    def call(self, observation, step_type=None, network_state=()):\n",
    "        shared = self._shared1(observation)\n",
    "        shared = self._shared2(shared)\n",
    "        adv = self._adv1(shared)\n",
    "        adv = self._adv2(adv)\n",
    "        adv = self._adv3(adv)\n",
    "        value = self._value1(shared)\n",
    "        value = self._value2(shared)\n",
    "        q_out = self._q_out([adv, value])\n",
    "        return q_out, network_state\n",
    "    \n",
    "    def copy(self, name):\n",
    "        print(\"copy override\")\n",
    "        return type(self)(self._input_tensor_spec, self.init_state_spec, name)\n",
    "    \n",
    "    @property\n",
    "    def state_spec(self):\n",
    "        return self._state_spec\n",
    "\n",
    "    def _build(self):\n",
    "        if not self.built and self.input_tensor_spec is not None:\n",
    "            random_input = tensor_spec.sample_spec_nest(self.input_tensor_spec, outer_dims=(1,))\n",
    "            #step_type = tf.expand_dims(time_step.StepType.FIRST, 0)\n",
    "            self.__call__(random_input, None, None)\n",
    "\n",
    "    @property\n",
    "    def input_tensor_spec(self):\n",
    "        return self._input_tensor_spec\n",
    "\n",
    "    @property\n",
    "    def variables(self):\n",
    "        try:\n",
    "            self._build()\n",
    "        except ValueError as e:\n",
    "            traceback = sys.exc_info()[2]\n",
    "            six.reraise(\n",
    "                ValueError, \"Failed to call build on the network when accessing \"\n",
    "                \"variables. Message: {!r}.\".format(e), traceback)\n",
    "        return self.weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_net = DDQN(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec())\n",
    "\n",
    "#q_net = q_network.QNetwork(\n",
    "#    train_env.observation_spec(),\n",
    "#    train_env.action_spec(),\n",
    "#    fc_layer_params=fc_layer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy override\n",
      "collect policy is e-greedy\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.compat.v2.Variable(0)\n",
    "\n",
    "tf_agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=dqn_agent.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = tf_agent.policy\n",
    "collect_policy = tf_agent.collect_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=tf_agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    replay_buffer.add_batch(traj)\n",
    "\n",
    "for _ in range(initial_collect_steps):\n",
    "    collect_step(train_env, random_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, sample_batch_size=batch_size, num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0710 09:14:26.605584 4753360320 deprecation.py:323] From /Users/jwatts/TensorflowProjects/lib/python3.7/site-packages/tf_agents/policies/epsilon_greedy_policy.py:96: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "td_errors_loss_fn=<function element_wise_squared_loss at 0x141825e18>\n",
      "_n_step_update=1\n",
      "no state spec, map structure\n",
      "multi_dim_actions=False\n",
      "reward_scale_factor=1.0; gamma=1.0\n",
      "td_errors_loss_fn=<function element_wise_squared_loss at 0x141825e18>\n",
      "_n_step_update=1\n",
      "no state spec, map structure\n",
      "multi_dim_actions=False\n",
      "reward_scale_factor=1.0; gamma=1.0\n",
      "step = 200: loss = 40.59079360961914\n",
      "step = 400: loss = 51.751182556152344\n",
      "step = 600: loss = 116.19235229492188\n",
      "step = 800: loss = 14.281712532043457\n",
      "step = 1000: loss = 48.84848403930664\n",
      "step = 1000: Average Return = 63.20000076293945\n",
      "step = 1200: loss = 175.82261657714844\n",
      "step = 1400: loss = 288.1424255371094\n",
      "step = 1600: loss = 202.8638916015625\n",
      "step = 1800: loss = 537.3378295898438\n",
      "step = 2000: loss = 658.6384887695312\n",
      "step = 2000: Average Return = 227.5\n",
      "step = 2200: loss = 259.1187744140625\n",
      "step = 2400: loss = 6298.740234375\n",
      "step = 2600: loss = 458.318115234375\n",
      "step = 2800: loss = 63762.1015625\n",
      "step = 3000: loss = 5582.7958984375\n",
      "step = 3000: Average Return = 199.5\n",
      "step = 3200: loss = 780.8543701171875\n",
      "step = 3400: loss = 2613.532958984375\n",
      "step = 3600: loss = 10358.26171875\n",
      "step = 3800: loss = 1346.2747802734375\n",
      "step = 4000: loss = 1538.5159912109375\n",
      "step = 4000: Average Return = 193.3000030517578\n",
      "step = 4200: loss = 16686.54296875\n",
      "step = 4400: loss = 2026.5419921875\n",
      "step = 4600: loss = 7121.3544921875\n",
      "step = 4800: loss = 1397.75048828125\n",
      "step = 5000: loss = 2611.02685546875\n",
      "step = 5000: Average Return = 171.60000610351562\n",
      "step = 5200: loss = 79454.96875\n",
      "step = 5400: loss = 19232.908203125\n",
      "step = 5600: loss = 6667.1044921875\n",
      "step = 5800: loss = 4372.849609375\n",
      "step = 6000: loss = 3911.771240234375\n",
      "step = 6000: Average Return = 252.3000030517578\n",
      "step = 6200: loss = 2115.2734375\n",
      "step = 6400: loss = 6861.291015625\n",
      "step = 6600: loss = 1749.0419921875\n",
      "step = 6800: loss = 1289.680419921875\n",
      "step = 7000: loss = 2486.818359375\n",
      "step = 7000: Average Return = 186.8000030517578\n",
      "step = 7200: loss = 2797.37841796875\n",
      "step = 7400: loss = 1418.7169189453125\n",
      "step = 7600: loss = 1547.6494140625\n",
      "step = 7800: loss = 13603.10546875\n",
      "step = 8000: loss = 1932.953369140625\n",
      "step = 8000: Average Return = 221.89999389648438\n",
      "step = 8200: loss = 3097.374267578125\n",
      "step = 8400: loss = 3951.541748046875\n",
      "step = 8600: loss = 24405.0234375\n",
      "step = 8800: loss = 48667.41015625\n",
      "step = 9000: loss = 7035.37158203125\n",
      "step = 9000: Average Return = 243.10000610351562\n",
      "step = 9200: loss = 5230.9794921875\n",
      "step = 9400: loss = 15706.7607421875\n",
      "step = 9600: loss = 23097.158203125\n",
      "step = 9800: loss = 3947.932861328125\n",
      "step = 10000: loss = 5453.6083984375\n",
      "step = 10000: Average Return = 203.10000610351562\n",
      "step = 10200: loss = 2472.567626953125\n",
      "step = 10400: loss = 3780.85009765625\n",
      "step = 10600: loss = 3292.3115234375\n",
      "step = 10800: loss = 2463.8466796875\n",
      "step = 11000: loss = 8951.4853515625\n",
      "step = 11000: Average Return = 262.6000061035156\n",
      "step = 11200: loss = 2510.0986328125\n",
      "step = 11400: loss = 2457.52099609375\n",
      "step = 11600: loss = 3513.451416015625\n",
      "step = 11800: loss = 3481.033935546875\n",
      "step = 12000: loss = 2687.58251953125\n",
      "step = 12000: Average Return = 217.5\n",
      "step = 12200: loss = 3891.455078125\n",
      "step = 12400: loss = 16493.0546875\n",
      "step = 12600: loss = 5298.7158203125\n",
      "step = 12800: loss = 4661.76025390625\n",
      "step = 13000: loss = 3114.94970703125\n",
      "step = 13000: Average Return = 282.29998779296875\n",
      "step = 13200: loss = 3793.9755859375\n",
      "step = 13400: loss = 11216.03515625\n",
      "step = 13600: loss = 6436.3994140625\n",
      "step = 13800: loss = 4326.47900390625\n",
      "step = 14000: loss = 3583.8330078125\n",
      "step = 14000: Average Return = 495.6000061035156\n",
      "step = 14200: loss = 6843.81689453125\n",
      "step = 14400: loss = 2833.681396484375\n",
      "step = 14600: loss = 2574.800537109375\n",
      "step = 14800: loss = 5856.23828125\n",
      "step = 15000: loss = 4294.9609375\n",
      "step = 15000: Average Return = 500.0\n",
      "step = 15200: loss = 7487.5595703125\n",
      "step = 15400: loss = 20461.421875\n",
      "step = 15600: loss = 7505.2470703125\n",
      "step = 15800: loss = 22725.05078125\n",
      "step = 16000: loss = 6903.666015625\n",
      "step = 16000: Average Return = 500.0\n",
      "step = 16200: loss = 7844.49560546875\n",
      "step = 16400: loss = 11313.05078125\n",
      "step = 16600: loss = 13964.1064453125\n",
      "step = 16800: loss = 23777.966796875\n",
      "step = 17000: loss = 25512.09765625\n",
      "step = 17000: Average Return = 500.0\n",
      "step = 17200: loss = 30865.1953125\n",
      "step = 17400: loss = 38530.0859375\n",
      "step = 17600: loss = 55789.1953125\n",
      "step = 17800: loss = 99465.484375\n",
      "step = 18000: loss = 58734.64453125\n",
      "step = 18000: Average Return = 500.0\n",
      "step = 18200: loss = 169306.203125\n",
      "step = 18400: loss = 239615.84375\n",
      "step = 18600: loss = 89087.484375\n",
      "step = 18800: loss = 6682542.5\n",
      "step = 19000: loss = 224411.59375\n",
      "step = 19000: Average Return = 500.0\n",
      "step = 19200: loss = 238543.234375\n",
      "step = 19400: loss = 370097.65625\n",
      "step = 19600: loss = 12433700.0\n",
      "step = 19800: loss = 486597.84375\n",
      "step = 20000: loss = 207236.34375\n",
      "step = 20000: Average Return = 500.0\n"
     ]
    }
   ],
   "source": [
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "tf_agent.train = common.function(tf_agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "tf_agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    for _ in range(collect_steps_per_iteration):\n",
    "        collect_step(train_env, tf_agent.collect_policy)\n",
    "\n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = tf_agent.train(experience)\n",
    "\n",
    "    step = tf_agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss.loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-15.55, 550)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(steps, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Step')\n",
    "plt.ylim(top=550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = eval_env.reset()\n",
    "while not time_step.is_last():\n",
    "    action_step = tf_agent.policy.action(time_step)\n",
    "    time_step = eval_env.step(action_step.action)\n",
    "    eval_py_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _experience_to_transitions(experience):\n",
    "    transitions = trajectory.to_transition(experience)\n",
    "\n",
    "    transitions = tf.nest.map_structure(lambda x: tf.squeeze(x, [1]), transitions)\n",
    "\n",
    "    time_steps, policy_steps, next_time_steps = transitions\n",
    "    actions = policy_steps.action\n",
    "    return time_steps, actions, next_time_steps\n",
    "\n",
    "def element_wise_squared_loss(x, y):\n",
    "    return tf.compat.v1.losses.mean_squared_error(x, y, reduction=tf.compat.v1.losses.Reduction.NONE)\n",
    "\n",
    "def element_wise_huber_loss(x, y):\n",
    "    return tf.compat.v1.losses.huber_loss(x, y, reduction=tf.compat.v1.losses.Reduction.NONE)\n",
    "\n",
    "def compute_td_targets(next_q_values, rewards, discounts):\n",
    "    return tf.stop_gradient(rewards + discounts * next_q_values)\n",
    "\n",
    "def index_with_actions(q_values, actions, multi_dim_actions=False):\n",
    "    if actions.shape.ndims is None:\n",
    "        raise ValueError('actions should have known rank.')\n",
    "    \n",
    "    batch_dims = actions.shape.ndims\n",
    "    if multi_dim_actions:\n",
    "        # In the multidimensional case, the last dimension of actions indexes the\n",
    "        # vector of actions for each batch, so exclude it from the batch dimensions.\n",
    "        batch_dims -= 1\n",
    "\n",
    "    outer_shape = tf.shape(input=actions)\n",
    "    batch_indices = tf.meshgrid(*[tf.range(outer_shape[i]) for i in range(batch_dims)], indexing='ij')\n",
    "    batch_indices = [\n",
    "        tf.expand_dims(batch_index, -1) for batch_index in batch_indices\n",
    "    ]\n",
    "    \n",
    "    if not multi_dim_actions:\n",
    "        actions = tf.expand_dims(actions, -1)\n",
    "        \n",
    "    action_indices = tf.concat(batch_indices + [actions], -1)\n",
    "    return tf.gather_nd(q_values, action_indices)\n",
    "\n",
    "def _compute_next_q_values(self, next_time_steps):\n",
    "    next_target_q_values, _ = self._target_q_network(next_time_steps.observation, next_time_steps.step_type)\n",
    "    return tf.reduce_max(input_tensor=next_target_q_values, axis=-1)\n",
    "\n",
    "def _loss(self, experience, reward_scale_factor=1.0, gamma = 1.0):\n",
    "    time_steps, actions, next_time_steps = self._experience_to_transitions(experience)\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "        actions = tf.nest.flatten(actions)[0]\n",
    "        q_values, _ = self._q_network(time_steps.observation, time_steps.step_type)\n",
    "\n",
    "        print(\"ndims={}\".format(tf.nest.flatten(self._action_spec)[0].shape.ndims))\n",
    "        multi_dim_actions = tf.nest.flatten(self._action_spec)[0].shape.ndims > 0\n",
    "        q_values = index_with_actions(q_values,\n",
    "                                      tf.cast(actions, dtype=tf.int32),\n",
    "                                      multi_dim_actions=multi_dim_actions)\n",
    "\n",
    "        next_q_values = self._compute_next_q_values(next_time_steps)\n",
    "\n",
    "        td_targets = compute_td_targets(next_q_values,\n",
    "                                        rewards=reward_scale_factor * next_time_steps.reward,\n",
    "                                        discounts=gamma * next_time_steps.discount)\n",
    "\n",
    "        valid_mask = tf.cast(~time_steps.is_last(), tf.float32)\n",
    "        td_error = valid_mask * (td_targets - q_values)\n",
    "\n",
    "        td_loss = valid_mask * element_wise_squared_loss(td_targets, q_values)\n",
    "\n",
    "        loss = tf.reduce_mean(input_tensor=td_loss)\n",
    "\n",
    "        return loss, td_loss, td_error\n",
    "    \n",
    "def _train(self, experience):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, td_loss, td_error = _loss(self, experience)\n",
    "    variables_to_train = self._q_network.trainable_weights\n",
    "    grads = tape.gradient(loss, variables_to_train)\n",
    "    # Tuple is used for py3, where zip is a generator producing values once.\n",
    "    grads_and_vars = tuple(zip(grads, variables_to_train))\n",
    "    self._optimizer.apply_gradients(grads_and_vars,\n",
    "                                    global_step=self.train_step_counter)\n",
    "\n",
    "    self._update_target()\n",
    "\n",
    "    return loss, td_loss, td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experience, unused_info = next(iterator)\n",
    "\n",
    "loss, td_loss, td_error = _train(tf_agent, experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss)\n",
    "print(td_loss)\n",
    "print(td_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
