{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'  # @param\n",
    "num_iterations = 20000  # @param\n",
    "\n",
    "initial_collect_steps = 1000  # @param\n",
    "collect_steps_per_iteration = 1  # @param\n",
    "replay_buffer_capacity = 100000  # @param\n",
    "\n",
    "batch_size = 64  # @param\n",
    "learning_rate = 1e-3  # @param\n",
    "log_interval = 200  # @param\n",
    "\n",
    "num_eval_episodes = 10  # @param\n",
    "eval_interval = 1000  # @param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = suite_gym.load(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.specs import tensor_spec\n",
    "\n",
    "class DDQN(tf.keras.Model):\n",
    "    def __init__(self, input_tensor_spec, state_spec, name=\"DDQN\"):\n",
    "        super(DDQN, self).__init__()\n",
    "        \n",
    "        self._input_tensor_spec = input_tensor_spec\n",
    "        self._state_spec = ()\n",
    "        self.init_state_spec = state_spec\n",
    "        \n",
    "        hidden_units = 32\n",
    "        action_spec = tf.nest.flatten(state_spec)[0]\n",
    "        num_actions = action_spec.maximum - action_spec.minimum + 1\n",
    "        \n",
    "        # Shared layers\n",
    "        self._shared1 = tf.keras.layers.Dense(hidden_units, \n",
    "                         activation='relu', \n",
    "                         kernel_initializer='he_uniform')\n",
    "     \n",
    "        self._shared2 = tf.keras.layers.Dense(hidden_units, \n",
    "                         activation='relu', \n",
    "                         kernel_initializer='he_uniform')\n",
    "\n",
    "        # Learn advantage\n",
    "        self._adv1 = tf.keras.layers.Dense(hidden_units, \n",
    "                         activation='relu', \n",
    "                         kernel_initializer='he_uniform')\n",
    "        self._adv2 = tf.keras.layers.Dense(num_actions, \n",
    "                         activation='linear', \n",
    "                         kernel_initializer='he_uniform')\n",
    "        self._adv3 = tf.keras.layers.Lambda(lambda a: a - tf.keras.backend.mean(a, axis=1, keepdims=True), \n",
    "                                            output_shape=(num_actions,))\n",
    "        \n",
    "        # Learn value\n",
    "        self._value1 = tf.keras.layers.Dense(hidden_units, \n",
    "                         activation='relu', \n",
    "                         kernel_initializer='he_uniform')\n",
    "        self._value2 = tf.keras.layers.Dense(1, \n",
    "                         activation='linear', \n",
    "                         kernel_initializer='he_uniform')\n",
    "        \n",
    "        # Combine into Q\n",
    "        self._q_out = tf.keras.layers.Add()\n",
    "\n",
    "    def call(self, observation, step_type=None, network_state=()):\n",
    "        shared = self._shared1(observation)\n",
    "        shared = self._shared2(shared)\n",
    "        adv = self._adv1(shared)\n",
    "        adv = self._adv2(adv)\n",
    "        adv = self._adv3(adv)\n",
    "        value = self._value1(shared)\n",
    "        value = self._value2(shared)\n",
    "        q_out = self._q_out([adv, value])\n",
    "        return q_out, network_state\n",
    "    \n",
    "    def copy(self, name):\n",
    "        return type(self)(self._input_tensor_spec, self.init_state_spec, name)\n",
    "    \n",
    "    @property\n",
    "    def state_spec(self):\n",
    "        return self._state_spec\n",
    "\n",
    "    def _build(self):\n",
    "        if not self.built and self.input_tensor_spec is not None:\n",
    "            random_input = tensor_spec.sample_spec_nest(self.input_tensor_spec, outer_dims=(1,))\n",
    "            #step_type = tf.expand_dims(time_step.StepType.FIRST, 0)\n",
    "            self.__call__(random_input, None, None)\n",
    "\n",
    "    @property\n",
    "    def input_tensor_spec(self):\n",
    "        return self._input_tensor_spec\n",
    "\n",
    "    @property\n",
    "    def variables(self):\n",
    "        try:\n",
    "            self._build()\n",
    "        except ValueError as e:\n",
    "            traceback = sys.exc_info()[2]\n",
    "            six.reraise(\n",
    "                ValueError, \"Failed to call build on the network when accessing \"\n",
    "                \"variables. Message: {!r}.\".format(e), traceback)\n",
    "        return self.weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_net = DDQN(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from tf_agents.policies import epsilon_greedy_policy\n",
    "from tf_agents.policies import greedy_policy\n",
    "from tf_agents.policies import q_policy\n",
    "from tf_agents.utils import nest_utils\n",
    "\n",
    "LossInfo = collections.namedtuple(\"LossInfo\", (\"loss\", \"extra\"))\n",
    "\n",
    "def element_wise_squared_loss(x, y):\n",
    "    return tf.compat.v1.losses.mean_squared_error(\n",
    "      x, y, reduction=tf.compat.v1.losses.Reduction.NONE)\n",
    "\n",
    "\n",
    "def element_wise_huber_loss(x, y):\n",
    "    return tf.compat.v1.losses.huber_loss(\n",
    "      x, y, reduction=tf.compat.v1.losses.Reduction.NONE)\n",
    "\n",
    "\n",
    "def compute_td_targets(next_q_values, rewards, discounts):\n",
    "    return tf.stop_gradient(rewards + discounts * next_q_values)\n",
    "\n",
    "\n",
    "class DqnLossInfo(collections.namedtuple('DqnLossInfo',\n",
    "                                         ('td_loss', 'td_error'))):\n",
    "    pass\n",
    "\n",
    "class MyDqnAgent:\n",
    "    def __init__(\n",
    "      self,\n",
    "      time_step_spec,\n",
    "      action_spec,\n",
    "      q_network,\n",
    "      optimizer,\n",
    "      epsilon_greedy=0.1,\n",
    "      # Params for target network updates\n",
    "      target_update_tau=1.0,\n",
    "      target_update_period=1,\n",
    "      # Params for training.\n",
    "      td_errors_loss_fn=None,\n",
    "      gamma=1.0,\n",
    "      gradient_clipping=None,\n",
    "      train_step_counter=None,\n",
    "      name=None):\n",
    "\n",
    "        flat_action_spec = tf.nest.flatten(action_spec)\n",
    "        self._num_actions = [\n",
    "            spec.maximum - spec.minimum + 1 for spec in flat_action_spec\n",
    "        ]\n",
    "\n",
    "        self._time_step_spec = time_step_spec\n",
    "        self._action_spec = action_spec\n",
    "        self._q_network = q_network\n",
    "        self._target_q_network = self._q_network.copy(name='TargetQNetwork')\n",
    "        self._epsilon_greedy = epsilon_greedy\n",
    "        self._optimizer = optimizer\n",
    "        self._td_errors_loss_fn = td_errors_loss_fn or element_wise_huber_loss\n",
    "        self._gamma = gamma\n",
    "        self._gradient_clipping = gradient_clipping\n",
    "        self._update_target = self._get_target_updater(target_update_tau, target_update_period)\n",
    "        self._train_step_counter = train_step_counter\n",
    "\n",
    "        policy = q_policy.QPolicy(\n",
    "            time_step_spec,\n",
    "            action_spec,\n",
    "            q_network=self._q_network,\n",
    "            emit_log_probability=False)\n",
    "\n",
    "        self._collect_policy = epsilon_greedy_policy.EpsilonGreedyPolicy(policy, epsilon=self._epsilon_greedy)\n",
    "        self._policy = greedy_policy.GreedyPolicy(policy)\n",
    "\n",
    "    def initialize(self):\n",
    "        common.soft_variables_update(\n",
    "            self._q_network.variables, self._target_q_network.variables, tau=1.0)\n",
    "\n",
    "    @property\n",
    "    def time_step_spec(self):\n",
    "        return self._time_step_spec\n",
    "    \n",
    "    @property\n",
    "    def action_spec(self):\n",
    "        return self._action_spec    \n",
    "\n",
    "    @property\n",
    "    def policy(self):\n",
    "        return self._policy\n",
    "\n",
    "    @property\n",
    "    def collect_policy(self):\n",
    "        return self._collect_policy\n",
    "    \n",
    "    @property\n",
    "    def collect_data_spec(self):\n",
    "        return self.collect_policy.trajectory_spec\n",
    "    \n",
    "    @property\n",
    "    def train_step_counter(self):\n",
    "        return self._train_step_counter\n",
    "        \n",
    "    def _get_target_updater(self, tau=1.0, period=1):\n",
    "        with tf.name_scope('update_targets'):\n",
    "\n",
    "            def update():\n",
    "                return common.soft_variables_update(\n",
    "                self._q_network.variables, self._target_q_network.variables, tau)\n",
    "\n",
    "            return common.Periodically(update, period, 'periodic_update_targets')\n",
    "\n",
    "    def _check_trajectory_dimensions(self, experience):\n",
    "        if not nest_utils.is_batched_nested_tensors(experience, self.collect_data_spec, num_outer_dims=2):\n",
    "            debug_str_1 = tf.nest.map_structure(lambda tp: tp.shape, experience)\n",
    "            debug_str_2 = tf.nest.map_structure(lambda spec: spec.shape,\n",
    "                                          self.collect_data_spec)\n",
    "            raise ValueError((debug_str_1, debug_str_2))\n",
    "            \n",
    "    def _experience_to_transitions(self, experience):\n",
    "        transitions = trajectory.to_transition(experience)\n",
    "\n",
    "        # Remove time dim if we are not using a recurrent network.\n",
    "        if not self._q_network.state_spec:\n",
    "            print(\"no state spec, map structure\")\n",
    "            transitions = tf.nest.map_structure(lambda x: tf.squeeze(x, [1]),\n",
    "                                          transitions)\n",
    "\n",
    "        time_steps, policy_steps, next_time_steps = transitions\n",
    "        actions = policy_steps.action\n",
    "        return time_steps, actions, next_time_steps\n",
    "\n",
    "    # Use @common.function in graph mode or for speeding up.\n",
    "    def train(self, experience, weights=None):\n",
    "        with tf.GradientTape() as tape:\n",
    "              loss_info = self._loss(\n",
    "                  experience,\n",
    "                  td_errors_loss_fn=self._td_errors_loss_fn,\n",
    "                  gamma=self._gamma)\n",
    "        tf.debugging.check_numerics(loss_info[0], 'Loss is inf or nan')\n",
    "        variables_to_train = self._q_network.trainable_weights\n",
    "        assert list(variables_to_train), \"No variables in the agent's q_network.\"\n",
    "        grads = tape.gradient(loss_info.loss, variables_to_train)\n",
    "        # Tuple is used for py3, where zip is a generator producing values once.\n",
    "        grads_and_vars = tuple(zip(grads, variables_to_train))\n",
    "        if self._gradient_clipping is not None:\n",
    "            print(\"gradient clipping enabled\")\n",
    "            grads_and_vars = eager_utils.clip_gradient_norms(grads_and_vars,\n",
    "                                                       self._gradient_clipping)\n",
    "\n",
    "        self._optimizer.apply_gradients(grads_and_vars,\n",
    "                                    global_step=self.train_step_counter)\n",
    "\n",
    "        self._update_target()\n",
    "\n",
    "        return loss_info\n",
    "\n",
    "    def _loss(self,\n",
    "            experience,\n",
    "            td_errors_loss_fn=element_wise_huber_loss,\n",
    "            gamma=1.0):\n",
    "        # Check that `experience` includes two outer dimensions [B, T, ...]. This\n",
    "        # method requires a time dimension to compute the loss properly.\n",
    "        self._check_trajectory_dimensions(experience)\n",
    "\n",
    "        time_steps, actions, next_time_steps = self._experience_to_transitions(experience)\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            actions = tf.nest.flatten(actions)[0]\n",
    "            q_values, _ = self._q_network(time_steps.observation,\n",
    "                                    time_steps.step_type)\n",
    "\n",
    "            # Handle action_spec.shape=(), and shape=(1,) by using the\n",
    "            # multi_dim_actions param.\n",
    "            multi_dim_actions = tf.nest.flatten(self._action_spec)[0].shape.ndims > 0\n",
    "            print(\"multi_dim_actions={}\".format(multi_dim_actions))\n",
    "            q_values = common.index_with_actions(\n",
    "                q_values,\n",
    "                tf.cast(actions, dtype=tf.int32),\n",
    "                multi_dim_actions=multi_dim_actions)\n",
    "\n",
    "            next_q_values = self._compute_next_q_values(next_time_steps)\n",
    "\n",
    "            td_targets = compute_td_targets(\n",
    "                next_q_values,\n",
    "                rewards=next_time_steps.reward,\n",
    "                discounts=gamma * next_time_steps.discount)\n",
    "\n",
    "            valid_mask = tf.cast(~time_steps.is_last(), tf.float32)\n",
    "            td_error = valid_mask * (td_targets - q_values)\n",
    "\n",
    "            td_loss = valid_mask * td_errors_loss_fn(td_targets, q_values)\n",
    "\n",
    "            # Average across the elements of the batch.\n",
    "            # Note: We use an element wise loss above to ensure each element is always\n",
    "            #   weighted by 1/N where N is the batch size, even when some of the\n",
    "            #   weights are zero due to boundary transitions. Weighting by 1/K where K\n",
    "            #   is the actual number of non-zero weight would artificially increase\n",
    "            #   their contribution in the loss. Think about what would happen as\n",
    "            #   the number of boundary samples increases.\n",
    "            loss = tf.reduce_mean(input_tensor=td_loss)\n",
    "\n",
    "            return LossInfo(loss, DqnLossInfo(td_loss=td_loss,\n",
    "                                                 td_error=td_error))\n",
    "\n",
    "    #def _compute_next_q_values(self, next_time_steps):\n",
    "    #    next_target_q_values, _ = self._target_q_network(\n",
    "    #    next_time_steps.observation, next_time_steps.step_type)\n",
    "    #    # Reduce_max below assumes q_values are [BxF] or [BxTxF]\n",
    "    #    assert next_target_q_values.shape.ndims in [2, 3]\n",
    "    #    return tf.reduce_max(input_tensor=next_target_q_values, axis=-1)\n",
    "    \n",
    "    def _compute_next_q_values(self, next_time_steps):\n",
    "        next_q_values, _ = self._q_network(next_time_steps.observation,\n",
    "                                           next_time_steps.step_type)\n",
    "        best_next_actions = tf.cast(\n",
    "            tf.argmax(input=next_q_values, axis=-1), dtype=tf.int32)\n",
    "        next_target_q_values, _ = self._target_q_network(\n",
    "            next_time_steps.observation, next_time_steps.step_type)\n",
    "        multi_dim_actions = best_next_actions.shape.ndims > 1\n",
    "        return common.index_with_actions(\n",
    "            next_target_q_values,\n",
    "            best_next_actions,\n",
    "            multi_dim_actions=multi_dim_actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.compat.v2.Variable(0)\n",
    "\n",
    "dqn_agent = MyDqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "dqn_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = dqn_agent.policy\n",
    "collect_policy = dqn_agent.collect_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=dqn_agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    replay_buffer.add_batch(traj)\n",
    "\n",
    "for _ in range(initial_collect_steps):\n",
    "    collect_step(train_env, random_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, sample_batch_size=batch_size, num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0712 08:00:00.513118 4439340480 deprecation.py:323] From /Users/jwatts/TensorflowProjects/lib/python3.7/site-packages/tf_agents/policies/epsilon_greedy_policy.py:96: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no state spec, map structure\n",
      "multi_dim_actions=False\n",
      "no state spec, map structure\n",
      "multi_dim_actions=False\n",
      "step = 200: loss = 76.5986328125\n",
      "step = 400: loss = 330.82330322265625\n",
      "step = 600: loss = 43.20022964477539\n",
      "step = 800: loss = 31.626176834106445\n",
      "step = 1000: loss = 77.24769592285156\n",
      "step = 1000: Average Return = 58.900001525878906\n",
      "step = 1200: loss = 94.8570785522461\n",
      "step = 1400: loss = 179.28231811523438\n",
      "step = 1600: loss = 572.884033203125\n",
      "step = 1800: loss = 83.27125549316406\n",
      "step = 2000: loss = 287.8370056152344\n",
      "step = 2000: Average Return = 55.5\n",
      "step = 2200: loss = 751.4537353515625\n",
      "step = 2400: loss = 128.01840209960938\n",
      "step = 2600: loss = 280.87744140625\n",
      "step = 2800: loss = 192.85565185546875\n",
      "step = 3000: loss = 1739.916259765625\n",
      "step = 3000: Average Return = 209.39999389648438\n",
      "step = 3200: loss = 1081.7496337890625\n",
      "step = 3400: loss = 145.79666137695312\n",
      "step = 3600: loss = 548.35205078125\n",
      "step = 3800: loss = 187.66607666015625\n",
      "step = 4000: loss = 1089.8914794921875\n",
      "step = 4000: Average Return = 300.0\n",
      "step = 4200: loss = 19171.560546875\n",
      "step = 4400: loss = 192.71090698242188\n",
      "step = 4600: loss = 164.71527099609375\n",
      "step = 4800: loss = 5214.08349609375\n",
      "step = 5000: loss = 734.4244384765625\n",
      "step = 5000: Average Return = 444.29998779296875\n",
      "step = 5200: loss = 21346.88671875\n",
      "step = 5400: loss = 5590.04052734375\n",
      "step = 5600: loss = 309.7943115234375\n",
      "step = 5800: loss = 561.0391845703125\n",
      "step = 6000: loss = 413.5128479003906\n",
      "step = 6000: Average Return = 468.8999938964844\n",
      "step = 6200: loss = 405.3775634765625\n",
      "step = 6400: loss = 293.47332763671875\n",
      "step = 6600: loss = 672.7794189453125\n",
      "step = 6800: loss = 35842.0625\n",
      "step = 7000: loss = 6526.67822265625\n",
      "step = 7000: Average Return = 312.1000061035156\n",
      "step = 7200: loss = 5255.66455078125\n",
      "step = 7400: loss = 1284.371826171875\n",
      "step = 7600: loss = 1593.3712158203125\n",
      "step = 7800: loss = 1427.3306884765625\n",
      "step = 8000: loss = 4074.7138671875\n",
      "step = 8000: Average Return = 298.1000061035156\n",
      "step = 8200: loss = 7878.2734375\n",
      "step = 8400: loss = 1590.511962890625\n",
      "step = 8600: loss = 961.68896484375\n",
      "step = 8800: loss = 2028.3214111328125\n",
      "step = 9000: loss = 18244.88671875\n",
      "step = 9000: Average Return = 297.0\n",
      "step = 9200: loss = 4587.4794921875\n",
      "step = 9400: loss = 120143.2109375\n",
      "step = 9600: loss = 4409.90673828125\n",
      "step = 9800: loss = 26828.4140625\n",
      "step = 10000: loss = 11467.7216796875\n",
      "step = 10000: Average Return = 240.1999969482422\n",
      "step = 10200: loss = 9623.5556640625\n",
      "step = 10400: loss = 8488.220703125\n",
      "step = 10600: loss = 6478.955078125\n",
      "step = 10800: loss = 4547.52001953125\n",
      "step = 11000: loss = 7022.67431640625\n",
      "step = 11000: Average Return = 427.79998779296875\n",
      "step = 11200: loss = 5057.4443359375\n",
      "step = 11400: loss = 28017.013671875\n",
      "step = 11600: loss = 323546.03125\n",
      "step = 11800: loss = 22352.662109375\n",
      "step = 12000: loss = 229849.5625\n",
      "step = 12000: Average Return = 299.29998779296875\n",
      "step = 12200: loss = 21159.72265625\n",
      "step = 12400: loss = 296167.28125\n",
      "step = 12600: loss = 20243.705078125\n",
      "step = 12800: loss = 23048.5703125\n",
      "step = 13000: loss = 29054.404296875\n",
      "step = 13000: Average Return = 221.5\n",
      "step = 13200: loss = 372826.03125\n",
      "step = 13400: loss = 22123.3125\n",
      "step = 13600: loss = 32152.3203125\n",
      "step = 13800: loss = 23974.978515625\n",
      "step = 14000: loss = 46323.8515625\n",
      "step = 14000: Average Return = 446.0\n",
      "step = 14200: loss = 109034.65625\n",
      "step = 14400: loss = 66993.625\n",
      "step = 14600: loss = 64928.90625\n",
      "step = 14800: loss = 190939.71875\n",
      "step = 15000: loss = 68567.9140625\n",
      "step = 15000: Average Return = 321.8999938964844\n",
      "step = 15200: loss = 77996.75\n",
      "step = 15400: loss = 79872.609375\n",
      "step = 15600: loss = 89413.6328125\n",
      "step = 15800: loss = 89423.4921875\n",
      "step = 16000: loss = 199789.4375\n",
      "step = 16000: Average Return = 420.8999938964844\n",
      "step = 16200: loss = 73053.59375\n",
      "step = 16400: loss = 105189.2109375\n",
      "step = 16600: loss = 274635.53125\n",
      "step = 16800: loss = 121540.6484375\n",
      "step = 17000: loss = 118290.8984375\n",
      "step = 17000: Average Return = 500.0\n",
      "step = 17200: loss = 230992.5625\n",
      "step = 17400: loss = 684035.0625\n",
      "step = 17600: loss = 10285942.0\n",
      "step = 17800: loss = 1037377.6875\n",
      "step = 18000: loss = 1003127.375\n",
      "step = 18000: Average Return = 464.79998779296875\n",
      "step = 18200: loss = 771600.125\n",
      "step = 18400: loss = 1177533.75\n",
      "step = 18600: loss = 17334278.0\n",
      "step = 18800: loss = 5547891.5\n",
      "step = 19000: loss = 1280014.5\n",
      "step = 19000: Average Return = 422.6000061035156\n",
      "step = 19200: loss = 2911391.0\n",
      "step = 19400: loss = 2808028.5\n",
      "step = 19600: loss = 2215183.5\n",
      "step = 19800: loss = 1969761.875\n",
      "step = 20000: loss = 325602336.0\n",
      "step = 20000: Average Return = 436.79998779296875\n"
     ]
    }
   ],
   "source": [
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "dqn_agent.train = common.function(dqn_agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "dqn_agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, dqn_agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    for _ in range(collect_steps_per_iteration):\n",
    "        collect_step(train_env, dqn_agent.collect_policy)\n",
    "\n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = dqn_agent.train(experience)\n",
    "\n",
    "    step = dqn_agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss.loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, dqn_agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-14.919999599456787, 550)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(steps, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Step')\n",
    "plt.ylim(top=550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = eval_env.reset()\n",
    "while not time_step.is_last():\n",
    "    action_step = dqn_agent.policy.action(time_step)\n",
    "    time_step = eval_env.step(action_step.action)\n",
    "    eval_py_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
