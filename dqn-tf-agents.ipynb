{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'  # @param\n",
    "num_iterations = 20000  # @param\n",
    "\n",
    "initial_collect_steps = 1000  # @param\n",
    "collect_steps_per_iteration = 1  # @param\n",
    "replay_buffer_capacity = 100000  # @param\n",
    "\n",
    "batch_size = 64  # @param\n",
    "learning_rate = 1e-3  # @param\n",
    "log_interval = 200  # @param\n",
    "\n",
    "num_eval_episodes = 10  # @param\n",
    "eval_interval = 1000  # @param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = suite_gym.load(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.specs import tensor_spec\n",
    "\n",
    "class DDQN(tf.keras.Model):\n",
    "    def __init__(self, input_tensor_spec, state_spec, name=\"DDQN\"):\n",
    "        super(DDQN, self).__init__()\n",
    "        \n",
    "        self._input_tensor_spec = input_tensor_spec\n",
    "        self._state_spec = ()\n",
    "        self.init_state_spec = state_spec\n",
    "        \n",
    "        hidden_units = 32\n",
    "        action_spec = tf.nest.flatten(state_spec)[0]\n",
    "        num_actions = action_spec.maximum - action_spec.minimum + 1\n",
    "        \n",
    "        # Shared layers\n",
    "        self._shared1 = tf.keras.layers.Dense(hidden_units, \n",
    "                         activation='relu', \n",
    "                         kernel_initializer='he_uniform')\n",
    "     \n",
    "        self._shared2 = tf.keras.layers.Dense(hidden_units, \n",
    "                         activation='relu', \n",
    "                         kernel_initializer='he_uniform')\n",
    "\n",
    "        # Learn advantage\n",
    "        self._adv1 = tf.keras.layers.Dense(hidden_units, \n",
    "                         activation='relu', \n",
    "                         kernel_initializer='he_uniform')\n",
    "        self._adv2 = tf.keras.layers.Dense(num_actions, \n",
    "                         activation='linear', \n",
    "                         kernel_initializer='he_uniform')\n",
    "        self._adv3 = tf.keras.layers.Lambda(lambda a: a - tf.keras.backend.mean(a, axis=1, keepdims=True), \n",
    "                                            output_shape=(num_actions,))\n",
    "        \n",
    "        # Learn value\n",
    "        self._value1 = tf.keras.layers.Dense(hidden_units, \n",
    "                         activation='relu', \n",
    "                         kernel_initializer='he_uniform')\n",
    "        self._value2 = tf.keras.layers.Dense(1, \n",
    "                         activation='linear', \n",
    "                         kernel_initializer='he_uniform')\n",
    "        \n",
    "        # Combine into Q\n",
    "        self._q_out = tf.keras.layers.Add()\n",
    "\n",
    "    def call(self, observation, step_type=None, network_state=()):\n",
    "        shared = self._shared1(observation)\n",
    "        shared = self._shared2(shared)\n",
    "        adv = self._adv1(shared)\n",
    "        adv = self._adv2(adv)\n",
    "        adv = self._adv3(adv)\n",
    "        value = self._value1(shared)\n",
    "        value = self._value2(shared)\n",
    "        q_out = self._q_out([adv, value])\n",
    "        return q_out, network_state\n",
    "    \n",
    "    def copy(self, name):\n",
    "        return type(self)(self._input_tensor_spec, self.init_state_spec, name)\n",
    "    \n",
    "    @property\n",
    "    def state_spec(self):\n",
    "        return self._state_spec\n",
    "\n",
    "    def _build(self):\n",
    "        if not self.built and self.input_tensor_spec is not None:\n",
    "            random_input = tensor_spec.sample_spec_nest(self.input_tensor_spec, outer_dims=(1,))\n",
    "            #step_type = tf.expand_dims(time_step.StepType.FIRST, 0)\n",
    "            self.__call__(random_input, None, None)\n",
    "\n",
    "    @property\n",
    "    def input_tensor_spec(self):\n",
    "        return self._input_tensor_spec\n",
    "\n",
    "    @property\n",
    "    def variables(self):\n",
    "        try:\n",
    "            self._build()\n",
    "        except ValueError as e:\n",
    "            traceback = sys.exc_info()[2]\n",
    "            six.reraise(\n",
    "                ValueError, \"Failed to call build on the network when accessing \"\n",
    "                \"variables. Message: {!r}.\".format(e), traceback)\n",
    "        return self.weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_net = DDQN(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from tf_agents.policies import epsilon_greedy_policy\n",
    "from tf_agents.policies import greedy_policy\n",
    "from tf_agents.policies import q_policy\n",
    "from tf_agents.utils import nest_utils\n",
    "\n",
    "LossInfo = collections.namedtuple(\"LossInfo\", (\"loss\", \"extra\"))\n",
    "\n",
    "def element_wise_squared_loss(x, y):\n",
    "    return tf.compat.v1.losses.mean_squared_error(\n",
    "      x, y, reduction=tf.compat.v1.losses.Reduction.NONE)\n",
    "\n",
    "\n",
    "def element_wise_huber_loss(x, y):\n",
    "    return tf.compat.v1.losses.huber_loss(\n",
    "      x, y, reduction=tf.compat.v1.losses.Reduction.NONE)\n",
    "\n",
    "\n",
    "def compute_td_targets(next_q_values, rewards, discounts):\n",
    "    return tf.stop_gradient(rewards + discounts * next_q_values)\n",
    "\n",
    "\n",
    "class DqnLossInfo(collections.namedtuple('DqnLossInfo',\n",
    "                                         ('td_loss', 'td_error'))):\n",
    "    pass\n",
    "\n",
    "class MyDqnAgent:\n",
    "    def __init__(\n",
    "      self,\n",
    "      time_step_spec,\n",
    "      action_spec,\n",
    "      q_network,\n",
    "      optimizer,\n",
    "      epsilon_greedy=0.1,\n",
    "      # Params for target network updates\n",
    "      target_update_tau=1.0,\n",
    "      target_update_period=1,\n",
    "      # Params for training.\n",
    "      gamma=1.0,\n",
    "      train_step_counter=None,\n",
    "      name=None):\n",
    "\n",
    "        flat_action_spec = tf.nest.flatten(action_spec)\n",
    "        self._num_actions = [\n",
    "            spec.maximum - spec.minimum + 1 for spec in flat_action_spec\n",
    "        ]\n",
    "\n",
    "        self._q_network = q_network\n",
    "        self._target_q_network = self._q_network.copy(name='TargetQNetwork')\n",
    "        self._epsilon_greedy = epsilon_greedy\n",
    "        self._optimizer = optimizer\n",
    "        self._gamma = gamma\n",
    "        self._update_target = self._get_target_updater(target_update_tau, target_update_period)\n",
    "        self._train_step_counter = train_step_counter\n",
    "\n",
    "        policy = q_policy.QPolicy(\n",
    "            time_step_spec,\n",
    "            action_spec,\n",
    "            q_network=self._q_network,\n",
    "            emit_log_probability=False)\n",
    "\n",
    "        self._collect_policy = epsilon_greedy_policy.EpsilonGreedyPolicy(policy, epsilon=self._epsilon_greedy)\n",
    "        self._policy = greedy_policy.GreedyPolicy(policy)\n",
    "\n",
    "    def initialize(self):\n",
    "        common.soft_variables_update(\n",
    "            self._q_network.variables, self._target_q_network.variables, tau=1.0)\n",
    "\n",
    "    @property\n",
    "    def policy(self):\n",
    "        return self._policy\n",
    "\n",
    "    @property\n",
    "    def collect_policy(self):\n",
    "        return self._collect_policy\n",
    "    \n",
    "    @property\n",
    "    def collect_data_spec(self):\n",
    "        return self.collect_policy.trajectory_spec\n",
    "    \n",
    "    @property\n",
    "    def train_step_counter(self):\n",
    "        return self._train_step_counter\n",
    "        \n",
    "    def _get_target_updater(self, tau=1.0, period=1):\n",
    "        with tf.name_scope('update_targets'):\n",
    "\n",
    "            def update():\n",
    "                return common.soft_variables_update(\n",
    "                self._q_network.variables, self._target_q_network.variables, tau)\n",
    "\n",
    "            return common.Periodically(update, period, 'periodic_update_targets')\n",
    "\n",
    "    def _experience_to_transitions(self, experience):\n",
    "        transitions = trajectory.to_transition(experience)\n",
    "\n",
    "        # Remove time dim if we are not using a recurrent network.\n",
    "        if not self._q_network.state_spec:\n",
    "            print(\"no state spec, map structure\")\n",
    "            transitions = tf.nest.map_structure(lambda x: tf.squeeze(x, [1]),\n",
    "                                          transitions)\n",
    "\n",
    "        time_steps, policy_steps, next_time_steps = transitions\n",
    "        actions = policy_steps.action\n",
    "        return time_steps, actions, next_time_steps\n",
    "\n",
    "    # Use @common.function in graph mode or for speeding up.\n",
    "    def train(self, experience, weights=None):\n",
    "        with tf.GradientTape() as tape:\n",
    "              loss_info = self._loss(experience)\n",
    "        \n",
    "        variables_to_train = self._q_network.trainable_weights\n",
    "\n",
    "        grads = tape.gradient(loss_info.loss, variables_to_train)\n",
    "\n",
    "        grads_and_vars = tuple(zip(grads, variables_to_train))\n",
    "\n",
    "        self._optimizer.apply_gradients(grads_and_vars,\n",
    "                                    global_step=self.train_step_counter)\n",
    "\n",
    "        self._update_target()\n",
    "\n",
    "        return loss_info\n",
    "\n",
    "    def _loss(self, experience):\n",
    "        time_steps, actions, next_time_steps = self._experience_to_transitions(experience)\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            actions = tf.nest.flatten(actions)[0]\n",
    "            q_values, _ = self._q_network(time_steps.observation,\n",
    "                                    time_steps.step_type)\n",
    "\n",
    "            q_values = common.index_with_actions(\n",
    "                q_values,\n",
    "                tf.cast(actions, dtype=tf.int32))\n",
    "\n",
    "            next_q_values = self._compute_next_q_values(next_time_steps)\n",
    "\n",
    "            td_targets = compute_td_targets(\n",
    "                next_q_values,\n",
    "                rewards=next_time_steps.reward,\n",
    "                discounts=self._gamma * next_time_steps.discount)\n",
    "\n",
    "            valid_mask = tf.cast(~time_steps.is_last(), tf.float32)\n",
    "            td_error = valid_mask * (td_targets - q_values)\n",
    "\n",
    "            td_loss = valid_mask * element_wise_squared_loss(td_targets, q_values)\n",
    "\n",
    "            loss = tf.reduce_mean(input_tensor=td_loss)\n",
    "\n",
    "            return LossInfo(loss, DqnLossInfo(td_loss=td_loss,\n",
    "                                                 td_error=td_error))\n",
    "\n",
    "    #def _compute_next_q_values(self, next_time_steps):\n",
    "    #    next_target_q_values, _ = self._target_q_network(\n",
    "    #    next_time_steps.observation, next_time_steps.step_type)\n",
    "    #    # Reduce_max below assumes q_values are [BxF] or [BxTxF]\n",
    "    #    assert next_target_q_values.shape.ndims in [2, 3]\n",
    "    #    return tf.reduce_max(input_tensor=next_target_q_values, axis=-1)\n",
    "    \n",
    "    def _compute_next_q_values(self, next_time_steps):\n",
    "        next_q_values, _ = self._q_network(next_time_steps.observation,\n",
    "                                           next_time_steps.step_type)\n",
    "        \n",
    "        best_next_actions = tf.cast(\n",
    "            tf.argmax(input=next_q_values, axis=-1), dtype=tf.int32)\n",
    "        \n",
    "        next_target_q_values, _ = self._target_q_network(\n",
    "            next_time_steps.observation, next_time_steps.step_type)\n",
    "        \n",
    "        multi_dim_actions = best_next_actions.shape.ndims > 1\n",
    "        \n",
    "        return common.index_with_actions(\n",
    "            next_target_q_values,\n",
    "            best_next_actions,\n",
    "            multi_dim_actions=multi_dim_actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)), observation=BoundedTensorSpec(shape=(4,), dtype=tf.float32, name=None, minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
       "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
       "      dtype=float32)))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env.time_step_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.compat.v2.Variable(0)\n",
    "\n",
    "dqn_agent = MyDqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    train_step_counter=train_step_counter)\n",
    "dqn_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = dqn_agent.policy\n",
    "collect_policy = dqn_agent.collect_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=dqn_agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    replay_buffer.add_batch(traj)\n",
    "\n",
    "for _ in range(initial_collect_steps):\n",
    "    collect_step(train_env, random_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, sample_batch_size=batch_size, num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0719 11:00:45.018661 4374353344 deprecation.py:323] From /Users/jwatts/TensorflowProjects/lib/python3.7/site-packages/tf_agents/policies/epsilon_greedy_policy.py:96: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no state spec, map structure\n",
      "no state spec, map structure\n",
      "step = 200: loss = 44.182376861572266\n",
      "step = 400: loss = 56.318115234375\n",
      "step = 600: loss = 24.0237979888916\n",
      "step = 800: loss = 32.79742431640625\n",
      "step = 1000: loss = 30.245777130126953\n",
      "step = 1000: Average Return = 100.69999694824219\n",
      "step = 1200: loss = 14.691826820373535\n",
      "step = 1400: loss = 38.07110595703125\n",
      "step = 1600: loss = 239.21841430664062\n",
      "step = 1800: loss = 545.8086547851562\n",
      "step = 2000: loss = 399.9332275390625\n",
      "step = 2000: Average Return = 108.69999694824219\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 2000\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "dqn_agent.train = common.function(dqn_agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "dqn_agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, dqn_agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    for _ in range(collect_steps_per_iteration):\n",
    "        collect_step(train_env, dqn_agent.collect_policy)\n",
    "\n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = dqn_agent.train(experience)\n",
    "\n",
    "    step = dqn_agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss.loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, dqn_agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.69000015258789, 550)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(steps, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Step')\n",
    "plt.ylim(top=550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'q0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-eb5744aaaf55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'q0' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.plot(q0)\n",
    "plt.plot(q1)\n",
    "plt.plot(q2)\n",
    "plt.plot(q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = eval_env.reset()\n",
    "while not time_step.is_last():\n",
    "    action_step = dqn_agent.policy.action(time_step)\n",
    "    time_step = eval_env.step(action_step.action)\n",
    "    eval_py_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
