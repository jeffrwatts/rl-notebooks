{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import dqn_agent\n",
    "#from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "#from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'  # @param\n",
    "num_iterations = 10000  # @param\n",
    "\n",
    "initial_collect_steps = 1000  # @param\n",
    "collect_steps_per_iteration = 1  # @param\n",
    "replay_buffer_capacity = 100000  # @param\n",
    "\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "batch_size = 64  # @param\n",
    "learning_rate = 1e-3  # @param\n",
    "log_interval = 200  # @param\n",
    "\n",
    "num_eval_episodes = 10  # @param\n",
    "eval_interval = 1000  # @param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = suite_gym.load(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "BoundedArraySpec(shape=(4,), dtype=dtype('float32'), name=None, minimum=[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], maximum=[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38])\n",
      "Action Spec:\n",
      "BoundedArraySpec(shape=(), dtype=dtype('int64'), name=None, minimum=0, maximum=1)\n"
     ]
    }
   ],
   "source": [
    "print('Observation Spec:')\n",
    "print(env.time_step_spec().observation)\n",
    "print('Action Spec:')\n",
    "print(env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.networks import network\n",
    "\n",
    "class DDQN(network.Network):\n",
    "    def __init__(self, observation_spec, action_spec, name=\"DDQN\"):\n",
    "        super(DDQN, self).__init__(\n",
    "            input_tensor_spec=observation_spec,\n",
    "            state_spec=(),\n",
    "            name=name)    \n",
    "        \n",
    "        hidden_units = 32\n",
    "        action_spec = tf.nest.flatten(action_spec)[0]\n",
    "        num_actions = action_spec.maximum - action_spec.minimum + 1\n",
    "        \n",
    "        # Shared layers\n",
    "        self._shared1 = tf.keras.layers.Dense(hidden_units, \n",
    "                         activation='relu', \n",
    "                         kernel_initializer='he_uniform')\n",
    "     \n",
    "        self._shared2 = tf.keras.layers.Dense(hidden_units, \n",
    "                         activation='relu', \n",
    "                         kernel_initializer='he_uniform')\n",
    "\n",
    "        # Learn advantage\n",
    "        self._adv1 = tf.keras.layers.Dense(hidden_units, \n",
    "                         activation='relu', \n",
    "                         kernel_initializer='he_uniform')\n",
    "        self._adv2 = tf.keras.layers.Dense(num_actions, \n",
    "                         activation='linear', \n",
    "                         kernel_initializer='he_uniform')\n",
    "        self._adv3 = tf.keras.layers.Lambda(lambda a: a - tf.keras.backend.mean(a, axis=1, keepdims=True), \n",
    "                                            output_shape=(num_actions,))\n",
    "        \n",
    "        # Learn value\n",
    "        self._value1 = tf.keras.layers.Dense(hidden_units, \n",
    "                         activation='relu', \n",
    "                         kernel_initializer='he_uniform')\n",
    "        self._value2 = tf.keras.layers.Dense(1, \n",
    "                         activation='linear', \n",
    "                         kernel_initializer='he_uniform')\n",
    "        \n",
    "        # Combine into Q\n",
    "        self._q_out = tf.keras.layers.Add()\n",
    "\n",
    "    def call(self, observation, step_type=None, network_state=()):\n",
    "        shared = self._shared1(observation)\n",
    "        shared = self._shared2(shared)\n",
    "        adv = self._adv1(shared)\n",
    "        adv = self._adv2(adv)\n",
    "        adv = self._adv3(adv)\n",
    "        value = self._value1(shared)\n",
    "        value = self._value2(shared)\n",
    "        q_out = self._q_out([adv, value])\n",
    "        return q_out, network_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_net = DDQN(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec())\n",
    "\n",
    "#q_net = q_network.QNetwork(\n",
    "#    train_env.observation_spec(),\n",
    "#    train_env.action_spec(),\n",
    "#    fc_layer_params=fc_layer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collect policy is e-greedy\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.compat.v2.Variable(0)\n",
    "\n",
    "tf_agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=dqn_agent.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = tf_agent.policy\n",
    "collect_policy = tf_agent.collect_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=tf_agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    replay_buffer.add_batch(traj)\n",
    "\n",
    "for _ in range(initial_collect_steps):\n",
    "    collect_step(train_env, random_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, sample_batch_size=batch_size, num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0710 07:38:04.869653 4437706176 deprecation.py:323] From /Users/jwatts/TensorflowProjects/lib/python3.7/site-packages/tf_agents/policies/epsilon_greedy_policy.py:96: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "td_errors_loss_fn=<function element_wise_squared_loss at 0x137364e18>\n",
      "_n_step_update=1\n",
      "no state spec, map structure\n",
      "multi_dim_actions=False\n",
      "reward_scale_factor=1.0; gamma=1.0\n",
      "td_errors_loss_fn=<function element_wise_squared_loss at 0x137364e18>\n",
      "_n_step_update=1\n",
      "no state spec, map structure\n",
      "multi_dim_actions=False\n",
      "reward_scale_factor=1.0; gamma=1.0\n",
      "step = 200: loss = 32.989261627197266\n",
      "step = 400: loss = 234.93902587890625\n",
      "step = 600: loss = 76.19415283203125\n",
      "step = 800: loss = 31.17449188232422\n",
      "step = 1000: loss = 114.59101867675781\n",
      "step = 1000: Average Return = 33.0\n",
      "step = 1200: loss = 56.44641876220703\n",
      "step = 1400: loss = 297.5062561035156\n",
      "step = 1600: loss = 255.69776916503906\n",
      "step = 1800: loss = 546.0511474609375\n",
      "step = 2000: loss = 151.6051483154297\n",
      "step = 2000: Average Return = 99.69999694824219\n",
      "step = 2200: loss = 747.3446044921875\n",
      "step = 2400: loss = 279.80804443359375\n",
      "step = 2600: loss = 1948.718017578125\n",
      "step = 2800: loss = 336.2585144042969\n",
      "step = 3000: loss = 746.2692260742188\n",
      "step = 3000: Average Return = 160.6999969482422\n",
      "step = 3200: loss = 4389.669921875\n",
      "step = 3400: loss = 10443.9736328125\n",
      "step = 3600: loss = 791.9422607421875\n",
      "step = 3800: loss = 638.9728393554688\n",
      "step = 4000: loss = 33149.02734375\n",
      "step = 4000: Average Return = 264.70001220703125\n",
      "step = 4200: loss = 1973.34375\n",
      "step = 4400: loss = 57942.59375\n",
      "step = 4600: loss = 1696.644775390625\n",
      "step = 4800: loss = 26441.802734375\n",
      "step = 5000: loss = 4028.06640625\n",
      "step = 5000: Average Return = 206.10000610351562\n",
      "step = 5200: loss = 1749.6678466796875\n",
      "step = 5400: loss = 1847.5538330078125\n",
      "step = 5600: loss = 100991.25\n",
      "step = 5800: loss = 84826.640625\n",
      "step = 6000: loss = 10952.5732421875\n",
      "step = 6000: Average Return = 220.10000610351562\n",
      "step = 6200: loss = 318198.125\n",
      "step = 6400: loss = 28914.9609375\n",
      "step = 6600: loss = 53411.9765625\n",
      "step = 6800: loss = 9285.1328125\n",
      "step = 7000: loss = 7076.423828125\n",
      "step = 7000: Average Return = 435.70001220703125\n",
      "step = 7200: loss = 16120.55859375\n",
      "step = 7400: loss = 386416.75\n",
      "step = 7600: loss = 40990.93359375\n",
      "step = 7800: loss = 10398.078125\n",
      "step = 8000: loss = 15513.3515625\n",
      "step = 8000: Average Return = 475.1000061035156\n",
      "step = 8200: loss = 5793.177734375\n",
      "step = 8400: loss = 68900.734375\n",
      "step = 8600: loss = 118266.984375\n",
      "step = 8800: loss = 10980.5341796875\n",
      "step = 9000: loss = 4702.02783203125\n",
      "step = 9000: Average Return = 405.5\n",
      "step = 9200: loss = 65428.19140625\n",
      "step = 9400: loss = 14219.0\n",
      "step = 9600: loss = 8011.04345703125\n",
      "step = 9800: loss = 13714.8046875\n",
      "step = 10000: loss = 16897.88671875\n",
      "step = 10000: Average Return = 439.0\n"
     ]
    }
   ],
   "source": [
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "tf_agent.train = common.function(tf_agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "tf_agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    for _ in range(collect_steps_per_iteration):\n",
    "        collect_step(train_env, tf_agent.collect_policy)\n",
    "\n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = tf_agent.train(experience)\n",
    "\n",
    "    step = tf_agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss.loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.694999694824219, 550)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(steps, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Step')\n",
    "plt.ylim(top=550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = eval_env.reset()\n",
    "while not time_step.is_last():\n",
    "    action_step = tf_agent.policy.action(time_step)\n",
    "    time_step = eval_env.step(action_step.action)\n",
    "    eval_py_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _experience_to_transitions(experience):\n",
    "    transitions = trajectory.to_transition(experience)\n",
    "\n",
    "    transitions = tf.nest.map_structure(lambda x: tf.squeeze(x, [1]), transitions)\n",
    "\n",
    "    time_steps, policy_steps, next_time_steps = transitions\n",
    "    actions = policy_steps.action\n",
    "    return time_steps, actions, next_time_steps\n",
    "\n",
    "def element_wise_squared_loss(x, y):\n",
    "    return tf.compat.v1.losses.mean_squared_error(x, y, reduction=tf.compat.v1.losses.Reduction.NONE)\n",
    "\n",
    "def element_wise_huber_loss(x, y):\n",
    "    return tf.compat.v1.losses.huber_loss(x, y, reduction=tf.compat.v1.losses.Reduction.NONE)\n",
    "\n",
    "def compute_td_targets(next_q_values, rewards, discounts):\n",
    "    return tf.stop_gradient(rewards + discounts * next_q_values)\n",
    "\n",
    "def index_with_actions(q_values, actions, multi_dim_actions=False):\n",
    "    if actions.shape.ndims is None:\n",
    "        raise ValueError('actions should have known rank.')\n",
    "    \n",
    "    batch_dims = actions.shape.ndims\n",
    "    if multi_dim_actions:\n",
    "        # In the multidimensional case, the last dimension of actions indexes the\n",
    "        # vector of actions for each batch, so exclude it from the batch dimensions.\n",
    "        batch_dims -= 1\n",
    "\n",
    "    outer_shape = tf.shape(input=actions)\n",
    "    batch_indices = tf.meshgrid(*[tf.range(outer_shape[i]) for i in range(batch_dims)], indexing='ij')\n",
    "    batch_indices = [\n",
    "        tf.expand_dims(batch_index, -1) for batch_index in batch_indices\n",
    "    ]\n",
    "    \n",
    "    if not multi_dim_actions:\n",
    "        actions = tf.expand_dims(actions, -1)\n",
    "        \n",
    "    action_indices = tf.concat(batch_indices + [actions], -1)\n",
    "    return tf.gather_nd(q_values, action_indices)\n",
    "\n",
    "def _compute_next_q_values(self, next_time_steps):\n",
    "    next_target_q_values, _ = self._target_q_network(next_time_steps.observation, next_time_steps.step_type)\n",
    "    return tf.reduce_max(input_tensor=next_target_q_values, axis=-1)\n",
    "\n",
    "def _loss(self, experience, reward_scale_factor=1.0, gamma = 1.0):\n",
    "    time_steps, actions, next_time_steps = self._experience_to_transitions(experience)\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "        actions = tf.nest.flatten(actions)[0]\n",
    "        q_values, _ = self._q_network(time_steps.observation, time_steps.step_type)\n",
    "\n",
    "        print(\"ndims={}\".format(tf.nest.flatten(self._action_spec)[0].shape.ndims))\n",
    "        multi_dim_actions = tf.nest.flatten(self._action_spec)[0].shape.ndims > 0\n",
    "        q_values = index_with_actions(q_values,\n",
    "                                      tf.cast(actions, dtype=tf.int32),\n",
    "                                      multi_dim_actions=multi_dim_actions)\n",
    "\n",
    "        next_q_values = self._compute_next_q_values(next_time_steps)\n",
    "\n",
    "        td_targets = compute_td_targets(next_q_values,\n",
    "                                        rewards=reward_scale_factor * next_time_steps.reward,\n",
    "                                        discounts=gamma * next_time_steps.discount)\n",
    "\n",
    "        valid_mask = tf.cast(~time_steps.is_last(), tf.float32)\n",
    "        td_error = valid_mask * (td_targets - q_values)\n",
    "\n",
    "        td_loss = valid_mask * element_wise_squared_loss(td_targets, q_values)\n",
    "\n",
    "        loss = tf.reduce_mean(input_tensor=td_loss)\n",
    "\n",
    "        return loss, td_loss, td_error\n",
    "    \n",
    "def _train(self, experience):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, td_loss, td_error = _loss(self, experience)\n",
    "    variables_to_train = self._q_network.trainable_weights\n",
    "    grads = tape.gradient(loss, variables_to_train)\n",
    "    # Tuple is used for py3, where zip is a generator producing values once.\n",
    "    grads_and_vars = tuple(zip(grads, variables_to_train))\n",
    "    self._optimizer.apply_gradients(grads_and_vars,\n",
    "                                    global_step=self.train_step_counter)\n",
    "\n",
    "    self._update_target()\n",
    "\n",
    "    return loss, td_loss, td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experience, unused_info = next(iterator)\n",
    "\n",
    "loss, td_loss, td_error = _train(tf_agent, experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss)\n",
    "print(td_loss)\n",
    "print(td_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
