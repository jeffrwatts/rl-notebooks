{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import dqn_agent\n",
    "#from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "#from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'  # @param\n",
    "num_iterations = 10000  # @param\n",
    "\n",
    "initial_collect_steps = 1000  # @param\n",
    "collect_steps_per_iteration = 1  # @param\n",
    "replay_buffer_capacity = 100000  # @param\n",
    "\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "batch_size = 64  # @param\n",
    "learning_rate = 1e-3  # @param\n",
    "log_interval = 200  # @param\n",
    "\n",
    "num_eval_episodes = 10  # @param\n",
    "eval_interval = 1000  # @param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = suite_gym.load(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Observation Spec:')\n",
    "print(env.time_step_spec().observation)\n",
    "print('Action Spec:')\n",
    "print(env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import six\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import time_step\n",
    "from tensorflow.python.keras.engine import network as keras_network  # TF internal\n",
    "from tensorflow.python.util import tf_decorator  # TF internal\n",
    "from tensorflow.python.util import tf_inspect  # TF internal\n",
    "\n",
    "class _NetworkMeta(abc.ABCMeta):\n",
    "    def __new__(mcs, classname, baseclasses, attrs):\n",
    "        if baseclasses[0] == keras_network.Network:\n",
    "            # This is just Network below.  Return early.\n",
    "            return abc.ABCMeta.__new__(mcs, classname, baseclasses, attrs)\n",
    "\n",
    "        init = attrs.get(\"__init__\", None)\n",
    "\n",
    "        if not init:\n",
    "            # This wrapper class does not define an __init__.  When someone creates\n",
    "            # the object, the __init__ of its parent class will be called.  We will\n",
    "            # call that __init__ instead separately since the parent class is also a\n",
    "            # subclass of Network.  Here just create the class and return.\n",
    "            return abc.ABCMeta.__new__(mcs, classname, baseclasses, attrs)\n",
    "\n",
    "        arg_spec = tf_inspect.getargspec(init)\n",
    "        if arg_spec.varargs is not None:\n",
    "            raise RuntimeError(\n",
    "                \"%s.__init__ function accepts *args.  This is not allowed.\" %\n",
    "                classname)\n",
    "\n",
    "        def capture_init(self, *args, **kwargs):\n",
    "            if len(args) > len(arg_spec.args) + 1:\n",
    "                # Error case: more inputs than args.  Call init so that the appropriate\n",
    "                # error can be raised to the user.\n",
    "                init(self, *args, **kwargs)\n",
    "            for i, arg in enumerate(args):\n",
    "                # Add +1 to skip `self` in arg_spec.args.\n",
    "                kwargs[arg_spec.args[1 + i]] = arg\n",
    "            init(self, **kwargs)\n",
    "            setattr(self, \"_saved_kwargs\", kwargs)\n",
    "\n",
    "        attrs[\"__init__\"] = tf_decorator.make_decorator(init, capture_init)\n",
    "        return abc.ABCMeta.__new__(mcs, classname, baseclasses, attrs)\n",
    "\n",
    "\n",
    "@six.add_metaclass(_NetworkMeta)\n",
    "class Network(keras_network.Network):\n",
    "    def __init__(self, input_tensor_spec, state_spec, name):\n",
    "        super(Network, self).__init__(name=name)\n",
    "        self._input_tensor_spec = input_tensor_spec\n",
    "        self._state_spec = state_spec\n",
    "\n",
    "    @property\n",
    "    def state_spec(self):\n",
    "        return self._state_spec\n",
    "\n",
    "    def _build(self):\n",
    "        if not self.built and self.input_tensor_spec is not None:\n",
    "            random_input = tensor_spec.sample_spec_nest(self.input_tensor_spec, outer_dims=(1,))\n",
    "            step_type = tf.expand_dims(time_step.StepType.FIRST, 0)\n",
    "            self.__call__(random_input, step_type, None)\n",
    "\n",
    "    @property\n",
    "    def input_tensor_spec(self):\n",
    "        \"\"\"Returns the spec of the input to the network of type InputSpec.\"\"\"\n",
    "        return self._input_tensor_spec\n",
    "\n",
    "    @property\n",
    "    def variables(self):\n",
    "        \"\"\"Return the variables for all the network layers.\n",
    "        If the network hasn't been built, builds it on random input (generated\n",
    "        using self._input_tensor_spec) to build all the layers and their variables.\n",
    "        Raises:\n",
    "        ValueError:  If the network fails to build.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self._build()\n",
    "        except ValueError as e:\n",
    "            traceback = sys.exc_info()[2]\n",
    "            six.reraise(\n",
    "                ValueError, \"Failed to call build on the network when accessing \"\n",
    "                \"variables. Message: {!r}.\".format(e), traceback)\n",
    "        return self.weights\n",
    "\n",
    "    def copy(self, **kwargs):\n",
    "        print(\"self._saved_kwargs={}\".format(self._saved_kwargs))\n",
    "        return type(self)(**dict(self._saved_kwargs, **kwargs))\n",
    "\n",
    "    def __call__(self, inputs, *args, **kwargs):\n",
    "        tf.nest.assert_same_structure(inputs, self.input_tensor_spec)\n",
    "        return super(Network, self).__call__(inputs, *args, **kwargs)\n",
    "\n",
    "\n",
    "class DDQN(tf.keras.Model):\n",
    "    def __init__(self, input_tensor_spec, state_spec, name=\"DDQN\"):\n",
    "        super(DDQN, self).__init__()\n",
    "        \n",
    "        self._input_tensor_spec = input_tensor_spec\n",
    "        self._state_spec = ()\n",
    "        self.init_state_spec = state_spec\n",
    "        \n",
    "        hidden_units = 32\n",
    "        action_spec = tf.nest.flatten(state_spec)[0]\n",
    "        num_actions = action_spec.maximum - action_spec.minimum + 1\n",
    "        \n",
    "        # Shared layers\n",
    "        self._shared1 = tf.keras.layers.Dense(hidden_units, \n",
    "                         activation='relu', \n",
    "                         kernel_initializer='he_uniform')\n",
    "     \n",
    "        self._shared2 = tf.keras.layers.Dense(hidden_units, \n",
    "                         activation='relu', \n",
    "                         kernel_initializer='he_uniform')\n",
    "\n",
    "        # Learn advantage\n",
    "        self._adv1 = tf.keras.layers.Dense(hidden_units, \n",
    "                         activation='relu', \n",
    "                         kernel_initializer='he_uniform')\n",
    "        self._adv2 = tf.keras.layers.Dense(num_actions, \n",
    "                         activation='linear', \n",
    "                         kernel_initializer='he_uniform')\n",
    "        self._adv3 = tf.keras.layers.Lambda(lambda a: a - tf.keras.backend.mean(a, axis=1, keepdims=True), \n",
    "                                            output_shape=(num_actions,))\n",
    "        \n",
    "        # Learn value\n",
    "        self._value1 = tf.keras.layers.Dense(hidden_units, \n",
    "                         activation='relu', \n",
    "                         kernel_initializer='he_uniform')\n",
    "        self._value2 = tf.keras.layers.Dense(1, \n",
    "                         activation='linear', \n",
    "                         kernel_initializer='he_uniform')\n",
    "        \n",
    "        # Combine into Q\n",
    "        self._q_out = tf.keras.layers.Add()\n",
    "\n",
    "    def call(self, observation, step_type=None, network_state=()):\n",
    "        shared = self._shared1(observation)\n",
    "        shared = self._shared2(shared)\n",
    "        adv = self._adv1(shared)\n",
    "        adv = self._adv2(adv)\n",
    "        adv = self._adv3(adv)\n",
    "        value = self._value1(shared)\n",
    "        value = self._value2(shared)\n",
    "        q_out = self._q_out([adv, value])\n",
    "        return q_out, network_state\n",
    "    \n",
    "    def copy(self, name):\n",
    "        print(\"copy override\")\n",
    "        return type(self)(self._input_tensor_spec, self.init_state_spec, name)\n",
    "    \n",
    "    @property\n",
    "    def state_spec(self):\n",
    "        return self._state_spec\n",
    "\n",
    "    def _build(self):\n",
    "        if not self.built and self.input_tensor_spec is not None:\n",
    "            random_input = tensor_spec.sample_spec_nest(self.input_tensor_spec, outer_dims=(1,))\n",
    "            #step_type = tf.expand_dims(time_step.StepType.FIRST, 0)\n",
    "            self.__call__(random_input, None, None)\n",
    "\n",
    "    @property\n",
    "    def input_tensor_spec(self):\n",
    "        \"\"\"Returns the spec of the input to the network of type InputSpec.\"\"\"\n",
    "        return self._input_tensor_spec\n",
    "\n",
    "    @property\n",
    "    def variables(self):\n",
    "        \"\"\"Return the variables for all the network layers.\n",
    "        If the network hasn't been built, builds it on random input (generated\n",
    "        using self._input_tensor_spec) to build all the layers and their variables.\n",
    "        Raises:\n",
    "        ValueError:  If the network fails to build.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"before build:\")\n",
    "            print(self.weights)\n",
    "            self._build()\n",
    "            print(\"after build:\")\n",
    "            print(self.weights)\n",
    "        except ValueError as e:\n",
    "            traceback = sys.exc_info()[2]\n",
    "            six.reraise(\n",
    "                ValueError, \"Failed to call build on the network when accessing \"\n",
    "                \"variables. Message: {!r}.\".format(e), traceback)\n",
    "        return self.weights\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_net = DDQN(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec())\n",
    "\n",
    "#q_net = q_network.QNetwork(\n",
    "#    train_env.observation_spec(),\n",
    "#    train_env.action_spec(),\n",
    "#    fc_layer_params=fc_layer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.compat.v2.Variable(0)\n",
    "\n",
    "tf_agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=dqn_agent.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = tf_agent.policy\n",
    "collect_policy = tf_agent.collect_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=tf_agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    replay_buffer.add_batch(traj)\n",
    "\n",
    "for _ in range(initial_collect_steps):\n",
    "    collect_step(train_env, random_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, sample_batch_size=batch_size, num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "tf_agent.train = common.function(tf_agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "tf_agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    for _ in range(collect_steps_per_iteration):\n",
    "        collect_step(train_env, tf_agent.collect_policy)\n",
    "\n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = tf_agent.train(experience)\n",
    "\n",
    "    step = tf_agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss.loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(steps, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Step')\n",
    "plt.ylim(top=550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = eval_env.reset()\n",
    "while not time_step.is_last():\n",
    "    action_step = tf_agent.policy.action(time_step)\n",
    "    time_step = eval_env.step(action_step.action)\n",
    "    eval_py_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _experience_to_transitions(experience):\n",
    "    transitions = trajectory.to_transition(experience)\n",
    "\n",
    "    transitions = tf.nest.map_structure(lambda x: tf.squeeze(x, [1]), transitions)\n",
    "\n",
    "    time_steps, policy_steps, next_time_steps = transitions\n",
    "    actions = policy_steps.action\n",
    "    return time_steps, actions, next_time_steps\n",
    "\n",
    "def element_wise_squared_loss(x, y):\n",
    "    return tf.compat.v1.losses.mean_squared_error(x, y, reduction=tf.compat.v1.losses.Reduction.NONE)\n",
    "\n",
    "def element_wise_huber_loss(x, y):\n",
    "    return tf.compat.v1.losses.huber_loss(x, y, reduction=tf.compat.v1.losses.Reduction.NONE)\n",
    "\n",
    "def compute_td_targets(next_q_values, rewards, discounts):\n",
    "    return tf.stop_gradient(rewards + discounts * next_q_values)\n",
    "\n",
    "def index_with_actions(q_values, actions, multi_dim_actions=False):\n",
    "    if actions.shape.ndims is None:\n",
    "        raise ValueError('actions should have known rank.')\n",
    "    \n",
    "    batch_dims = actions.shape.ndims\n",
    "    if multi_dim_actions:\n",
    "        # In the multidimensional case, the last dimension of actions indexes the\n",
    "        # vector of actions for each batch, so exclude it from the batch dimensions.\n",
    "        batch_dims -= 1\n",
    "\n",
    "    outer_shape = tf.shape(input=actions)\n",
    "    batch_indices = tf.meshgrid(*[tf.range(outer_shape[i]) for i in range(batch_dims)], indexing='ij')\n",
    "    batch_indices = [\n",
    "        tf.expand_dims(batch_index, -1) for batch_index in batch_indices\n",
    "    ]\n",
    "    \n",
    "    if not multi_dim_actions:\n",
    "        actions = tf.expand_dims(actions, -1)\n",
    "        \n",
    "    action_indices = tf.concat(batch_indices + [actions], -1)\n",
    "    return tf.gather_nd(q_values, action_indices)\n",
    "\n",
    "def _compute_next_q_values(self, next_time_steps):\n",
    "    next_target_q_values, _ = self._target_q_network(next_time_steps.observation, next_time_steps.step_type)\n",
    "    return tf.reduce_max(input_tensor=next_target_q_values, axis=-1)\n",
    "\n",
    "def _loss(self, experience, reward_scale_factor=1.0, gamma = 1.0):\n",
    "    time_steps, actions, next_time_steps = self._experience_to_transitions(experience)\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "        actions = tf.nest.flatten(actions)[0]\n",
    "        q_values, _ = self._q_network(time_steps.observation, time_steps.step_type)\n",
    "\n",
    "        print(\"ndims={}\".format(tf.nest.flatten(self._action_spec)[0].shape.ndims))\n",
    "        multi_dim_actions = tf.nest.flatten(self._action_spec)[0].shape.ndims > 0\n",
    "        q_values = index_with_actions(q_values,\n",
    "                                      tf.cast(actions, dtype=tf.int32),\n",
    "                                      multi_dim_actions=multi_dim_actions)\n",
    "\n",
    "        next_q_values = self._compute_next_q_values(next_time_steps)\n",
    "\n",
    "        td_targets = compute_td_targets(next_q_values,\n",
    "                                        rewards=reward_scale_factor * next_time_steps.reward,\n",
    "                                        discounts=gamma * next_time_steps.discount)\n",
    "\n",
    "        valid_mask = tf.cast(~time_steps.is_last(), tf.float32)\n",
    "        td_error = valid_mask * (td_targets - q_values)\n",
    "\n",
    "        td_loss = valid_mask * element_wise_squared_loss(td_targets, q_values)\n",
    "\n",
    "        loss = tf.reduce_mean(input_tensor=td_loss)\n",
    "\n",
    "        return loss, td_loss, td_error\n",
    "    \n",
    "def _train(self, experience):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, td_loss, td_error = _loss(self, experience)\n",
    "    variables_to_train = self._q_network.trainable_weights\n",
    "    grads = tape.gradient(loss, variables_to_train)\n",
    "    # Tuple is used for py3, where zip is a generator producing values once.\n",
    "    grads_and_vars = tuple(zip(grads, variables_to_train))\n",
    "    self._optimizer.apply_gradients(grads_and_vars,\n",
    "                                    global_step=self.train_step_counter)\n",
    "\n",
    "    self._update_target()\n",
    "\n",
    "    return loss, td_loss, td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experience, unused_info = next(iterator)\n",
    "\n",
    "loss, td_loss, td_error = _train(tf_agent, experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss)\n",
    "print(td_loss)\n",
    "print(td_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
