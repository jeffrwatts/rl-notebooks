{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import dqn_agent\n",
    "#from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "#from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'  # @param\n",
    "num_iterations = 20000  # @param\n",
    "\n",
    "initial_collect_steps = 1000  # @param\n",
    "collect_steps_per_iteration = 1  # @param\n",
    "replay_buffer_capacity = 100000  # @param\n",
    "\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "batch_size = 64  # @param\n",
    "learning_rate = 1e-3  # @param\n",
    "log_interval = 200  # @param\n",
    "\n",
    "num_eval_episodes = 10  # @param\n",
    "eval_interval = 1000  # @param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = suite_gym.load(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "BoundedArraySpec(shape=(4,), dtype=dtype('float32'), name=None, minimum=[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], maximum=[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38])\n",
      "Action Spec:\n",
      "BoundedArraySpec(shape=(), dtype=dtype('int64'), name=None, minimum=0, maximum=1)\n"
     ]
    }
   ],
   "source": [
    "print('Observation Spec:')\n",
    "print(env.time_step_spec().observation)\n",
    "print('Action Spec:')\n",
    "print(env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.specs import tensor_spec\n",
    "\n",
    "class DDQN(tf.keras.Model):\n",
    "    def __init__(self, input_tensor_spec, state_spec, name=\"DDQN\"):\n",
    "        super(DDQN, self).__init__()\n",
    "        \n",
    "        self._input_tensor_spec = input_tensor_spec\n",
    "        self._state_spec = ()\n",
    "        self.init_state_spec = state_spec\n",
    "        \n",
    "        hidden_units = 32\n",
    "        action_spec = tf.nest.flatten(state_spec)[0]\n",
    "        num_actions = action_spec.maximum - action_spec.minimum + 1\n",
    "        \n",
    "        # Shared layers\n",
    "        self._shared1 = tf.keras.layers.Dense(hidden_units, \n",
    "                         activation='relu', \n",
    "                         kernel_initializer='he_uniform')\n",
    "     \n",
    "        self._shared2 = tf.keras.layers.Dense(hidden_units, \n",
    "                         activation='relu', \n",
    "                         kernel_initializer='he_uniform')\n",
    "\n",
    "        # Learn advantage\n",
    "        self._adv1 = tf.keras.layers.Dense(hidden_units, \n",
    "                         activation='relu', \n",
    "                         kernel_initializer='he_uniform')\n",
    "        self._adv2 = tf.keras.layers.Dense(num_actions, \n",
    "                         activation='linear', \n",
    "                         kernel_initializer='he_uniform')\n",
    "        self._adv3 = tf.keras.layers.Lambda(lambda a: a - tf.keras.backend.mean(a, axis=1, keepdims=True), \n",
    "                                            output_shape=(num_actions,))\n",
    "        \n",
    "        # Learn value\n",
    "        self._value1 = tf.keras.layers.Dense(hidden_units, \n",
    "                         activation='relu', \n",
    "                         kernel_initializer='he_uniform')\n",
    "        self._value2 = tf.keras.layers.Dense(1, \n",
    "                         activation='linear', \n",
    "                         kernel_initializer='he_uniform')\n",
    "        \n",
    "        # Combine into Q\n",
    "        self._q_out = tf.keras.layers.Add()\n",
    "\n",
    "    def call(self, observation, step_type=None, network_state=()):\n",
    "        shared = self._shared1(observation)\n",
    "        shared = self._shared2(shared)\n",
    "        adv = self._adv1(shared)\n",
    "        adv = self._adv2(adv)\n",
    "        adv = self._adv3(adv)\n",
    "        value = self._value1(shared)\n",
    "        value = self._value2(shared)\n",
    "        q_out = self._q_out([adv, value])\n",
    "        return q_out, network_state\n",
    "    \n",
    "    def copy(self, name):\n",
    "        print(\"copy override\")\n",
    "        return type(self)(self._input_tensor_spec, self.init_state_spec, name)\n",
    "    \n",
    "    @property\n",
    "    def state_spec(self):\n",
    "        return self._state_spec\n",
    "\n",
    "    def _build(self):\n",
    "        if not self.built and self.input_tensor_spec is not None:\n",
    "            random_input = tensor_spec.sample_spec_nest(self.input_tensor_spec, outer_dims=(1,))\n",
    "            #step_type = tf.expand_dims(time_step.StepType.FIRST, 0)\n",
    "            self.__call__(random_input, None, None)\n",
    "\n",
    "    @property\n",
    "    def input_tensor_spec(self):\n",
    "        return self._input_tensor_spec\n",
    "\n",
    "    @property\n",
    "    def variables(self):\n",
    "        try:\n",
    "            self._build()\n",
    "        except ValueError as e:\n",
    "            traceback = sys.exc_info()[2]\n",
    "            six.reraise(\n",
    "                ValueError, \"Failed to call build on the network when accessing \"\n",
    "                \"variables. Message: {!r}.\".format(e), traceback)\n",
    "        return self.weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_net = DDQN(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec())\n",
    "\n",
    "#q_net = q_network.QNetwork(\n",
    "#    train_env.observation_spec(),\n",
    "#    train_env.action_spec(),\n",
    "#    fc_layer_params=fc_layer_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gin\n",
    "import collections\n",
    "from tf_agents.agents import tf_agent\n",
    "from tf_agents.policies import epsilon_greedy_policy\n",
    "from tf_agents.policies import greedy_policy\n",
    "from tf_agents.policies import q_policy\n",
    "from tf_agents.utils import nest_utils\n",
    "\n",
    "\n",
    "def element_wise_squared_loss(x, y):\n",
    "    return tf.compat.v1.losses.mean_squared_error(\n",
    "      x, y, reduction=tf.compat.v1.losses.Reduction.NONE)\n",
    "\n",
    "\n",
    "def element_wise_huber_loss(x, y):\n",
    "    return tf.compat.v1.losses.huber_loss(\n",
    "      x, y, reduction=tf.compat.v1.losses.Reduction.NONE)\n",
    "\n",
    "\n",
    "def compute_td_targets(next_q_values, rewards, discounts):\n",
    "    return tf.stop_gradient(rewards + discounts * next_q_values)\n",
    "\n",
    "\n",
    "class DqnLossInfo(collections.namedtuple('DqnLossInfo',\n",
    "                                         ('td_loss', 'td_error'))):\n",
    "    pass\n",
    "\n",
    "@gin.configurable\n",
    "class DqnAgent(tf_agent.TFAgent):\n",
    "    def __init__(\n",
    "      self,\n",
    "      time_step_spec,\n",
    "      action_spec,\n",
    "      q_network,\n",
    "      optimizer,\n",
    "      epsilon_greedy=0.1,\n",
    "      n_step_update=1,\n",
    "      boltzmann_temperature=None,\n",
    "      emit_log_probability=False,\n",
    "      # Params for target network updates\n",
    "      target_update_tau=1.0,\n",
    "      target_update_period=1,\n",
    "      # Params for training.\n",
    "      td_errors_loss_fn=None,\n",
    "      gamma=1.0,\n",
    "      reward_scale_factor=1.0,\n",
    "      gradient_clipping=None,\n",
    "      # Params for debugging\n",
    "      debug_summaries=False,\n",
    "      summarize_grads_and_vars=False,\n",
    "      train_step_counter=None,\n",
    "      name=None):\n",
    "        tf.Module.__init__(self, name=name)\n",
    "\n",
    "        flat_action_spec = tf.nest.flatten(action_spec)\n",
    "        self._num_actions = [\n",
    "            spec.maximum - spec.minimum + 1 for spec in flat_action_spec\n",
    "        ]\n",
    "\n",
    "        # TODO(oars): Get DQN working with more than one dim in the actions.\n",
    "        if len(flat_action_spec) > 1 or flat_action_spec[0].shape.ndims > 1:\n",
    "            raise ValueError('Only one dimensional actions are supported now.')\n",
    "\n",
    "        if not all(spec.minimum == 0 for spec in flat_action_spec):\n",
    "            raise ValueError(\n",
    "              'Action specs should have minimum of 0, but saw: {0}'.format(\n",
    "                  [spec.minimum for spec in flat_action_spec]))\n",
    "\n",
    "        if epsilon_greedy is not None and boltzmann_temperature is not None:\n",
    "            raise ValueError(\n",
    "              'Configured both epsilon_greedy value {} and temperature {}, '\n",
    "              'however only one of them can be used for exploration.'.format(\n",
    "                  epsilon_greedy, boltzmann_temperature))\n",
    "\n",
    "        self._q_network = q_network\n",
    "        self._target_q_network = self._q_network.copy(name='TargetQNetwork')\n",
    "        self._epsilon_greedy = epsilon_greedy\n",
    "        self._n_step_update = n_step_update\n",
    "        self._boltzmann_temperature = boltzmann_temperature\n",
    "        self._optimizer = optimizer\n",
    "        self._td_errors_loss_fn = td_errors_loss_fn or element_wise_huber_loss\n",
    "        self._gamma = gamma\n",
    "        self._reward_scale_factor = reward_scale_factor\n",
    "        self._gradient_clipping = gradient_clipping\n",
    "        self._update_target = self._get_target_updater(\n",
    "            target_update_tau, target_update_period)\n",
    "\n",
    "        policy = q_policy.QPolicy(\n",
    "            time_step_spec,\n",
    "            action_spec,\n",
    "            q_network=self._q_network,\n",
    "            emit_log_probability=emit_log_probability)\n",
    "\n",
    "        if boltzmann_temperature is not None:\n",
    "            print(\"collect policy is boltzmann\")\n",
    "            collect_policy = boltzmann_policy.BoltzmannPolicy(\n",
    "              policy, temperature=self._boltzmann_temperature)\n",
    "        else:\n",
    "            print (\"collect policy is e-greedy\")\n",
    "            collect_policy = epsilon_greedy_policy.EpsilonGreedyPolicy(\n",
    "              policy, epsilon=self._epsilon_greedy)\n",
    "        policy = greedy_policy.GreedyPolicy(policy)\n",
    "\n",
    "        if q_network.state_spec and n_step_update != 1:\n",
    "            raise NotImplementedError(\n",
    "              'DqnAgent does not currently support n-step updates with stateful '\n",
    "              'networks (i.e., RNNs), but n_step_update = {}'.format(n_step_update))\n",
    "\n",
    "        train_sequence_length = (\n",
    "            n_step_update + 1 if not q_network.state_spec else None)\n",
    "\n",
    "        super(DqnAgent, self).__init__(\n",
    "            time_step_spec,\n",
    "            action_spec,\n",
    "            policy,\n",
    "            collect_policy,\n",
    "            train_sequence_length=train_sequence_length,\n",
    "            debug_summaries=debug_summaries,\n",
    "            summarize_grads_and_vars=summarize_grads_and_vars,\n",
    "            train_step_counter=train_step_counter)\n",
    "\n",
    "    def _initialize(self):\n",
    "        common.soft_variables_update(\n",
    "            self._q_network.variables, self._target_q_network.variables, tau=1.0)\n",
    "\n",
    "    def _get_target_updater(self, tau=1.0, period=1):\n",
    "        with tf.name_scope('update_targets'):\n",
    "\n",
    "            def update():\n",
    "                return common.soft_variables_update(\n",
    "                self._q_network.variables, self._target_q_network.variables, tau)\n",
    "\n",
    "            return common.Periodically(update, period, 'periodic_update_targets')\n",
    "\n",
    "    def _experience_to_transitions(self, experience):\n",
    "        transitions = trajectory.to_transition(experience)\n",
    "\n",
    "        # Remove time dim if we are not using a recurrent network.\n",
    "        if not self._q_network.state_spec:\n",
    "            print(\"no state spec, map structure\")\n",
    "            transitions = tf.nest.map_structure(lambda x: tf.squeeze(x, [1]),\n",
    "                                          transitions)\n",
    "\n",
    "        time_steps, policy_steps, next_time_steps = transitions\n",
    "        actions = policy_steps.action\n",
    "        return time_steps, actions, next_time_steps\n",
    "\n",
    "    # Use @common.function in graph mode or for speeding up.\n",
    "    def _train(self, experience, weights):\n",
    "        with tf.GradientTape() as tape:\n",
    "              loss_info = self._loss(\n",
    "                  experience,\n",
    "                  td_errors_loss_fn=self._td_errors_loss_fn,\n",
    "                  gamma=self._gamma,\n",
    "                  reward_scale_factor=self._reward_scale_factor,\n",
    "                  weights=weights)\n",
    "        tf.debugging.check_numerics(loss_info[0], 'Loss is inf or nan')\n",
    "        variables_to_train = self._q_network.trainable_weights\n",
    "        assert list(variables_to_train), \"No variables in the agent's q_network.\"\n",
    "        grads = tape.gradient(loss_info.loss, variables_to_train)\n",
    "        # Tuple is used for py3, where zip is a generator producing values once.\n",
    "        grads_and_vars = tuple(zip(grads, variables_to_train))\n",
    "        if self._gradient_clipping is not None:\n",
    "            print(\"gradient clipping enabled\")\n",
    "            grads_and_vars = eager_utils.clip_gradient_norms(grads_and_vars,\n",
    "                                                       self._gradient_clipping)\n",
    "\n",
    "        if self._summarize_grads_and_vars:\n",
    "            eager_utils.add_variables_summaries(grads_and_vars,\n",
    "                                          self.train_step_counter)\n",
    "            eager_utils.add_gradients_summaries(grads_and_vars,\n",
    "                                          self.train_step_counter)\n",
    "\n",
    "        self._optimizer.apply_gradients(grads_and_vars,\n",
    "                                    global_step=self.train_step_counter)\n",
    "\n",
    "        self._update_target()\n",
    "\n",
    "        return loss_info\n",
    "\n",
    "    def _loss(self,\n",
    "            experience,\n",
    "            td_errors_loss_fn=element_wise_huber_loss,\n",
    "            gamma=1.0,\n",
    "            reward_scale_factor=1.0,\n",
    "            weights=None):\n",
    "        # Check that `experience` includes two outer dimensions [B, T, ...]. This\n",
    "        # method requires a time dimension to compute the loss properly.\n",
    "        self._check_trajectory_dimensions(experience)\n",
    "\n",
    "        time_steps, actions, next_time_steps = self._experience_to_transitions(experience)\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            actions = tf.nest.flatten(actions)[0]\n",
    "            q_values, _ = self._q_network(time_steps.observation,\n",
    "                                    time_steps.step_type)\n",
    "\n",
    "            # Handle action_spec.shape=(), and shape=(1,) by using the\n",
    "            # multi_dim_actions param.\n",
    "            multi_dim_actions = tf.nest.flatten(self._action_spec)[0].shape.ndims > 0\n",
    "            print(\"multi_dim_actions={}\".format(multi_dim_actions))\n",
    "            q_values = common.index_with_actions(\n",
    "                q_values,\n",
    "                tf.cast(actions, dtype=tf.int32),\n",
    "                multi_dim_actions=multi_dim_actions)\n",
    "\n",
    "            next_q_values = self._compute_next_q_values(next_time_steps)\n",
    "\n",
    "            td_targets = compute_td_targets(\n",
    "                next_q_values,\n",
    "                rewards=reward_scale_factor * next_time_steps.reward,\n",
    "                discounts=gamma * next_time_steps.discount)\n",
    "\n",
    "            valid_mask = tf.cast(~time_steps.is_last(), tf.float32)\n",
    "            td_error = valid_mask * (td_targets - q_values)\n",
    "\n",
    "            td_loss = valid_mask * td_errors_loss_fn(td_targets, q_values)\n",
    "\n",
    "            if nest_utils.is_batched_nested_tensors(time_steps, self.time_step_spec, num_outer_dims=2):\n",
    "                # Do a sum over the time dimension.\n",
    "                print(\"is nested\")\n",
    "                td_loss = tf.reduce_sum(input_tensor=td_loss, axis=1)\n",
    "\n",
    "            if weights is not None:\n",
    "                print(\"has weights\")\n",
    "                td_loss *= weights\n",
    "\n",
    "            # Average across the elements of the batch.\n",
    "            # Note: We use an element wise loss above to ensure each element is always\n",
    "            #   weighted by 1/N where N is the batch size, even when some of the\n",
    "            #   weights are zero due to boundary transitions. Weighting by 1/K where K\n",
    "            #   is the actual number of non-zero weight would artificially increase\n",
    "            #   their contribution in the loss. Think about what would happen as\n",
    "            #   the number of boundary samples increases.\n",
    "            loss = tf.reduce_mean(input_tensor=td_loss)\n",
    "\n",
    "            return tf_agent.LossInfo(loss, DqnLossInfo(td_loss=td_loss,\n",
    "                                                 td_error=td_error))\n",
    "\n",
    "    def _compute_next_q_values(self, next_time_steps):\n",
    "        next_target_q_values, _ = self._target_q_network(\n",
    "        next_time_steps.observation, next_time_steps.step_type)\n",
    "        # Reduce_max below assumes q_values are [BxF] or [BxTxF]\n",
    "        assert next_target_q_values.shape.ndims in [2, 3]\n",
    "        return tf.reduce_max(input_tensor=next_target_q_values, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy override\n",
      "collect policy is e-greedy\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.compat.v2.Variable(0)\n",
    "\n",
    "dqn_agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=dqn_agent.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "dqn_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = dqn_agent.policy\n",
    "collect_policy = dqn_agent.collect_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=dqn_agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    replay_buffer.add_batch(traj)\n",
    "\n",
    "for _ in range(initial_collect_steps):\n",
    "    collect_step(train_env, random_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, sample_batch_size=batch_size, num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0711 09:03:55.514138 4614600128 deprecation.py:323] From /Users/jwatts/TensorflowProjects/lib/python3.7/site-packages/tf_agents/policies/epsilon_greedy_policy.py:96: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "td_errors_loss_fn=<function element_wise_squared_loss at 0x139f4ae18>\n",
      "_n_step_update=1\n",
      "no state spec, map structure\n",
      "multi_dim_actions=False\n",
      "reward_scale_factor=1.0; gamma=1.0\n",
      "td_errors_loss_fn=<function element_wise_squared_loss at 0x139f4ae18>\n",
      "_n_step_update=1\n",
      "no state spec, map structure\n",
      "multi_dim_actions=False\n",
      "reward_scale_factor=1.0; gamma=1.0\n",
      "step = 200: loss = 70.516845703125\n",
      "step = 400: loss = 31.066225051879883\n",
      "step = 600: loss = 49.29338836669922\n",
      "step = 800: loss = 93.052490234375\n",
      "step = 1000: loss = 38.62739181518555\n",
      "step = 1000: Average Return = 500.0\n",
      "step = 1200: loss = 97.64010620117188\n",
      "step = 1400: loss = 216.76687622070312\n",
      "step = 1600: loss = 847.4637451171875\n",
      "step = 1800: loss = 7704.18603515625\n",
      "step = 2000: loss = 2600.293701171875\n",
      "step = 2000: Average Return = 60.5\n",
      "step = 2200: loss = 6456.88671875\n",
      "step = 2400: loss = 1501.7064208984375\n",
      "step = 2600: loss = 18047.22265625\n",
      "step = 2800: loss = 1306.30322265625\n",
      "step = 3000: loss = 1718.62548828125\n",
      "step = 3000: Average Return = 143.0\n",
      "step = 3200: loss = 6198.0888671875\n",
      "step = 3400: loss = 1593.23583984375\n",
      "step = 3600: loss = 43712.76171875\n",
      "step = 3800: loss = 6137.66796875\n",
      "step = 4000: loss = 20701.431640625\n",
      "step = 4000: Average Return = 117.80000305175781\n",
      "step = 4200: loss = 16479.0234375\n",
      "step = 4400: loss = 22618.28515625\n",
      "step = 4600: loss = 2470.7333984375\n",
      "step = 4800: loss = 2866.7900390625\n",
      "step = 5000: loss = 2166.0322265625\n",
      "step = 5000: Average Return = 148.5\n",
      "step = 5200: loss = 41033.10546875\n",
      "step = 5400: loss = 4224.15380859375\n",
      "step = 5600: loss = 45761.73046875\n",
      "step = 5800: loss = 19674.41015625\n",
      "step = 6000: loss = 2507.4580078125\n",
      "step = 6000: Average Return = 187.3000030517578\n",
      "step = 6200: loss = 45404.75390625\n",
      "step = 6400: loss = 5498.19091796875\n",
      "step = 6600: loss = 172463.75\n",
      "step = 6800: loss = 3766.86474609375\n",
      "step = 7000: loss = 31265.052734375\n",
      "step = 7000: Average Return = 170.0\n",
      "step = 7200: loss = 55436.25390625\n",
      "step = 7400: loss = 77255.1484375\n",
      "step = 7600: loss = 5118.89404296875\n",
      "step = 7800: loss = 52757.2109375\n",
      "step = 8000: loss = 58334.30859375\n",
      "step = 8000: Average Return = 215.6999969482422\n",
      "step = 8200: loss = 46350.5625\n",
      "step = 8400: loss = 3848.732666015625\n",
      "step = 8600: loss = 4823.77783203125\n",
      "step = 8800: loss = 14365.439453125\n",
      "step = 9000: loss = 48682.7734375\n",
      "step = 9000: Average Return = 253.6999969482422\n",
      "step = 9200: loss = 6675.412109375\n",
      "step = 9400: loss = 4021.537841796875\n",
      "step = 9600: loss = 2768.60888671875\n",
      "step = 9800: loss = 18611.302734375\n",
      "step = 10000: loss = 2978.759521484375\n",
      "step = 10000: Average Return = 252.8000030517578\n",
      "step = 10200: loss = 31316.724609375\n",
      "step = 10400: loss = 2655.386474609375\n",
      "step = 10600: loss = 2312.385498046875\n",
      "step = 10800: loss = 8501.9296875\n",
      "step = 11000: loss = 2065.68603515625\n",
      "step = 11000: Average Return = 336.0\n",
      "step = 11200: loss = 946.310546875\n",
      "step = 11400: loss = 2151.4482421875\n",
      "step = 11600: loss = 8912.8486328125\n",
      "step = 11800: loss = 3436.096435546875\n",
      "step = 12000: loss = 2064.973388671875\n",
      "step = 12000: Average Return = 200.5\n",
      "step = 12200: loss = 2177.63720703125\n",
      "step = 12400: loss = 25228.009765625\n",
      "step = 12600: loss = 3571.156005859375\n",
      "step = 12800: loss = 2373.974365234375\n",
      "step = 13000: loss = 895.005615234375\n",
      "step = 13000: Average Return = 461.79998779296875\n",
      "step = 13200: loss = 5722.80859375\n",
      "step = 13400: loss = 5994.8310546875\n",
      "step = 13600: loss = 1647.65771484375\n",
      "step = 13800: loss = 2599.635009765625\n",
      "step = 14000: loss = 2473.512451171875\n",
      "step = 14000: Average Return = 414.3999938964844\n",
      "step = 14200: loss = 12021.4951171875\n",
      "step = 14400: loss = 2475.3740234375\n",
      "step = 14600: loss = 3072.956298828125\n",
      "step = 14800: loss = 3043.955078125\n",
      "step = 15000: loss = 2642.25\n",
      "step = 15000: Average Return = 467.70001220703125\n",
      "step = 15200: loss = 4284.5185546875\n",
      "step = 15400: loss = 56471.9453125\n",
      "step = 15600: loss = 6764.25244140625\n",
      "step = 15800: loss = 4831.7333984375\n",
      "step = 16000: loss = 4662.865234375\n",
      "step = 16000: Average Return = 500.0\n",
      "step = 16200: loss = 63057.21484375\n",
      "step = 16400: loss = 9426.2841796875\n",
      "step = 16600: loss = 18321.9375\n",
      "step = 16800: loss = 22594.546875\n",
      "step = 17000: loss = 17798.93359375\n",
      "step = 17000: Average Return = 500.0\n",
      "step = 17200: loss = 8090.26171875\n",
      "step = 17400: loss = 40970.4375\n",
      "step = 17600: loss = 433306.65625\n",
      "step = 17800: loss = 72175.6875\n",
      "step = 18000: loss = 4340403.5\n",
      "step = 18000: Average Return = 477.0\n",
      "step = 18200: loss = 9142233.0\n",
      "step = 18400: loss = 55684.04296875\n",
      "step = 18600: loss = 187740.390625\n",
      "step = 18800: loss = 106125.671875\n",
      "step = 19000: loss = 3783547.0\n",
      "step = 19000: Average Return = 466.0\n",
      "step = 19200: loss = 141371.65625\n",
      "step = 19400: loss = 97837.09375\n",
      "step = 19600: loss = 171921.59375\n",
      "step = 19800: loss = 157612.296875\n",
      "step = 20000: loss = 229847.109375\n",
      "step = 20000: Average Return = 422.20001220703125\n"
     ]
    }
   ],
   "source": [
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "dqn_agent.train = common.function(dqn_agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "dqn_agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, dqn_agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    for _ in range(collect_steps_per_iteration):\n",
    "        collect_step(train_env, dqn_agent.collect_policy)\n",
    "\n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = dqn_agent.train(experience)\n",
    "\n",
    "    step = dqn_agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss.loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, dqn_agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35.16499919891358, 550)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(steps, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Step')\n",
    "plt.ylim(top=550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = eval_env.reset()\n",
    "while not time_step.is_last():\n",
    "    action_step = dqn_agent.policy.action(time_step)\n",
    "    time_step = eval_env.step(action_step.action)\n",
    "    eval_py_env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
