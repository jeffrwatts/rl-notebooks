{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents import tf_agent\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.policies import q_policy\n",
    "from tf_agents.policies import epsilon_greedy_policy\n",
    "from tf_agents.policies import greedy_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.utils import composite\n",
    "from tf_agents.utils import training as training_lib\n",
    "\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 20000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 1000  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v0'\n",
    "\n",
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layer_params = (100,)\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLossInfo(collections.namedtuple('MyLossInfo',\n",
    "                                         ('td_loss', 'td_error'))):\n",
    "    pass\n",
    "\n",
    "def compute_td_targets(next_q_values, rewards, discounts):\n",
    "    return tf.stop_gradient(rewards + discounts * next_q_values)\n",
    "\n",
    "class MyAgent(tf_agent.TFAgent):\n",
    "    def __init__(self,\n",
    "                 time_step_spec,\n",
    "                 action_spec,\n",
    "                 q_network,\n",
    "                 optimizer,\n",
    "                 epsilon_greedy=0.1,\n",
    "                 gamma=1.0,\n",
    "                 n_step_update=1,\n",
    "                 train_step_counter=None,\n",
    "                 name=None):\n",
    "        tf.Module.__init__(self, name=name)\n",
    "\n",
    "        self._q_network = q_network\n",
    "        q_network.create_variables()\n",
    "        self._target_q_network = common.maybe_copy_target_network_with_checks(\n",
    "            self._q_network, None, 'TargetQNetwork')\n",
    "\n",
    "        self._epsilon_greedy = epsilon_greedy\n",
    "        self._n_step_update = n_step_update\n",
    "        self._optimizer = optimizer\n",
    "        self._td_errors_loss_fn = common.element_wise_squared_loss\n",
    "        self._gamma = gamma\n",
    "        self._update_target = self._get_target_updater()\n",
    "\n",
    "        policy = q_policy.QPolicy(time_step_spec, action_spec, q_network=self._q_network)\n",
    "        collect_policy = epsilon_greedy_policy.EpsilonGreedyPolicy(policy, epsilon=self._epsilon_greedy)\n",
    "        policy = greedy_policy.GreedyPolicy(policy)\n",
    "\n",
    "        # Create self._target_greedy_policy in order to compute target Q-values.\n",
    "        target_policy = q_policy.QPolicy(time_step_spec, action_spec, q_network=self._target_q_network)\n",
    "        self._target_greedy_policy = greedy_policy.GreedyPolicy(target_policy)\n",
    "\n",
    "        train_sequence_length = n_step_update + 1\n",
    "\n",
    "        super(MyAgent, self).__init__(\n",
    "            time_step_spec,\n",
    "            action_spec,\n",
    "            policy,\n",
    "            collect_policy,\n",
    "            train_sequence_length=train_sequence_length,\n",
    "            train_step_counter=train_step_counter)\n",
    "        \n",
    "    def _get_target_updater(self, tau=1.0, period=1):\n",
    "        with tf.name_scope('update_targets'):\n",
    "\n",
    "            def update():\n",
    "                return common.soft_variables_update(\n",
    "                    self._q_network.variables,\n",
    "                    self._target_q_network.variables,\n",
    "                    tau,\n",
    "                    tau_non_trainable=1.0)\n",
    "\n",
    "        return common.Periodically(update, period, 'periodic_update_targets')\n",
    "        \n",
    "    def _initialize(self):\n",
    "        common.soft_variables_update(self._q_network.variables, self._target_q_network.variables, tau=1.0)\n",
    "        \n",
    "    def _experience_to_transitions(self, experience):\n",
    "        transitions = trajectory.to_transition(experience)\n",
    "\n",
    "        # Remove time dim if we are not using a recurrent network.\n",
    "        if not self._q_network.state_spec:\n",
    "            transitions = tf.nest.map_structure(lambda x: composite.squeeze(x, 1), transitions)\n",
    "\n",
    "        time_steps, policy_steps, next_time_steps = transitions\n",
    "        actions = policy_steps.action\n",
    "        return time_steps, actions, next_time_steps\n",
    "\n",
    "    # Use @common.function in graph mode or for speeding up.\n",
    "    def _train(self, experience, weights):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_info = self._loss(experience, training=True)\n",
    "        \n",
    "        tf.debugging.check_numerics(loss_info[0], 'Loss is inf or nan')\n",
    "        variables_to_train = self._q_network.trainable_weights\n",
    "        non_trainable_weights = self._q_network.non_trainable_weights\n",
    "        assert list(variables_to_train), \"No variables in the agent's q_network.\"\n",
    "        grads = tape.gradient(loss_info.loss, variables_to_train)\n",
    "        # Tuple is used for py3, where zip is a generator producing values once.\n",
    "        grads_and_vars = list(zip(grads, variables_to_train))\n",
    "\n",
    "        training_lib.apply_gradients(self._optimizer, grads_and_vars, global_step=self.train_step_counter)\n",
    "\n",
    "        self._update_target()\n",
    "\n",
    "        return loss_info\n",
    "\n",
    "    def _loss(self, experience, training=False):\n",
    "        time_steps, actions, next_time_steps = self._experience_to_transitions(experience)\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            q_values = self._compute_q_values(time_steps, actions, training=training)\n",
    "\n",
    "            next_q_values = self._compute_next_q_values(next_time_steps)\n",
    "\n",
    "            # Special case for n = 1 to avoid a loss of performance.\n",
    "            td_targets = compute_td_targets(\n",
    "                next_q_values,\n",
    "                rewards = next_time_steps.reward,\n",
    "                discounts = self._gamma * next_time_steps.discount)\n",
    "\n",
    "            valid_mask = tf.cast(~time_steps.is_last(), tf.float32)\n",
    "            td_error = valid_mask * (td_targets - q_values)\n",
    "\n",
    "            td_loss = valid_mask * self._td_errors_loss_fn(td_targets, q_values)\n",
    "\n",
    "            loss = tf.reduce_mean(input_tensor=td_loss)\n",
    "\n",
    "            # Add network loss (such as regularization loss)\n",
    "            if self._q_network.losses:\n",
    "                loss = loss + tf.reduce_mean(self._q_network.losses)\n",
    "\n",
    "            return tf_agent.LossInfo(loss, MyLossInfo(td_loss=td_loss,\n",
    "                                                     td_error=td_error))\n",
    "\n",
    "    def _compute_q_values(self, time_steps, actions, training=False):\n",
    "        network_observation = time_steps.observation\n",
    "\n",
    "        q_values, _ = self._q_network(network_observation, time_steps.step_type,\n",
    "                                      training=training)\n",
    "        multi_dim_actions = self._action_spec.shape.rank > 0\n",
    "        return common.index_with_actions(\n",
    "            q_values,\n",
    "            tf.cast(actions, dtype=tf.int32),\n",
    "            multi_dim_actions=multi_dim_actions)\n",
    "\n",
    "    def _compute_next_q_values(self, next_time_steps):\n",
    "        network_observation = next_time_steps.observation\n",
    "\n",
    "        next_target_q_values, _ = self._target_q_network(network_observation, next_time_steps.step_type)\n",
    "        batch_size = (next_target_q_values.shape[0] or tf.shape(next_target_q_values)[0])\n",
    "        dummy_state = self._target_greedy_policy.get_initial_state(batch_size)\n",
    "        # Find the greedy actions using our target greedy policy. This ensures that\n",
    "        # action constraints are respected and helps centralize the greedy logic.\n",
    "        greedy_actions = self._target_greedy_policy.action(next_time_steps, dummy_state).action\n",
    "\n",
    "        # Handle action_spec.shape=(), and shape=(1,) by using the multi_dim_actions\n",
    "        # param. Note: assumes len(tf.nest.flatten(action_spec)) == 1.\n",
    "        multi_dim_actions = tf.nest.flatten(self._action_spec)[0].shape.rank > 0\n",
    "        return common.index_with_actions(\n",
    "            next_target_q_values,\n",
    "            greedy_actions,\n",
    "            multi_dim_actions=multi_dim_actions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "#agent = dqn_agent.DqnAgent(\n",
    "#    train_env.time_step_spec(),\n",
    "#    train_env.action_spec(),\n",
    "#    q_network=q_net,\n",
    "#    optimizer=optimizer,\n",
    "#    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "#    train_step_counter=train_step_counter)\n",
    "\n",
    "agent = MyAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]\n",
    "\n",
    "def collect_step(environment, policy, buffer):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "    for _ in range(steps):\n",
    "        collect_step(env, policy, buffer)\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "\n",
    "collect_data(train_env, random_policy, replay_buffer, steps=100)\n",
    "\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 200: loss = 11.006783485412598\n",
      "step = 400: loss = 3.7969253063201904\n",
      "step = 600: loss = 4.921608924865723\n",
      "step = 800: loss = 8.25903034210205\n",
      "step = 1000: loss = 6.7414398193359375\n",
      "step = 1000: Average Return = 12.199999809265137\n",
      "step = 1200: loss = 29.57880210876465\n",
      "step = 1400: loss = 20.229900360107422\n",
      "step = 1600: loss = 10.648536682128906\n",
      "step = 1800: loss = 13.900444984436035\n",
      "step = 2000: loss = 30.028514862060547\n",
      "step = 2000: Average Return = 21.200000762939453\n",
      "step = 2200: loss = 11.769752502441406\n",
      "step = 2400: loss = 9.325913429260254\n",
      "step = 2600: loss = 20.44812774658203\n",
      "step = 2800: loss = 9.726699829101562\n",
      "step = 3000: loss = 47.587310791015625\n",
      "step = 3000: Average Return = 50.900001525878906\n",
      "step = 3200: loss = 19.15411376953125\n",
      "step = 3400: loss = 59.7524299621582\n",
      "step = 3600: loss = 33.184104919433594\n",
      "step = 3800: loss = 24.32809829711914\n",
      "step = 4000: loss = 47.7923469543457\n",
      "step = 4000: Average Return = 68.80000305175781\n",
      "step = 4200: loss = 50.71818923950195\n",
      "step = 4400: loss = 50.45116424560547\n",
      "step = 4600: loss = 79.32862854003906\n",
      "step = 4800: loss = 5.847166538238525\n",
      "step = 5000: loss = 8.21571159362793\n",
      "step = 5000: Average Return = 100.69999694824219\n",
      "step = 5200: loss = 18.050878524780273\n",
      "step = 5400: loss = 23.83114242553711\n",
      "step = 5600: loss = 5.344626426696777\n",
      "step = 5800: loss = 77.77577209472656\n",
      "step = 6000: loss = 93.43060302734375\n",
      "step = 6000: Average Return = 154.60000610351562\n",
      "step = 6200: loss = 201.55419921875\n",
      "step = 6400: loss = 70.58033752441406\n",
      "step = 6600: loss = 215.60145568847656\n",
      "step = 6800: loss = 117.82559204101562\n",
      "step = 7000: loss = 32.445133209228516\n",
      "step = 7000: Average Return = 165.1999969482422\n",
      "step = 7200: loss = 84.81305694580078\n",
      "step = 7400: loss = 136.14541625976562\n",
      "step = 7600: loss = 7.633849143981934\n",
      "step = 7800: loss = 162.30184936523438\n",
      "step = 8000: loss = 114.02947235107422\n",
      "step = 8000: Average Return = 188.5\n",
      "step = 8200: loss = 16.654672622680664\n",
      "step = 8400: loss = 108.30030059814453\n",
      "step = 8600: loss = 117.6104736328125\n",
      "step = 8800: loss = 41.191532135009766\n",
      "step = 9000: loss = 342.76251220703125\n",
      "step = 9000: Average Return = 196.1999969482422\n",
      "step = 9200: loss = 15.54762077331543\n",
      "step = 9400: loss = 10.158441543579102\n",
      "step = 9600: loss = 17.100360870361328\n",
      "step = 9800: loss = 119.8798828125\n",
      "step = 10000: loss = 315.068359375\n",
      "step = 10000: Average Return = 198.0\n",
      "step = 10200: loss = 28.00056266784668\n",
      "step = 10400: loss = 16.330625534057617\n",
      "step = 10600: loss = 12.532190322875977\n",
      "step = 10800: loss = 37.05101013183594\n",
      "step = 11000: loss = 93.01898193359375\n",
      "step = 11000: Average Return = 200.0\n",
      "step = 11200: loss = 14.950153350830078\n",
      "step = 11400: loss = 395.1084289550781\n",
      "step = 11600: loss = 29.042333602905273\n",
      "step = 11800: loss = 22.08003807067871\n",
      "step = 12000: loss = 25.517120361328125\n",
      "step = 12000: Average Return = 200.0\n",
      "step = 12200: loss = 10.762689590454102\n",
      "step = 12400: loss = 11.454733848571777\n",
      "step = 12600: loss = 906.5877685546875\n",
      "step = 12800: loss = 659.284912109375\n",
      "step = 13000: loss = 77.76626586914062\n",
      "step = 13000: Average Return = 200.0\n",
      "step = 13200: loss = 526.8002319335938\n",
      "step = 13400: loss = 647.859619140625\n",
      "step = 13600: loss = 172.3209991455078\n",
      "step = 13800: loss = 32.02008819580078\n",
      "step = 14000: loss = 28.419456481933594\n",
      "step = 14000: Average Return = 200.0\n",
      "step = 14200: loss = 17.33201026916504\n",
      "step = 14400: loss = 14.131773948669434\n",
      "step = 14600: loss = 27.302223205566406\n",
      "step = 14800: loss = 1774.4078369140625\n",
      "step = 15000: loss = 37.00108337402344\n",
      "step = 15000: Average Return = 200.0\n",
      "step = 15200: loss = 381.0692138671875\n",
      "step = 15400: loss = 44.661720275878906\n",
      "step = 15600: loss = 31.047061920166016\n",
      "step = 15800: loss = 30.156578063964844\n",
      "step = 16000: loss = 21.177997589111328\n",
      "step = 16000: Average Return = 200.0\n",
      "step = 16200: loss = 570.4092407226562\n",
      "step = 16400: loss = 44.94774627685547\n",
      "step = 16600: loss = 21.75640106201172\n",
      "step = 16800: loss = 111.99411010742188\n",
      "step = 17000: loss = 17.302085876464844\n",
      "step = 17000: Average Return = 200.0\n",
      "step = 17200: loss = 4341.8134765625\n",
      "step = 17400: loss = 50.84101104736328\n",
      "step = 17600: loss = 102.26072692871094\n",
      "step = 17800: loss = 48.90376281738281\n",
      "step = 18000: loss = 782.0137329101562\n",
      "step = 18000: Average Return = 200.0\n",
      "step = 18200: loss = 2624.381591796875\n",
      "step = 18400: loss = 29.817182540893555\n",
      "step = 18600: loss = 33.29918670654297\n",
      "step = 18800: loss = 3265.470947265625\n",
      "step = 19000: loss = 43.97056579589844\n",
      "step = 19000: Average Return = 200.0\n",
      "step = 19200: loss = 58.485084533691406\n",
      "step = 19400: loss = 59.480594635009766\n",
      "step = 19600: loss = 854.6743774414062\n",
      "step = 19800: loss = 62.69694137573242\n",
      "step = 20000: loss = 82.34294128417969\n",
      "step = 20000: Average Return = 200.0\n"
     ]
    }
   ],
   "source": [
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    for _ in range(collect_steps_per_iteration):\n",
    "        collect_step(train_env, agent.collect_policy, replay_buffer)\n",
    "\n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = agent.train(experience).loss\n",
    "\n",
    "    step = agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
