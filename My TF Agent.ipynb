{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.utils import composite\n",
    "from tf_agents.utils import training as training_lib\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.trajectories import policy_step\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.policies import tf_policy\n",
    "from tf_agents.networks import network\n",
    "from tf_agents.agents import tf_agent\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 500000 \n",
    "\n",
    "initial_collect_steps = 1000   \n",
    "collect_steps_per_iteration = 1  \n",
    "replay_buffer_max_length = 100000  \n",
    "\n",
    "batch_size = 64  \n",
    "learning_rate = 1e-3  \n",
    "log_interval = 10000  \n",
    "\n",
    "num_eval_episodes = 10  \n",
    "eval_interval = 100000 \n",
    "max_episode_steps = 1000\n",
    "\n",
    "gamma = 0.99 \n",
    "temp = 0.001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoundedTensorSpec(shape=(8,), dtype=tf.float32, name='observation', minimum=array(-3.4028235e+38, dtype=float32), maximum=array(3.4028235e+38, dtype=float32))\n",
      "BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(3))\n"
     ]
    }
   ],
   "source": [
    "env_name = 'LunarLander-v2'\n",
    "\n",
    "train_py_env = suite_gym.load(env_name, max_episode_steps=max_episode_steps)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "\n",
    "print(train_env.observation_spec())\n",
    "print(train_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionValueNetwork(network.Network):\n",
    "    def __init__(self,\n",
    "                 input_tensor_spec,\n",
    "                 action_spec,\n",
    "                 hidden_units,\n",
    "                 name=None):\n",
    "        \n",
    "        super(ActionValueNetwork, self).__init__(\n",
    "            input_tensor_spec=input_tensor_spec,\n",
    "            state_spec=(),\n",
    "            name=name)\n",
    "\n",
    "        action_spec = tf.nest.flatten(action_spec)[0]\n",
    "        num_actions = action_spec.maximum - action_spec.minimum + 1\n",
    "\n",
    "        self._hidden = tf.keras.layers.Dense(hidden_units, \n",
    "                                             activation=tf.keras.activations.relu, \n",
    "                                             kernel_initializer='orthogonal',\n",
    "                                             input_shape=input_tensor_spec.shape)\n",
    "        \n",
    "        self._q_value_layer = tf.keras.layers.Dense(num_actions,\n",
    "                                                    activation=None,\n",
    "                                                    kernel_initializer='orthogonal')\n",
    "\n",
    "\n",
    "    def call(self, observation, step_type=None, network_state=(), training=False):\n",
    "        action_values = self._hidden(observation, training=training)\n",
    "        action_values = self._q_value_layer(action_values, training=training)\n",
    "        return action_values, network_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_net = ActionValueNetwork(train_env.observation_spec(), \n",
    "                           train_env.action_spec(), \n",
    "                           256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temp(q_values, temp):\n",
    "    preferences = tf.divide(q_values, temp)\n",
    "    max_preference = tf.math.reduce_max(preferences, axis=1, keepdims=True)\n",
    "    exp_preferences = tf.exp(preferences-max_preference)\n",
    "    sum_exp_preferences = tf.reduce_sum(tf.exp(preferences-max_preference), axis=1, keepdims=True)\n",
    "    return tf.divide(exp_preferences, sum_exp_preferences)\n",
    "\n",
    "class SoftMaxPolicyWithTemp(tf_policy.Base):\n",
    "    def __init__(self, time_step_spec, action_spec, q_network, temp, name=None):\n",
    "        \n",
    "        super(SoftMaxPolicyWithTemp, self).__init__(\n",
    "            time_step_spec,\n",
    "            action_spec,\n",
    "            policy_state_spec=q_network.state_spec,\n",
    "            name=name)\n",
    "        \n",
    "        self._temp = temp\n",
    "        self._q_network = q_network\n",
    "        \n",
    "    def _variables(self):\n",
    "        return self._q_network.variables\n",
    "    \n",
    "    def _action(self, time_step, policy_state, seed):\n",
    "        network_observation = time_step.observation\n",
    "        q_values, policy_state = self._q_network(network_observation, time_step.step_type, policy_state)\n",
    "        probs = softmax_with_temp(q_values, self._temp)\n",
    "        action = tf.random.categorical(tf.math.log(probs), 1)\n",
    "        action = tf.expand_dims(tf.squeeze(action), 0)\n",
    "        return policy_step.PolicyStep(action, policy_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLossInfo(collections.namedtuple('MyLossInfo',\n",
    "                                         ('td_loss', 'td_error'))):\n",
    "    pass\n",
    "\n",
    "def compute_td_targets(next_q_values, rewards, discounts):\n",
    "    return tf.stop_gradient(rewards + discounts * next_q_values)\n",
    "\n",
    "class MyAgent(tf_agent.TFAgent):\n",
    "    def __init__(self,\n",
    "                 time_step_spec,\n",
    "                 action_spec,\n",
    "                 q_network,\n",
    "                 optimizer,\n",
    "                 gamma,\n",
    "                 temp,\n",
    "                 n_step_update=1,\n",
    "                 train_step_counter=None,\n",
    "                 name=None):\n",
    "        tf.Module.__init__(self, name=name)\n",
    "\n",
    "        self._q_network = q_network\n",
    "        self._q_network.create_variables()\n",
    "        self._target_q_network = common.maybe_copy_target_network_with_checks(\n",
    "            self._q_network, None, 'TargetQNetwork')\n",
    "\n",
    "        self._n_step_update = n_step_update\n",
    "        self._optimizer = optimizer\n",
    "        self._td_errors_loss_fn = common.element_wise_squared_loss\n",
    "        self._gamma = gamma\n",
    "        self._update_target = self._get_target_updater()\n",
    "        \n",
    "        self._temp = temp\n",
    "\n",
    "        collect_policy = SoftMaxPolicyWithTemp(train_env.time_step_spec(), train_env.action_spec(), q_network=q_net, temp=self._temp)\n",
    "        policy = SoftMaxPolicyWithTemp(train_env.time_step_spec(), train_env.action_spec(), q_network=q_net, temp=self._temp)\n",
    "\n",
    "        train_sequence_length = n_step_update + 1\n",
    "\n",
    "        print(\"gamma={}; temp={}\".format(gamma, temp))\n",
    "        super(MyAgent, self).__init__(\n",
    "            time_step_spec,\n",
    "            action_spec,\n",
    "            policy,\n",
    "            collect_policy,\n",
    "            train_sequence_length=train_sequence_length,\n",
    "            train_step_counter=train_step_counter)\n",
    "        \n",
    "    def _get_target_updater(self, tau=1.0, period=1):\n",
    "        with tf.name_scope('update_targets'):\n",
    "\n",
    "            def update():\n",
    "                return common.soft_variables_update(\n",
    "                    self._q_network.variables,\n",
    "                    self._target_q_network.variables,\n",
    "                    tau,\n",
    "                    tau_non_trainable=1.0)\n",
    "\n",
    "        return common.Periodically(update, period, 'periodic_update_targets')\n",
    "        \n",
    "    def _initialize(self):\n",
    "        common.soft_variables_update(self._q_network.variables, self._target_q_network.variables, tau=1.0)\n",
    "        \n",
    "    def _experience_to_transitions(self, experience):\n",
    "        transitions = trajectory.to_transition(experience)\n",
    "        transitions = tf.nest.map_structure(lambda x: composite.squeeze(x, 1),\n",
    "                                          transitions)\n",
    "        time_steps, policy_steps, next_time_steps = transitions\n",
    "        actions = policy_steps.action\n",
    "        return time_steps, actions, next_time_steps\n",
    "\n",
    "    # Use @common.function in graph mode or for speeding up.\n",
    "    def _train(self, experience, weights):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_info = self._loss(experience, training=True)\n",
    "        \n",
    "        tf.debugging.check_numerics(loss_info[0], 'Loss is inf or nan')\n",
    "        variables_to_train = self._q_network.trainable_weights\n",
    "        non_trainable_weights = self._q_network.non_trainable_weights\n",
    "        assert list(variables_to_train), \"No variables in the agent's q_network.\"\n",
    "        grads = tape.gradient(loss_info.loss, variables_to_train)\n",
    "        # Tuple is used for py3, where zip is a generator producing values once.\n",
    "        grads_and_vars = list(zip(grads, variables_to_train))\n",
    "\n",
    "        training_lib.apply_gradients(self._optimizer, grads_and_vars, global_step=self.train_step_counter)\n",
    "\n",
    "        self._update_target()\n",
    "\n",
    "        return loss_info\n",
    "\n",
    "    def _loss(self, experience, training=False):\n",
    "        time_steps, actions, next_time_steps = self._experience_to_transitions(experience)\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            q_values = self._compute_q_values(time_steps, actions, training=training)\n",
    "\n",
    "            next_q_values = self._compute_next_q_values(next_time_steps)\n",
    "\n",
    "            td_targets = compute_td_targets(\n",
    "                next_q_values,\n",
    "                rewards = next_time_steps.reward,\n",
    "                discounts = self._gamma * next_time_steps.discount)\n",
    "            \n",
    "            valid_mask = tf.cast(~time_steps.is_last(), tf.float32)\n",
    "            td_error = valid_mask * (td_targets - q_values)\n",
    "\n",
    "            td_loss = valid_mask * self._td_errors_loss_fn(td_targets, q_values)\n",
    "\n",
    "            loss = tf.reduce_mean(input_tensor=td_loss)\n",
    "\n",
    "            if self._q_network.losses:\n",
    "                loss = loss + tf.reduce_mean(self._q_network.losses)\n",
    "\n",
    "            return tf_agent.LossInfo(loss, MyLossInfo(td_loss=td_loss,\n",
    "                                                     td_error=td_error))\n",
    "\n",
    "        \n",
    "    def _compute_q_values(self, time_steps, actions, training=False):\n",
    "        network_observation = time_steps.observation\n",
    "\n",
    "        q_values, _ = self._q_network(network_observation, time_steps.step_type,\n",
    "                                      training=training)\n",
    "        multi_dim_actions = self._action_spec.shape.rank > 0\n",
    "        return common.index_with_actions(\n",
    "            q_values,\n",
    "            tf.cast(actions, dtype=tf.int32),\n",
    "            multi_dim_actions=multi_dim_actions)\n",
    "    \n",
    "    def _compute_next_q_values(self, next_time_steps):\n",
    "        network_observation = next_time_steps.observation\n",
    "\n",
    "        next_target_q_values, _ = self._target_q_network(network_observation, next_time_steps.step_type)\n",
    "\n",
    "        probs = softmax_with_temp(next_target_q_values, self._temp)\n",
    "\n",
    "        v = tf.math.reduce_sum(next_target_q_values * probs, axis=1, keepdims=True)\n",
    "        return tf.squeeze(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma=0.99; temp=0.001\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = MyAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    gamma=gamma,\n",
    "    temp=temp,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the replay buffer for training\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "\n",
    "# Collect some random samples to start.\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())\n",
    "\n",
    "dynamic_step_driver.DynamicStepDriver(\n",
    "    train_env, \n",
    "    random_policy,\n",
    "    observers=[replay_buffer.add_batch],\n",
    "    num_steps=initial_collect_steps).run()\n",
    "\n",
    "# Create collection driver\n",
    "num_episodes = tf_metrics.NumberOfEpisodes()\n",
    "env_steps = tf_metrics.EnvironmentSteps()\n",
    "observers = [num_episodes, env_steps, replay_buffer.add_batch]\n",
    "\n",
    "collect_op = dynamic_step_driver.DynamicStepDriver(\n",
    "    train_env, \n",
    "    agent.collect_policy,\n",
    "    observers=observers,\n",
    "    num_steps=collect_steps_per_iteration)\n",
    "\n",
    "# Create a data set for the training loop\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 710000: loss = 2.149153709411621\n",
      "step = 720000: loss = 1.0350096225738525\n",
      "step = 730000: loss = 1.582383394241333\n",
      "step = 740000: loss = 1.2889387607574463\n",
      "step = 750000: loss = 0.8634942770004272\n",
      "step = 760000: loss = 1.226359486579895\n",
      "step = 770000: loss = 0.9879847764968872\n",
      "step = 780000: loss = 0.6483640074729919\n",
      "step = 790000: loss = 0.9350162744522095\n",
      "step = 800000: loss = 0.7048282623291016\n",
      "step = 800000: episodes=964: Average Return = 161.76890563964844\n"
     ]
    }
   ],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    collect_op.run()\n",
    "    \n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = agent.train(experience).loss\n",
    "\n",
    "    step = env_steps.result().numpy()\n",
    "    episodes = num_episodes.result().numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: episodes={1}: Average Return = {2}'.format(step, episodes, avg_return))\n",
    "        returns.append(avg_return)\n",
    "        if avg_return > 100:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([213.87143], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "rewards = 0.0\n",
    "time_step = eval_env.reset()\n",
    "while not time_step.is_last():\n",
    "    action_step = agent.policy.action(time_step)\n",
    "    time_step = eval_env.step(action_step.action)\n",
    "    rewards += time_step.reward\n",
    "    eval_py_env.render()\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
