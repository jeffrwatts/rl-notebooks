{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from envs.gridworld import GridworldEnv\n",
    "from envs.windy_gridworld import WindyGridworldEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, V, pi, gamma, theta):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.observation_space.n):\n",
    "            v = V[s]\n",
    "            bellman_update(env, V, pi, s, gamma)\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta <= theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "def bellman_update(env, V, pi, s, gamma):\n",
    "    v = 0\n",
    "    for action, p_pi in enumerate(pi[s]): \n",
    "        transitions = env.P[s][action]\n",
    "        for p, s_, r, _ in transitions:\n",
    "            v += p_pi * p * (r + gamma*V[s_])\n",
    "    \n",
    "    V[s] = v\n",
    "        \n",
    "def q_greedify_policy(env, V, pi, s, gamma):\n",
    "    q_max = -float('inf')\n",
    "    a_max = 0\n",
    "    for action, _ in enumerate(pi[s]):\n",
    "        q = 0\n",
    "        pi[s][action] = 0\n",
    "        transitions = env.P[s][action]\n",
    "        \n",
    "        for p, s_, r, _ in transitions:\n",
    "            q += p * (r + gamma*V[s_])\n",
    "            \n",
    "        if (q > q_max):\n",
    "            q_max = q\n",
    "            a_max = action\n",
    "            \n",
    "    pi[s][a_max] = 1.0\n",
    "    \n",
    "def improve_policy(env, V, pi, gamma):\n",
    "    policy_stable = True\n",
    "    for s in range(env.observation_space.n):\n",
    "        old = pi[s].copy()\n",
    "        q_greedify_policy(env, V, pi, s, gamma)\n",
    "        if not np.array_equal(pi[s], old):\n",
    "            policy_stable = False\n",
    "    return pi, policy_stable\n",
    "\n",
    "def policy_iteration(env, gamma, theta):\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    pi = np.ones((env.observation_space.n, env.action_space.n)) / env.action_space.n\n",
    "    policy_stable = False\n",
    "    while not policy_stable:\n",
    "        V = evaluate_policy(env, V, pi, gamma, theta)\n",
    "        pi, policy_stable = improve_policy(env, V, pi, gamma)\n",
    "    return V, pi\n",
    "\n",
    "def value_iteration(env, gamma, theta):\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.observation_space.n):\n",
    "            v = V[s]\n",
    "            bellman_optimality_update(env, V, s, gamma)\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta <= theta:\n",
    "            break\n",
    "    pi = np.ones((env.observation_space.n, env.action_space.n)) / env.action_space.n\n",
    "    for s in range(env.observation_space.n):\n",
    "        q_greedify_policy(env, V, pi, s, gamma)\n",
    "    return V, pi\n",
    "\n",
    "def bellman_optimality_update(env, V, s, gamma):\n",
    "    v_max = -float('inf')\n",
    "    for action in range(env.action_space.n):\n",
    "        v = 0\n",
    "        transitions = env.P[s][action]\n",
    "        \n",
    "        for p, s_, r, _ in transitions:\n",
    "            v += p * (r + gamma*V[s_])\n",
    "        \n",
    "        if (v > v_max):\n",
    "            v_max = v\n",
    "    \n",
    "    V[s] = v_max\n",
    "\n",
    "def run_policy(env, pi, episodes):\n",
    "    rewards = 0\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_rewards = 0\n",
    "        while not done:\n",
    "            state, reward, done, _ = env.step(np.argmax(pi[state]))\n",
    "            episode_rewards += reward\n",
    "\n",
    "        rewards += episode_rewards\n",
    "    return (rewards / episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_fl = gym.make('FrozenLake-v0', map_name=\"4x4\", is_slippery=False).env\n",
    "env_fl_slippery = gym.make('FrozenLake-v0', map_name=\"4x4\", is_slippery=True).env\n",
    "\n",
    "gamma_fl = 0.999\n",
    "theta_fl = 0.001\n",
    "shape_fl = (4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_fl = np.zeros(env_fl.observation_space.n)\n",
    "V_fl_slippery = np.zeros(env_fl_slippery.observation_space.n)\n",
    "\n",
    "pi_fl = np.ones((env_fl.observation_space.n, env_fl.action_space.n)) / env_fl.action_space.n\n",
    "pi_fl_slippery = np.ones((env_fl_slippery.observation_space.n, env_fl_slippery.action_space.n)) / env_fl_slippery.action_space.n\n",
    "\n",
    "V_fl, pi_fl = policy_iteration(env_fl, gamma_fl, theta_fl)\n",
    "V_fl_slippery, pi_fl_slippery = policy_iteration(env_fl_slippery, gamma_fl, theta_fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99500999 0.996006   0.997003   0.996006  ]\n",
      " [0.996006   0.         0.998001   0.        ]\n",
      " [0.997003   0.998001   0.999      0.        ]\n",
      " [0.         0.999      1.         0.        ]]\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n"
     ]
    }
   ],
   "source": [
    "print(V_fl.reshape(shape_fl))\n",
    "print(np.argmax(pi_fl, axis=1).reshape(shape_fl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.77068065 0.75840701 0.7500604  0.74585316]\n",
      " [0.77452039 0.         0.49532    0.        ]\n",
      " [0.78155756 0.79163217 0.73765149 0.        ]\n",
      " [0.         0.85853354 0.92832142 0.        ]]\n",
      "[[0 3 3 3]\n",
      " [0 0 0 0]\n",
      " [3 1 0 0]\n",
      " [0 2 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(V_fl_slippery.reshape(shape_fl))\n",
    "print(np.argmax(pi_fl_slippery, axis=1).reshape(shape_fl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env (pi) = 1.0\n",
      "env_slippery (pi_slippery) = 0.8245\n",
      "env_slippery (pi) = 0.0507\n"
     ]
    }
   ],
   "source": [
    "episodes = 10000\n",
    "print(\"env (pi) = {}\".format(run_policy(env_fl, pi_fl, episodes)))\n",
    "print(\"env_slippery (pi_slippery) = {}\".format(run_policy(env_fl_slippery, pi_fl_slippery, episodes)))\n",
    "print(\"env_slippery (pi) = {}\".format(run_policy(env_fl_slippery, pi_fl, episodes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7692161  0.75689418 0.7485434  0.74434035]\n",
      " [0.773235   0.         0.49463026 0.        ]\n",
      " [0.78054185 0.79094601 0.7371104  0.        ]\n",
      " [0.         0.8580588  0.92808042 0.        ]]\n",
      "[[0 3 3 3]\n",
      " [0 0 0 0]\n",
      " [3 1 0 0]\n",
      " [0 2 1 0]]\n"
     ]
    }
   ],
   "source": [
    "V_fl_slippery = np.zeros(env_fl_slippery.observation_space.n)\n",
    "pi_fl_slippery = np.ones((env_fl_slippery.observation_space.n, env_fl_slippery.action_space.n)) / env_fl_slippery.action_space.n\n",
    "\n",
    "V_fl_slippery, pi_fl_slippery = value_iteration(env_fl_slippery, gamma_fl, theta_fl)\n",
    "\n",
    "print(V_fl_slippery.reshape(shape_fl))\n",
    "print(np.argmax(pi_fl_slippery, axis=1).reshape(shape_fl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_grid = GridworldEnv()\n",
    "\n",
    "gamma_grid = 1\n",
    "theta_grid = 0\n",
    "shape_grid=(4, 4)\n",
    "\n",
    "V_grid = np.zeros(env_grid.observation_space.n)\n",
    "pi_grid = np.ones((env_grid.observation_space.n, env_grid.action_space.n)) / env_grid.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "[[0 3 3 3]\n",
      " [0 0 2 2]\n",
      " [0 1 2 2]\n",
      " [0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "V_grid=evaluate_policy(env_grid, V_grid, pi_grid, gamma_grid, theta_grid)\n",
    "pi_grid, _ = improve_policy(env_grid, V_grid, pi_grid, gamma_grid)\n",
    "print(V_grid.reshape(shape_grid))\n",
    "print(np.argmax(pi_grid, axis=1).reshape(shape_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "[[0 3 3 2]\n",
      " [0 0 0 2]\n",
      " [0 0 1 2]\n",
      " [0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "V_grid, pi_grid = value_iteration(env_grid, gamma_grid, theta_grid)\n",
    "print(V_grid.reshape(shape_grid))\n",
    "print(np.argmax(pi_grid, axis=1).reshape(shape_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cliff Walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_cliff = gym.make('CliffWalking-v0')\n",
    "\n",
    "gamma_cliff = 0.9\n",
    "theta_cliff = 0.001\n",
    "shape_cliff=(4, 12)\n",
    "\n",
    "V_cliff = np.zeros(env_cliff.observation_space.n)\n",
    "pi_cliff = np.ones((env_cliff.observation_space.n, env_cliff.action_space.n)) / env_cliff.action_space.n\n",
    "\n",
    "# Need to differentiate end transition reward to get dynamic programming to work\n",
    "env_cliff.P[47][0][0]=(1.0, 47, 0, True)\n",
    "env_cliff.P[47][1][0]=(1.0, 47, 0, True)\n",
    "env_cliff.P[47][2][0]=(1.0, 47, 0, True)\n",
    "env_cliff.P[47][3][0]=(1.0, 47, 0, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.71232075 -7.45813417 -7.17570464 -6.86189404 -6.5132156  -6.12579511\n",
      "  -5.6953279  -5.217031   -4.68559    -4.0951     -3.439      -2.71      ]\n",
      " [-7.45813417 -7.17570464 -6.86189404 -6.5132156  -6.12579511 -5.6953279\n",
      "  -5.217031   -4.68559    -4.0951     -3.439      -2.71       -1.9       ]\n",
      " [-7.17570464 -6.86189404 -6.5132156  -6.12579511 -5.6953279  -5.217031\n",
      "  -4.68559    -4.0951     -3.439      -2.71       -1.9        -1.        ]\n",
      " [-7.45813417 -7.17570464 -6.86189404 -6.5132156  -6.12579511 -5.6953279\n",
      "  -5.217031   -4.68559    -4.0951     -3.439      -1.          0.        ]]\n",
      "[[1 1 1 1 1 1 1 1 1 1 1 2]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 2]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 2]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "V_cliff, pi_cliff = value_iteration(env_cliff, gamma_cliff, theta_cliff)\n",
    "print(V_cliff.reshape(shape_cliff))\n",
    "print(np.argmax(pi_cliff, axis=1).reshape(shape_cliff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-13.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_policy(env_cliff, pi_cliff, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.71232075 -7.45813417 -7.17570464 -6.86189404 -6.5132156  -6.12579511\n",
      "  -5.6953279  -5.217031   -4.68559    -4.0951     -3.439      -2.71      ]\n",
      " [-7.45813417 -7.17570464 -6.86189404 -6.5132156  -6.12579511 -5.6953279\n",
      "  -5.217031   -4.68559    -4.0951     -3.439      -2.71       -1.9       ]\n",
      " [-7.17570464 -6.86189404 -6.5132156  -6.12579511 -5.6953279  -5.217031\n",
      "  -4.68559    -4.0951     -3.439      -2.71       -1.9        -1.        ]\n",
      " [-7.45813417 -7.17570464 -6.86189404 -6.5132156  -6.12579511 -5.6953279\n",
      "  -5.217031   -4.68559    -4.0951     -3.439      -1.          0.        ]]\n",
      "[[1 1 1 1 1 1 1 1 1 1 1 2]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 2]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 2]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "V_cliff, pi_cliff = policy_iteration(env_cliff, gamma_cliff, theta_cliff)\n",
    "print(V_cliff.reshape(shape_cliff))\n",
    "print(np.argmax(pi_cliff, axis=1).reshape(shape_cliff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-13.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_policy(env_cliff, pi_cliff, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windy Grid World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_windy = WindyGridworldEnv()\n",
    "\n",
    "gamma_windy = 0.9\n",
    "theta_windy = 0.001\n",
    "shape_windy=(7, 10)\n",
    "\n",
    "V_windy = np.zeros(env_windy.observation_space.n)\n",
    "pi_windy = np.ones((env_windy.observation_space.n, env_windy.action_space.n)) / env_windy.action_space.n\n",
    "\n",
    "# Need to differentiate end state reward to get dynamic programming to work\n",
    "env_windy.P[37][0][0] = (1.0, 37, 0, True)\n",
    "env_windy.P[37][1][0] = (1.0, 37, 0, True)\n",
    "env_windy.P[37][2][0] = (1.0, 37, 0, True)\n",
    "env_windy.P[37][3][0] = (1.0, 37, 0, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.94108868 -7.71232075 -7.45813417 -7.17570464 -6.86189404 -6.5132156\n",
      "  -6.12579511 -5.6953279  -5.217031   -4.68559   ]\n",
      " [-7.94108868 -7.71232075 -7.45813417 -7.17570464 -6.86189404 -6.5132156\n",
      "  -6.12579511 -5.6953279  -5.217031   -4.0951    ]\n",
      " [-7.94108868 -7.71232075 -7.45813417 -7.17570464 -6.86189404 -6.5132156\n",
      "  -6.12579511 -5.6953279  -4.68559    -3.439     ]\n",
      " [-7.94108868 -7.71232075 -7.45813417 -7.17570464 -6.86189404 -6.5132156\n",
      "  -6.12579511  0.         -4.0951     -2.71      ]\n",
      " [-7.94108868 -7.71232075 -7.45813417 -7.17570464 -6.86189404 -6.5132156\n",
      "  -6.12579511 -1.         -1.         -1.9       ]\n",
      " [-7.94108868 -7.71232075 -7.45813417 -7.17570464 -6.86189404 -6.5132156\n",
      "  -1.         -1.9        -1.9        -2.71      ]\n",
      " [-7.94108868 -7.71232075 -7.45813417 -7.17570464 -6.86189404 -1.9\n",
      "  -1.9        -1.         -1.9        -2.71      ]]\n",
      "[[1 1 1 1 1 1 1 1 1 2]\n",
      " [1 1 1 1 1 1 1 1 1 2]\n",
      " [1 1 1 1 1 1 1 1 1 2]\n",
      " [1 1 1 1 1 1 1 0 1 2]\n",
      " [1 1 1 1 1 1 1 2 3 3]\n",
      " [1 1 1 1 1 1 1 2 3 0]\n",
      " [1 1 1 1 1 1 1 0 0 3]]\n"
     ]
    }
   ],
   "source": [
    "V_windy, pi_windy = value_iteration(env_windy, gamma_windy, theta_windy)\n",
    "print(V_windy.reshape(shape_windy))\n",
    "print(np.argmax(pi_windy, axis=1).reshape(shape_windy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-15.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_policy(env_windy, pi_windy, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.94108868 -7.71232075 -7.45813417 -7.17570464 -6.86189404 -6.5132156\n",
      "  -6.12579511 -5.6953279  -5.217031   -4.68559   ]\n",
      " [-7.94108868 -7.71232075 -7.45813417 -7.17570464 -6.86189404 -6.5132156\n",
      "  -6.12579511 -5.6953279  -5.217031   -4.0951    ]\n",
      " [-7.94108868 -7.71232075 -7.45813417 -7.17570464 -6.86189404 -6.5132156\n",
      "  -6.12579511 -5.6953279  -4.68559    -3.439     ]\n",
      " [-7.94108868 -7.71232075 -7.45813417 -7.17570464 -6.86189404 -6.5132156\n",
      "  -6.12579511  0.         -4.0951     -2.71      ]\n",
      " [-7.94108868 -7.71232075 -7.45813417 -7.17570464 -6.86189404 -6.5132156\n",
      "  -6.12579511 -1.         -1.         -1.9       ]\n",
      " [-7.94108868 -7.71232075 -7.45813417 -7.17570464 -6.86189404 -6.5132156\n",
      "  -1.         -1.9        -1.9        -2.71      ]\n",
      " [-7.94108868 -7.71232075 -7.45813417 -7.17570464 -6.86189404 -1.9\n",
      "  -1.9        -1.         -1.9        -2.71      ]]\n",
      "[[1 1 1 1 1 1 1 1 1 2]\n",
      " [1 1 1 1 1 1 1 1 1 2]\n",
      " [1 1 1 1 1 1 1 1 1 2]\n",
      " [1 1 1 1 1 1 1 0 1 2]\n",
      " [1 1 1 1 1 1 1 2 3 3]\n",
      " [1 1 1 1 1 1 1 2 3 0]\n",
      " [1 1 1 1 1 1 1 0 0 3]]\n"
     ]
    }
   ],
   "source": [
    "V_windy, pi_windy = policy_iteration(env_windy, gamma_windy, theta_windy)\n",
    "print(V_windy.reshape(shape_windy))\n",
    "print(np.argmax(pi_windy, axis=1).reshape(shape_windy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-15.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_policy(env_windy, pi_windy, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
