{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from envs.gridworld import GridworldEnv\n",
    "from envs.windy_gridworld import WindyGridworldEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, V, pi, gamma, theta):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.observation_space.n):\n",
    "            v = V[s]\n",
    "            bellman_update(env, V, pi, s, gamma)\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta <= theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "def bellman_update(env, V, pi, s, gamma):\n",
    "    v = 0\n",
    "    for action, p_pi in enumerate(pi[s]): \n",
    "        transitions = env.P[s][action]\n",
    "        for p, s_, r, _ in transitions:\n",
    "            v += p_pi * p * (r + gamma*V[s_])\n",
    "    \n",
    "    V[s] = v\n",
    "    \n",
    "def q_greedify_policy(env, V, pi, s, gamma):\n",
    "    q_max = -float('inf')\n",
    "    a_max = 0\n",
    "    for action, _ in enumerate(pi[s]):\n",
    "        q = 0\n",
    "        pi[s][action] = 0\n",
    "        transitions = env.P[s][action]\n",
    "        \n",
    "        for p, s_, r, _ in transitions:\n",
    "            q += p * (r + gamma*V[s_])\n",
    "            \n",
    "        if (q > q_max):\n",
    "            q_max = q\n",
    "            a_max = action\n",
    "            \n",
    "    pi[s][a_max] = 1.0\n",
    "    \n",
    "def improve_policy(env, V, pi, gamma):\n",
    "    policy_stable = True\n",
    "    for s in range(env.observation_space.n):\n",
    "        old = pi[s].copy()\n",
    "        q_greedify_policy(env, V, pi, s, gamma)\n",
    "        if not np.array_equal(pi[s], old):\n",
    "            policy_stable = False\n",
    "    return pi, policy_stable\n",
    "\n",
    "def policy_iteration(env, gamma, theta):\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    pi = np.ones((env.observation_space.n, env.action_space.n)) / env.action_space.n\n",
    "    policy_stable = False\n",
    "    while not policy_stable:\n",
    "        V = evaluate_policy(env, V, pi, gamma, theta)\n",
    "        pi, policy_stable = improve_policy(env, V, pi, gamma)\n",
    "    return V, pi\n",
    "\n",
    "def value_iteration(env, gamma, theta):\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.observation_space.n):\n",
    "            v = V[s]\n",
    "            bellman_optimality_update(env, V, s, gamma)\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta <= theta:\n",
    "            break\n",
    "    pi = np.ones((env.observation_space.n, env.action_space.n)) / env.action_space.n\n",
    "    for s in range(env.observation_space.n):\n",
    "        q_greedify_policy(env, V, pi, s, gamma)\n",
    "    return V, pi\n",
    "\n",
    "def bellman_optimality_update(env, V, s, gamma):\n",
    "    v_max = -float('inf')\n",
    "    for action in range(env.action_space.n):\n",
    "        v = 0\n",
    "        transitions = env.P[s][action]\n",
    "        \n",
    "        for p, s_, r, _ in transitions:\n",
    "            v += p * (r + gamma*V[s_])\n",
    "        \n",
    "        if (v > v_max):\n",
    "            v_max = v\n",
    "    \n",
    "    V[s] = v_max\n",
    "\n",
    "def run_policy(env, pi, episodes):\n",
    "    rewards = 0\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_rewards = 0\n",
    "        while not done:\n",
    "            state, reward, done, _ = env.step(np.argmax(pi[state]))\n",
    "            episode_rewards += reward\n",
    "\n",
    "        rewards += episode_rewards\n",
    "    return (rewards / episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_fl = gym.make('FrozenLake-v0', map_name=\"4x4\", is_slippery=False).env\n",
    "env_fl_slippery = gym.make('FrozenLake-v0', map_name=\"4x4\", is_slippery=True).env\n",
    "\n",
    "gamma_fl = 0.999\n",
    "theta_fl = 0\n",
    "shape_fl = (4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_fl = np.zeros(env_fl.observation_space.n)\n",
    "V_fl_slippery = np.zeros(env_fl_slippery.observation_space.n)\n",
    "\n",
    "pi_fl = np.ones((env_fl.observation_space.n, env_fl.action_space.n)) / env_fl.action_space.n\n",
    "pi_fl_slippery = np.ones((env_fl_slippery.observation_space.n, env_fl_slippery.action_space.n)) / env_fl_slippery.action_space.n\n",
    "\n",
    "V_fl, pi_fl = policy_iteration(env_fl, gamma_fl, theta_fl)\n",
    "V_fl_slippery, pi_fl_slippery = policy_iteration(env_fl_slippery, gamma_fl, theta_fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99500999 0.996006   0.997003   0.996006  ]\n",
      " [0.996006   0.         0.998001   0.        ]\n",
      " [0.997003   0.998001   0.999      0.        ]\n",
      " [0.         0.999      1.         0.        ]]\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n"
     ]
    }
   ],
   "source": [
    "print(V_fl.reshape(shape_fl))\n",
    "print(np.argmax(pi_fl, axis=1).reshape(shape_fl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.78553326 0.77855409 0.77391292 0.77159582]\n",
      " [0.78789222 0.         0.50573092 0.        ]\n",
      " [0.79261722 0.79972245 0.74479855 0.        ]\n",
      " [0.         0.86415315 0.93117891 0.        ]]\n",
      "[[0 3 3 3]\n",
      " [0 0 0 0]\n",
      " [3 1 0 0]\n",
      " [0 2 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(V_fl_slippery.reshape(shape_fl))\n",
    "print(np.argmax(pi_fl_slippery, axis=1).reshape(shape_fl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env (pi) = 1.0\n",
      "env_slippery (pi_slippery) = 0.8241\n",
      "env_slippery (pi) = 0.045\n"
     ]
    }
   ],
   "source": [
    "episodes = 10000\n",
    "print(\"env (pi) = {}\".format(run_policy(env_fl, pi_fl, episodes)))\n",
    "print(\"env_slippery (pi_slippery) = {}\".format(run_policy(env_fl_slippery, pi_fl_slippery, episodes)))\n",
    "print(\"env_slippery (pi) = {}\".format(run_policy(env_fl_slippery, pi_fl, episodes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.78553326 0.77855409 0.77391292 0.77159582]\n",
      " [0.78789222 0.         0.50573092 0.        ]\n",
      " [0.79261722 0.79972245 0.74479855 0.        ]\n",
      " [0.         0.86415315 0.93117891 0.        ]]\n",
      "[[0 3 3 3]\n",
      " [0 0 0 0]\n",
      " [3 1 0 0]\n",
      " [0 2 1 0]]\n"
     ]
    }
   ],
   "source": [
    "V_fl_slippery = np.zeros(env_fl_slippery.observation_space.n)\n",
    "pi_fl_slippery = np.ones((env_fl_slippery.observation_space.n, env_fl_slippery.action_space.n)) / env_fl_slippery.action_space.n\n",
    "\n",
    "V_fl_slippery, pi_fl_slippery = value_iteration(env_fl_slippery, gamma_fl, theta_fl)\n",
    "\n",
    "print(V_fl_slippery.reshape(shape_fl))\n",
    "print(np.argmax(pi_fl_slippery, axis=1).reshape(shape_fl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_grid = GridworldEnv()\n",
    "\n",
    "gamma_grid = 1\n",
    "theta_grid = 0\n",
    "shape_grid=(4, 4)\n",
    "\n",
    "V_grid = np.zeros(env_grid.observation_space.n)\n",
    "pi_grid = np.ones((env_grid.observation_space.n, env_grid.action_space.n)) / env_grid.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "[[0 3 3 3]\n",
      " [0 0 2 2]\n",
      " [0 1 2 2]\n",
      " [0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "V_grid=evaluate_policy(env_grid, V_grid, pi_grid, gamma_grid, theta_grid)\n",
    "pi_grid, _ = improve_policy(env_grid, V_grid, pi_grid, gamma_grid)\n",
    "print(V_grid.reshape(shape_grid))\n",
    "print(np.argmax(pi_grid, axis=1).reshape(shape_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cliff Walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_cliff = gym.make('CliffWalking-v0')\n",
    "\n",
    "gamma_cliff = 0.9\n",
    "theta_cliff = 0.01\n",
    "shape_cliff=(4, 12)\n",
    "\n",
    "V_cliff = np.zeros(env_cliff.observation_space.n)\n",
    "pi_cliff = np.ones((env_cliff.observation_space.n, env_cliff.action_space.n)) / env_cliff.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[[-10.02436955 -10.0219326  -10.01973934 -10.0177654  -10.01598886\n",
      "  -10.01438998 -10.01295098 -10.01165588 -10.03715712 -10.03344141\n",
      "  -10.03344141 -10.03344141]\n",
      " [-10.0219326  -10.01973934 -10.0177654  -10.01598886 -10.01438998\n",
      "  -10.01295098 -10.01165588 -10.01049029 -10.03344141 -10.03009727\n",
      "  -10.03009727 -10.03009727]\n",
      " [-10.01973934 -10.0177654  -10.01598886 -10.01438998 -10.01295098\n",
      "  -10.01165588 -10.01049029 -10.00944126 -10.03009727 -10.02708754\n",
      "  -10.02708754 -10.02708754]\n",
      " [-10.0177654  -10.01598886 -10.01438998 -10.01295098 -10.01165588\n",
      "  -10.01049029 -10.00944126 -10.00849714 -10.02708754 -10.02437879\n",
      "  -10.02437879 -10.02437879]]\n",
      "[[1 1 1 1 1 1 1 2 3 2 2 2]\n",
      " [1 1 1 1 1 1 1 2 3 2 2 2]\n",
      " [1 1 1 1 1 1 1 0 3 1 1 2]\n",
      " [2 0 0 0 0 0 0 0 0 0 1 1]]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(pi_cliff)\n",
    "\n",
    "\n",
    "V_cliff = evaluate_policy(env_cliff, V_cliff, pi_cliff, gamma_cliff, theta_cliff)\n",
    "pi_cliff, policy_stable = improve_policy(env_cliff, V_cliff, pi_cliff, gamma_cliff)\n",
    "print(V_cliff.reshape(shape_cliff))\n",
    "print(np.argmax(pi_cliff, axis=1).reshape(shape_cliff))\n",
    "print(policy_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-10. -10. -10. -10. -10. -10. -10. -10. -10. -10. -10. -10.]\n",
      " [-10. -10. -10. -10. -10. -10. -10. -10. -10. -10. -10. -10.]\n",
      " [-10. -10. -10. -10. -10. -10. -10. -10. -10. -10. -10. -10.]\n",
      " [-10. -10. -10. -10. -10. -10. -10. -10. -10. -10. -10. -10.]]\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "V_cliff, pi_cliff = policy_iteration(env_cliff, gamma_cliff, theta_cliff)\n",
    "print(V_cliff.reshape(shape_cliff))\n",
    "print(np.argmax(pi_cliff, axis=1).reshape(shape_cliff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windy Grid World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_windy = WindyGridworldEnv()\n",
    "\n",
    "gamma_windy = 0.9\n",
    "theta_windy = 0\n",
    "shape_windy=(7, 10)\n",
    "\n",
    "V_windy = np.zeros(env_windy.observation_space.n)\n",
    "pi_windy = np.ones((env_windy.observation_space.n, env_windy.action_space.n)) / env_windy.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-10. -10. -10. -10. -10. -10. -10. -10. -10. -10.]\n",
      " [-10. -10. -10. -10. -10. -10. -10. -10. -10. -10.]\n",
      " [-10. -10. -10. -10. -10. -10. -10. -10. -10. -10.]\n",
      " [-10. -10. -10. -10. -10. -10. -10. -10. -10. -10.]\n",
      " [-10. -10. -10. -10. -10. -10. -10. -10. -10. -10.]\n",
      " [-10. -10. -10. -10. -10. -10. -10. -10. -10. -10.]\n",
      " [-10. -10. -10. -10. -10. -10. -10. -10. -10. -10.]]\n",
      "[[0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "V_windy = evaluate_policy(env_windy, V_windy, pi_windy, gamma_windy, theta_windy)\n",
    "pi_windy, policy_stable = improve_policy(env_windy, V_windy, pi_windy, gamma_windy)\n",
    "print(V_windy.reshape(shape_windy))\n",
    "print(np.argmax(pi_windy, axis=1).reshape(shape_windy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-10. -10. -10. -10. -10. -10. -10. -10. -10. -10.]\n",
      " [-10. -10. -10. -10. -10. -10. -10. -10. -10. -10.]\n",
      " [-10. -10. -10. -10. -10. -10. -10. -10. -10. -10.]\n",
      " [-10. -10. -10. -10. -10. -10. -10. -10. -10. -10.]\n",
      " [-10. -10. -10. -10. -10. -10. -10. -10. -10. -10.]\n",
      " [-10. -10. -10. -10. -10. -10. -10. -10. -10. -10.]\n",
      " [-10. -10. -10. -10. -10. -10. -10. -10. -10. -10.]]\n",
      "[[0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "V_windy, pi_windy = policy_iteration(env_windy, gamma_windy, theta_windy)\n",
    "print(V_windy.reshape(shape_windy))\n",
    "print(np.argmax(pi_windy, axis=1).reshape(shape_windy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
