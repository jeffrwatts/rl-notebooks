{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.specs import distribution_spec\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.trajectories import policy_step\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.networks import network\n",
    "from tf_agents.policies import tf_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.agents import tf_agent\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.utils import nest_utils\n",
    "from tf_agents.utils import common\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_name = \"Pendulum-v0\"\n",
    "env_name = \"LunarLanderContinuous-v2\" \n",
    "\n",
    "num_iterations = 100000 \n",
    "\n",
    "initial_collect_steps = 10000  \n",
    "collect_steps_per_iteration = 1 \n",
    "replay_buffer_max_length = num_iterations \n",
    "\n",
    "batch_size = 256 \n",
    "\n",
    "value_learning_rate = 3e-4 \n",
    "softq_learning_rate = 3e-4 \n",
    "policy_learning_rate = 3e-4 \n",
    "target_update_tau = 0.005 \n",
    "target_update_period = 1 \n",
    "gamma = 0.99 \n",
    "\n",
    "value_fc_layer_params = (256, 256)\n",
    "softq_fc_layer_params = (256, 256)\n",
    "policy_fc_layer_params = (256, 256)\n",
    "\n",
    "log_interval = 200#5000 \n",
    "\n",
    "num_eval_episodes = 10 \n",
    "eval_interval = 5000#10000 \n",
    "max_episode_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environments\n",
    "train_py_env = suite_gym.load(env_name)#, max_episode_steps=max_episode_steps)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "\n",
    "time_step_spec = train_env.time_step_spec()\n",
    "observation_spec = train_env.observation_spec()\n",
    "action_spec = train_env.action_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions and Networks\n",
    "class ValueNetwork(network.Network):\n",
    "    def __init__(self,\n",
    "                 observation_spec,\n",
    "                 fc_layer_params,\n",
    "                 init_w = 3e-3,\n",
    "                 name='ValueNetwork'):\n",
    "        \n",
    "        super(ValueNetwork, self).__init__(\n",
    "            input_tensor_spec=observation_spec,\n",
    "            state_spec=(),\n",
    "            name=name)\n",
    "          \n",
    "        self._encoding_layers = []\n",
    "        for num_units in fc_layer_params:\n",
    "            self._encoding_layers.append(tf.keras.layers.Dense(\n",
    "                num_units,\n",
    "                activation=tf.keras.activations.relu,\n",
    "                kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_w, maxval=init_w),\n",
    "                name='%s/dense' % name))\n",
    "        \n",
    "        self._value = tf.keras.layers.Dense(\n",
    "            1,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_w, maxval=init_w),\n",
    "            name='value')\n",
    "\n",
    "\n",
    "    def call(self, inputs, step_type=(), network_state=(), training=False):\n",
    "        encoding = inputs\n",
    "        \n",
    "        for layer in self._encoding_layers:\n",
    "            encoding = layer(encoding, training=training)\n",
    "\n",
    "        value = self._value(encoding, training=training)\n",
    "        return tf.reshape(value, [-1]), network_state\n",
    "\n",
    "class SoftQNetwork(network.Network):\n",
    "    def __init__(self,\n",
    "                 observation_and_action_spec,\n",
    "                 fc_layer_params,\n",
    "                 init_w = 3e-3,\n",
    "                 name='SoftQNetwork'):\n",
    "        \n",
    "        super(SoftQNetwork, self).__init__(\n",
    "            input_tensor_spec=observation_and_action_spec,\n",
    "            state_spec=(),\n",
    "            name=name)\n",
    "          \n",
    "        self._encoding_layers = []\n",
    "        for num_units in fc_layer_params:\n",
    "            self._encoding_layers.append(tf.keras.layers.Dense(\n",
    "                num_units,\n",
    "                activation=tf.keras.activations.relu,\n",
    "                kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_w, maxval=init_w),\n",
    "                name='%s/dense' % name))\n",
    "        \n",
    "        self._value = tf.keras.layers.Dense(\n",
    "            1,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_w, maxval=init_w),\n",
    "            name='value')\n",
    "\n",
    "\n",
    "    def call(self, inputs, step_type=(), network_state=(), training=False):\n",
    "        observations, actions = inputs\n",
    "        encoding = tf.concat([observations, actions], 1)\n",
    "        \n",
    "        for layer in self._encoding_layers:\n",
    "            encoding = layer(encoding, training=training)\n",
    "\n",
    "        value = self._value(encoding, training=training)\n",
    "        return tf.reshape(value, [-1]), network_state\n",
    "\n",
    "def spec_means_and_magnitudes(action_spec):\n",
    "    action_means = tf.nest.map_structure(\n",
    "        lambda spec: (spec.maximum + spec.minimum) / 2.0, action_spec)\n",
    "    action_magnitudes = tf.nest.map_structure(\n",
    "        lambda spec: (spec.maximum - spec.minimum) / 2.0, action_spec)\n",
    "    return tf.cast(action_means, dtype=tf.float32), tf.cast(action_magnitudes, dtype=tf.float32) \n",
    "    \n",
    "class PolicyNetwork(network.DistributionNetwork):\n",
    "    def __init__(self,\n",
    "                 observation_spec,\n",
    "                 action_spec,\n",
    "                 fc_layer_params,\n",
    "                 init_w=3e-3, \n",
    "                 log_std_min=-20, \n",
    "                 log_std_max=2,\n",
    "                 name=\"ActorNormalDistributionNetwork\"):\n",
    "        \n",
    "        action_dist_spec = self._build_distribution_spec(action_spec, name) \n",
    "        \n",
    "        super(PolicyNetwork, self).__init__(\n",
    "            input_tensor_spec=observation_spec,\n",
    "            state_spec=(),\n",
    "            output_spec=action_dist_spec,\n",
    "            name=name)\n",
    "        \n",
    "        self._log_std_min = log_std_min\n",
    "        self._log_std_max = log_std_max\n",
    "\n",
    "        num_actions = action_spec.shape.num_elements()\n",
    "          \n",
    "        self._encoding_layers = []\n",
    "        for num_units in fc_layer_params:\n",
    "            self._encoding_layers.append(tf.keras.layers.Dense(\n",
    "                num_units,\n",
    "                activation=tf.keras.activations.relu,\n",
    "                kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_w, maxval=init_w),\n",
    "                name='%s/dense' % name))\n",
    "        \n",
    "        self._means_linear = tf.keras.layers.Dense(\n",
    "            num_actions,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_w, maxval=init_w),\n",
    "            name='means_linear')\n",
    "\n",
    "        # standard dev layer for distribution\n",
    "        self._log_std_linear = tf.keras.layers.Dense(\n",
    "            num_actions,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_w, maxval=init_w),\n",
    "            name='log_std_linear')\n",
    "        \n",
    "        action_means, action_magnitudes = spec_means_and_magnitudes(action_spec)\n",
    "        bijectors = [tfp.bijectors.Shift(action_means),\n",
    "                     tfp.bijectors.Scale(action_magnitudes),\n",
    "                     tfp.bijectors.Tanh()]\n",
    "\n",
    "        self._bijector_chain = tfp.bijectors.Chain(bijectors)\n",
    "        \n",
    "        \n",
    "    def _build_distribution_spec(self, sample_spec, network_name):\n",
    "        input_param_shapes = tfp.distributions.Normal.param_static_shapes(sample_spec.shape)\n",
    "\n",
    "        input_param_spec = {\n",
    "            name: tensor_spec.TensorSpec(  \n",
    "                shape=shape,\n",
    "                dtype=sample_spec.dtype,\n",
    "                name=network_name + '_' + name)\n",
    "            for name, shape in input_param_shapes.items()\n",
    "        }\n",
    "\n",
    "        return distribution_spec.DistributionSpec(None, input_param_spec, sample_spec=sample_spec)\n",
    "\n",
    "    \n",
    "    def call(self, observations, step_type, network_state, training=False):  \n",
    "        # Feed through fc layers.\n",
    "        encoding = observations\n",
    "        \n",
    "        for layer in self._encoding_layers:\n",
    "            encoding = layer(encoding, training=training)\n",
    "        \n",
    "        # Compute means.\n",
    "        means = self._means_linear(encoding, training=training)\n",
    "\n",
    "        # Compute stds:  (take log of std, clip, and exponentiate to get std.)\n",
    "        log_stds = self._log_std_linear(encoding, training=training)\n",
    "        log_stds = tf.clip_by_value(log_stds, self._log_std_min, self._log_std_max)\n",
    "        stds = tf.exp(log_stds)\n",
    "        \n",
    "        # Build a distribution using the means and stds.\n",
    "        distribution = tfp.distributions.Normal(loc=means, scale=stds)\n",
    "        \n",
    "        # Take the TanH and shift and scale to fit action spec.\n",
    "        distribution = tfp.distributions.TransformedDistribution(distribution=distribution, bijector=self._bijector_chain)\n",
    "        \n",
    "        return distribution, network_state\n",
    "    \n",
    "class ActorPolicy(tf_policy.Base):\n",
    "    def __init__(self,\n",
    "        time_step_spec,\n",
    "        action_spec,\n",
    "        policy_network,\n",
    "        training=False):\n",
    "\n",
    "        policy_network.create_variables()\n",
    "        self._policy_network = policy_network\n",
    "        self._training = training\n",
    "\n",
    "        super(ActorPolicy, self).__init__(\n",
    "            time_step_spec=time_step_spec,\n",
    "            action_spec=action_spec,\n",
    "            policy_state_spec=policy_network.state_spec)\n",
    "\n",
    "    def _variables(self):\n",
    "        return self._policy_network.variables\n",
    "\n",
    "    def _distribution(self, time_step, policy_state):\n",
    "        distributions, policy_state = self._policy_network(time_step.observation,\n",
    "                                                           time_step.step_type,\n",
    "                                                           policy_state,\n",
    "                                                           training=self._training)\n",
    "\n",
    "        return policy_step.PolicyStep(distributions, policy_state)\n",
    "    \n",
    "def actions_and_logprobs(policy, time_steps, epsilon=1e-6):\n",
    "    action_distribution = policy.distribution(time_steps).action\n",
    "    actions = action_distribution.sample()\n",
    "    log_probs = action_distribution.log_prob(actions) - tf.math.log(1-tf.math.pow(actions, 2) + epsilon)\n",
    "    log_probs = tf.reduce_sum(input_tensor=log_probs, axis=1)\n",
    "    return actions, log_probs, action_distribution.distribution.loc, action_distribution.distribution.scale\n",
    "\n",
    "def experience_to_transitions(experience):\n",
    "    transitions = trajectory.to_transition(experience)\n",
    "    time_steps, policy_steps, next_time_steps = transitions\n",
    "    actions = policy_steps.action\n",
    "    time_steps, actions, next_time_steps = tf.nest.map_structure(\n",
    "        lambda t: tf.squeeze(t, axis=1),\n",
    "        (time_steps, actions, next_time_steps))\n",
    "    return time_steps, actions, next_time_steps\n",
    "\n",
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create networks\n",
    "value_network = ValueNetwork(observation_spec, value_fc_layer_params)\n",
    "value_network.create_variables()\n",
    "\n",
    "target_value_network = value_network.copy(name='TargetValueNetwork')\n",
    "target_value_network.create_variables()\n",
    "\n",
    "softq_network = SoftQNetwork((observation_spec, action_spec), softq_fc_layer_params)\n",
    "softq_network.create_variables()\n",
    "\n",
    "policy_network = PolicyNetwork(observation_spec, action_spec, policy_fc_layer_params)\n",
    "policy_network.create_variables()\n",
    "\n",
    "collect_policy = ActorPolicy(time_step_spec, action_spec, policy_network, training=False)\n",
    "train_policy = ActorPolicy(time_step_spec, action_spec, policy_network, training=True)\n",
    "\n",
    "# Full copy of network variables.\n",
    "common.soft_variables_update(\n",
    "    value_network.variables,\n",
    "    target_value_network.variables,\n",
    "    tau=1.0)\n",
    "\n",
    "# Optimizers\n",
    "value_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=value_learning_rate)\n",
    "softq_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=softq_learning_rate)\n",
    "policy_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=policy_learning_rate)\n",
    "\n",
    "# Loss Objects\n",
    "value_loss_fn  = tf.compat.v1.losses.mean_squared_error\n",
    "softq_loss_fn = tf.compat.v1.losses.mean_squared_error\n",
    "\n",
    "# Create the replay buffer for training\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=collect_policy.trajectory_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "\n",
    "# Collect some random samples to start.\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())\n",
    "\n",
    "dynamic_step_driver.DynamicStepDriver(\n",
    "    train_env, \n",
    "    random_policy,\n",
    "    observers=[replay_buffer.add_batch],\n",
    "    num_steps=initial_collect_steps).run()\n",
    "\n",
    "# Create collection driver\n",
    "num_episodes = tf_metrics.NumberOfEpisodes()\n",
    "env_steps = tf_metrics.EnvironmentSteps()\n",
    "observers = [num_episodes, env_steps, replay_buffer.add_batch]\n",
    "\n",
    "collect_op = dynamic_step_driver.DynamicStepDriver(\n",
    "    train_env, \n",
    "    collect_policy,\n",
    "    observers=observers,\n",
    "    num_steps=collect_steps_per_iteration)\n",
    "\n",
    "# Create a data set for the training loop\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Functions\n",
    "def compute_softq_loss(time_steps, \n",
    "                       actions, \n",
    "                       next_time_steps,\n",
    "                       gamma=0.99):\n",
    "    expected_q_values,_ = softq_network((time_steps.observation, actions))    \n",
    "    expected_values,_ = value_network(time_steps.observation)\n",
    "    target_values,_ = target_value_network(next_time_steps.observation)\n",
    "    next_q_values = tf.stop_gradient(next_time_steps.reward + \n",
    "                                     tf.where(next_time_steps.is_last(), 0.0, gamma) * target_values)\n",
    "    softq_loss = softq_loss_fn(expected_q_values, next_q_values)\n",
    "    return softq_loss\n",
    "\n",
    "\n",
    "\n",
    "def compute_value_loss(time_steps):    \n",
    "    expected_values,_ = value_network(time_steps.observation)    \n",
    "    new_actions, log_probs,_,_ = actions_and_logprobs(train_policy, time_steps)    \n",
    "    expected_new_q_values,_ = softq_network((time_steps.observation, new_actions))    \n",
    "    next_values = tf.stop_gradient(expected_new_q_values - log_probs)    \n",
    "    value_loss = value_loss_fn(expected_values, next_values)\n",
    "    \n",
    "    return value_loss\n",
    "\n",
    "def compute_policy_loss(time_steps,\n",
    "                        mean_lambda=1e-3,\n",
    "                        std_lambda=1e-3):\n",
    "    \n",
    "    new_actions, log_probs, means, stds = actions_and_logprobs(train_policy, time_steps)\n",
    "    expected_new_q_values,_ = softq_network((time_steps.observation, new_actions))\n",
    "    expected_values,_ = value_network(time_steps.observation)\n",
    "    \n",
    "    log_prob_targets = expected_new_q_values - expected_values\n",
    "    policy_loss = tf.reduce_mean(log_probs * (log_probs - log_prob_targets))\n",
    "    \n",
    "    mean_loss = tf.reduce_mean(mean_lambda * tf.math.pow(means, 2))\n",
    "    std_loss = tf.reduce_mean(std_lambda * tf.math.pow(tf.math.log(stds), 2))\n",
    "    \n",
    "    policy_loss = policy_loss + mean_loss + std_loss\n",
    "    \n",
    "    return policy_loss\n",
    "    \n",
    "def train(experience):\n",
    "    time_steps, actions, next_time_steps = experience_to_transitions(experience)    \n",
    "    \n",
    "    # Soft Q Network Update\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        tape.watch(softq_network.trainable_variables)\n",
    "        softq_loss = compute_softq_loss(time_steps, actions, next_time_steps)\n",
    "\n",
    "    tf.debugging.check_numerics(softq_loss, 'softq_loss is inf or nan.')\n",
    "    softq_grads = tape.gradient(softq_loss, softq_network.trainable_variables)\n",
    "    softq_optimizer.apply_gradients(list(zip(softq_grads, softq_network.trainable_variables)))\n",
    "    \n",
    "    # Value Network Update\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        tape.watch(value_network.trainable_variables)\n",
    "        value_loss = compute_value_loss(time_steps)\n",
    "\n",
    "    tf.debugging.check_numerics(value_loss, 'value_loss is inf or nan.')\n",
    "    value_grads = tape.gradient(value_loss, value_network.trainable_variables)\n",
    "    value_optimizer.apply_gradients(list(zip(value_grads, value_network.trainable_variables)))\n",
    "    \n",
    "    # Policy Network Update\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        tape.watch(policy_network.trainable_variables)\n",
    "        policy_loss = compute_policy_loss(time_steps)\n",
    "\n",
    "    tf.debugging.check_numerics(policy_loss, 'value_loss is inf or nan.')\n",
    "    policy_grads = tape.gradient(policy_loss, policy_network.trainable_variables)\n",
    "    policy_optimizer.apply_gradients(list(zip(policy_grads, policy_network.trainable_variables)))\n",
    "    \n",
    "    loss = softq_loss + value_loss + policy_loss\n",
    "    \n",
    "    common.soft_variables_update(\n",
    "        value_network.variables,\n",
    "        target_value_network.variables,\n",
    "        tau=target_update_tau)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_return=-255.2966766357422; max_return=-255.2966766357422\n",
      "step = 200: loss = 73.11649322509766\n",
      "step = 400: loss = 80.35567474365234\n",
      "step = 600: loss = 177.26473999023438\n",
      "step = 800: loss = 66.3226089477539\n",
      "step = 1000: loss = 26.098495483398438\n",
      "step = 1200: loss = 23.025917053222656\n",
      "step = 1400: loss = 59.6632080078125\n",
      "step = 1600: loss = 31.872257232666016\n",
      "step = 1800: loss = 91.0958251953125\n",
      "step = 2000: loss = 81.86097717285156\n",
      "step = 2200: loss = 71.95896911621094\n",
      "step = 2400: loss = 136.72573852539062\n",
      "step = 2600: loss = 21.989234924316406\n",
      "step = 2800: loss = 47.65901565551758\n",
      "step = 3000: loss = 18.632272720336914\n",
      "step = 3200: loss = 36.99702072143555\n",
      "step = 3400: loss = 23.883214950561523\n",
      "step = 3600: loss = 19.61241340637207\n",
      "step = 3800: loss = 54.72982406616211\n",
      "step = 4000: loss = 12.680683135986328\n",
      "step = 4200: loss = 23.739927291870117\n",
      "step = 4400: loss = 22.95187759399414\n",
      "step = 4600: loss = 33.02753448486328\n",
      "step = 4800: loss = 85.91046142578125\n",
      "step = 5000: loss = 21.08170509338379\n",
      "step = 5000: episodes=47: Average Return = -156.31918334960938\n",
      "step = 5200: loss = 19.985084533691406\n",
      "step = 5400: loss = 64.4293212890625\n",
      "step = 5600: loss = 88.0073471069336\n",
      "step = 5800: loss = 102.16941833496094\n",
      "step = 6000: loss = 68.67507934570312\n",
      "step = 6200: loss = 46.0247917175293\n",
      "step = 6400: loss = 47.414764404296875\n",
      "step = 6600: loss = 116.09623718261719\n",
      "step = 6800: loss = 50.35478591918945\n",
      "step = 7000: loss = 117.37525939941406\n",
      "step = 7200: loss = 95.00566101074219\n",
      "step = 7400: loss = 51.25412368774414\n",
      "step = 7600: loss = 77.30338287353516\n",
      "step = 7800: loss = 15.296905517578125\n",
      "step = 8000: loss = 56.1372184753418\n",
      "step = 8200: loss = 62.014617919921875\n",
      "step = 8400: loss = 49.80246353149414\n",
      "step = 8600: loss = 164.47618103027344\n",
      "step = 8800: loss = 58.95327377319336\n",
      "step = 9000: loss = 54.35431671142578\n",
      "step = 9200: loss = 93.45633697509766\n",
      "step = 9400: loss = 111.31724548339844\n",
      "step = 9600: loss = 7.514625072479248\n",
      "step = 9800: loss = 37.864471435546875\n",
      "step = 10000: loss = 138.03604125976562\n",
      "step = 10000: episodes=90: Average Return = -118.4453353881836\n",
      "step = 10200: loss = 98.23683166503906\n",
      "step = 10400: loss = 39.70711135864258\n",
      "step = 10600: loss = 26.865331649780273\n",
      "step = 10800: loss = 11.538712501525879\n",
      "step = 11000: loss = 102.1783218383789\n",
      "step = 11200: loss = 104.31092071533203\n",
      "step = 11400: loss = 39.252845764160156\n",
      "step = 11600: loss = 21.40396499633789\n",
      "step = 11800: loss = 57.992828369140625\n",
      "step = 12000: loss = 16.46991729736328\n",
      "step = 12200: loss = 90.32518768310547\n",
      "step = 12400: loss = 116.24967956542969\n",
      "step = 12600: loss = 133.9420623779297\n",
      "step = 12800: loss = 139.98193359375\n",
      "step = 13000: loss = 88.7153091430664\n",
      "step = 13200: loss = 62.85545349121094\n",
      "step = 13400: loss = 43.73892593383789\n",
      "step = 13600: loss = 126.36625671386719\n",
      "step = 13800: loss = 17.407804489135742\n",
      "step = 14000: loss = 52.996212005615234\n",
      "step = 14200: loss = 96.28966522216797\n",
      "step = 14400: loss = 66.1225357055664\n",
      "step = 14600: loss = 18.955272674560547\n",
      "step = 14800: loss = 25.11827278137207\n",
      "step = 15000: loss = 71.6554183959961\n",
      "step = 15000: episodes=133: Average Return = -112.66312408447266\n",
      "step = 15200: loss = 118.05540466308594\n",
      "step = 15400: loss = 32.61505126953125\n",
      "step = 15600: loss = 137.34812927246094\n",
      "step = 15800: loss = 155.9205322265625\n",
      "step = 16000: loss = 91.88761901855469\n",
      "step = 16200: loss = 34.943145751953125\n",
      "step = 16400: loss = 47.314048767089844\n",
      "step = 16600: loss = 36.4587516784668\n",
      "step = 16800: loss = 54.199676513671875\n",
      "step = 17000: loss = 152.39418029785156\n",
      "step = 17200: loss = 81.9440689086914\n",
      "step = 17400: loss = 11.003230094909668\n",
      "step = 17600: loss = 65.49408721923828\n",
      "step = 17800: loss = 21.799535751342773\n",
      "step = 18000: loss = 55.98550033569336\n",
      "step = 18200: loss = 34.9422492980957\n",
      "step = 18400: loss = 32.180843353271484\n",
      "step = 18600: loss = 18.840343475341797\n",
      "step = 18800: loss = 26.955989837646484\n",
      "step = 19000: loss = 100.98760986328125\n",
      "step = 19200: loss = 53.69696044921875\n",
      "step = 19400: loss = 250.37889099121094\n",
      "step = 19600: loss = 188.96791076660156\n",
      "step = 19800: loss = 49.09150695800781\n",
      "step = 20000: loss = 15.203906059265137\n",
      "step = 20000: episodes=168: Average Return = -101.31800842285156\n",
      "step = 20200: loss = 77.77521514892578\n",
      "step = 20400: loss = 136.4547119140625\n",
      "step = 20600: loss = 19.101762771606445\n",
      "step = 20800: loss = 137.8621368408203\n",
      "step = 21000: loss = 96.80970764160156\n",
      "step = 21200: loss = 24.978748321533203\n",
      "step = 21400: loss = 21.252553939819336\n",
      "step = 21600: loss = 18.33969497680664\n",
      "step = 21800: loss = 26.30414581298828\n",
      "step = 22000: loss = 82.61880493164062\n",
      "step = 22200: loss = 31.476655960083008\n",
      "step = 22400: loss = 83.89968872070312\n",
      "step = 22600: loss = 43.083126068115234\n",
      "step = 22800: loss = 10.896683692932129\n",
      "step = 23000: loss = 14.878320693969727\n",
      "step = 23200: loss = 19.056241989135742\n",
      "step = 23400: loss = 69.09563446044922\n",
      "step = 23600: loss = 22.716726303100586\n",
      "step = 23800: loss = 10.526386260986328\n",
      "step = 24000: loss = 27.076282501220703\n",
      "step = 24200: loss = 54.61208724975586\n",
      "step = 24400: loss = 53.21405029296875\n",
      "step = 24600: loss = 16.543981552124023\n",
      "step = 24800: loss = 17.220579147338867\n",
      "step = 25000: loss = 70.20570373535156\n",
      "step = 25000: episodes=211: Average Return = -78.29336547851562\n",
      "step = 25200: loss = 47.06584930419922\n",
      "step = 25400: loss = 45.27896499633789\n",
      "step = 25600: loss = 48.14594268798828\n",
      "step = 25800: loss = 273.7405700683594\n",
      "step = 26000: loss = 24.52752113342285\n",
      "step = 26200: loss = 40.22910690307617\n",
      "step = 26400: loss = 26.916324615478516\n",
      "step = 26600: loss = 8.596220016479492\n",
      "step = 26800: loss = 134.6198272705078\n",
      "step = 27000: loss = 169.79721069335938\n",
      "step = 27200: loss = 140.63980102539062\n",
      "step = 27400: loss = 12.456846237182617\n",
      "step = 27600: loss = 57.48902130126953\n",
      "step = 27800: loss = 15.899507522583008\n",
      "step = 28000: loss = 61.819862365722656\n",
      "step = 28200: loss = 56.311500549316406\n",
      "step = 28400: loss = 78.150634765625\n",
      "step = 28600: loss = 30.981037139892578\n",
      "step = 28800: loss = 52.21572494506836\n",
      "step = 29000: loss = 45.43113708496094\n",
      "step = 29200: loss = 34.61148452758789\n",
      "step = 29400: loss = 24.619159698486328\n",
      "step = 29600: loss = 5.4866132736206055\n",
      "step = 29800: loss = 33.68260192871094\n",
      "step = 30000: loss = 35.06428909301758\n",
      "step = 30000: episodes=253: Average Return = -91.59295654296875\n",
      "step = 30200: loss = 92.59033966064453\n",
      "step = 30400: loss = 187.1741485595703\n",
      "step = 30600: loss = 20.951440811157227\n",
      "step = 30800: loss = 119.77562713623047\n",
      "step = 31000: loss = 117.2656478881836\n",
      "step = 31200: loss = 113.97238159179688\n",
      "step = 31400: loss = 23.1209716796875\n",
      "step = 31600: loss = 17.205930709838867\n",
      "step = 31800: loss = 42.58826446533203\n",
      "step = 32000: loss = 48.825191497802734\n",
      "step = 32200: loss = 24.730329513549805\n",
      "step = 32400: loss = 41.771148681640625\n",
      "step = 32600: loss = 27.981815338134766\n",
      "step = 32800: loss = 70.57109832763672\n",
      "step = 33000: loss = 13.725852966308594\n",
      "step = 33200: loss = 42.200130462646484\n",
      "step = 33400: loss = 40.546470642089844\n",
      "step = 33600: loss = 32.081764221191406\n",
      "step = 33800: loss = 20.004060745239258\n",
      "step = 34000: loss = 69.9133529663086\n",
      "step = 34200: loss = 9.92953109741211\n",
      "step = 34400: loss = 22.007291793823242\n",
      "step = 34600: loss = 37.25507736206055\n",
      "step = 34800: loss = 18.405086517333984\n",
      "step = 35000: loss = 67.6347427368164\n",
      "step = 35000: episodes=298: Average Return = -58.068763732910156\n",
      "step = 35200: loss = 61.58454895019531\n",
      "step = 35400: loss = 38.54079055786133\n",
      "step = 35600: loss = 46.601890563964844\n",
      "step = 35800: loss = 12.198019027709961\n",
      "step = 36000: loss = 23.78955078125\n",
      "step = 36200: loss = 27.311511993408203\n",
      "step = 36400: loss = 28.482810974121094\n",
      "step = 36600: loss = 128.093994140625\n",
      "step = 36800: loss = 18.58030891418457\n",
      "step = 37000: loss = 10.187753677368164\n",
      "step = 37200: loss = 6.186131477355957\n",
      "step = 37400: loss = 26.446794509887695\n",
      "step = 37600: loss = 32.9926643371582\n",
      "step = 37800: loss = 55.251625061035156\n",
      "step = 38000: loss = 9.63988971710205\n",
      "step = 38200: loss = 94.69232177734375\n",
      "step = 38400: loss = 61.51612854003906\n",
      "step = 38600: loss = 72.51835632324219\n",
      "step = 38800: loss = 68.66104888916016\n",
      "step = 39000: loss = 119.73468780517578\n",
      "step = 39200: loss = 75.12358856201172\n",
      "step = 39400: loss = 21.424335479736328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 39600: loss = 27.06711196899414\n",
      "step = 39800: loss = 43.92499542236328\n",
      "step = 40000: loss = 18.906190872192383\n",
      "step = 40000: episodes=332: Average Return = -117.76213073730469\n",
      "step = 40200: loss = 13.8683500289917\n",
      "step = 40400: loss = 37.60437774658203\n",
      "step = 40600: loss = 19.521469116210938\n",
      "step = 40800: loss = 48.09539031982422\n",
      "step = 41000: loss = 30.523204803466797\n",
      "step = 41200: loss = 23.770299911499023\n",
      "step = 41400: loss = 8.290160179138184\n",
      "step = 41600: loss = 36.392330169677734\n",
      "step = 41800: loss = 22.972782135009766\n",
      "step = 42000: loss = 53.22207260131836\n",
      "step = 42200: loss = 61.2640380859375\n",
      "step = 42400: loss = 57.871978759765625\n",
      "step = 42600: loss = 10.37488079071045\n",
      "step = 42800: loss = 16.660268783569336\n",
      "step = 43000: loss = 7.201270580291748\n",
      "step = 43200: loss = 16.15927505493164\n",
      "step = 43400: loss = 29.45286750793457\n",
      "step = 43600: loss = 9.896378517150879\n",
      "step = 43800: loss = 25.404382705688477\n",
      "step = 44000: loss = 46.80332946777344\n",
      "step = 44200: loss = 40.283748626708984\n",
      "step = 44400: loss = 42.3387336730957\n",
      "step = 44600: loss = 59.40727996826172\n",
      "step = 44800: loss = 61.191497802734375\n",
      "step = 45000: loss = 63.704185485839844\n",
      "step = 45000: episodes=373: Average Return = -66.32799530029297\n",
      "step = 45200: loss = 49.54087829589844\n",
      "step = 45400: loss = 9.724979400634766\n",
      "step = 45600: loss = 27.321407318115234\n",
      "step = 45800: loss = 8.586661338806152\n",
      "step = 46000: loss = 30.9668025970459\n",
      "step = 46200: loss = 12.462766647338867\n",
      "step = 46400: loss = 14.554865837097168\n",
      "step = 46600: loss = 13.900736808776855\n",
      "step = 46800: loss = 25.184362411499023\n",
      "step = 47000: loss = 37.87210464477539\n",
      "step = 47200: loss = 32.54222869873047\n",
      "step = 47400: loss = 56.05617141723633\n",
      "step = 47600: loss = 23.8809757232666\n",
      "step = 47800: loss = 7.054024696350098\n",
      "step = 48000: loss = 104.50480651855469\n",
      "step = 48200: loss = 44.5326042175293\n",
      "step = 48400: loss = 28.931310653686523\n",
      "step = 48600: loss = 71.8821029663086\n",
      "step = 48800: loss = 59.577674865722656\n",
      "step = 49000: loss = 5.622681617736816\n",
      "step = 49200: loss = 56.709171295166016\n",
      "step = 49400: loss = 38.179073333740234\n",
      "step = 49600: loss = 77.54036712646484\n",
      "step = 49800: loss = 57.96450424194336\n",
      "step = 50000: loss = 21.32896614074707\n",
      "step = 50000: episodes=406: Average Return = -71.99491882324219\n",
      "step = 50200: loss = 108.31848907470703\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4363e98234d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Collect a few steps using collect_policy and save to the replay buffer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcollect_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Sample a batch of data from the buffer and update the agent's network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tf_agents/drivers/dynamic_step_driver.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, time_step, policy_state, maximum_iterations)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mtime_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mpolicy_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         maximum_iterations=maximum_iterations)\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[0;31m# TODO(b/113529538): Add tests for policy_state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tf_agents/utils/common.py\u001b[0m in \u001b[0;36mwith_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# autodep-like behavior is already expected of fn.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresource_variables_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMISSING_RESOURCE_VARIABLES_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tf_agents/drivers/dynamic_step_driver.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, time_step, policy_state, maximum_iterations)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mparallel_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         name='driver_loop')\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop_v2\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, maximum_iterations, name)\u001b[0m\n\u001b[1;32m   2476\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m       \u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2478\u001b[0;31m       return_same_structure=True)\n\u001b[0m\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   2712\u001b[0m                                               list(loop_vars))\n\u001b[1;32m   2713\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2714\u001b[0;31m         \u001b[0mloop_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2715\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basetuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2716\u001b[0m           \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tf_agents/drivers/dynamic_step_driver.py\u001b[0m in \u001b[0;36mloop_body\u001b[0;34m(counter, time_step, policy_state)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mloop_vars\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnext\u001b[0m \u001b[0miteration\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhile_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m       \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m       \u001b[0maction_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m       \u001b[0mnext_time_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36maction\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_automatic_state_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_reset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclip_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tf_agents/utils/common.py\u001b[0m in \u001b[0;36mwith_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# autodep-like behavior is already expected of fn.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresource_variables_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMISSING_RESOURCE_VARIABLES_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36m_action\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    466\u001b[0m     \"\"\"\n\u001b[1;32m    467\u001b[0m     \u001b[0mseed_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeedStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msalt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ppo_policy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m     \u001b[0mdistribution_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m     actions = tf.nest.map_structure(\n\u001b[1;32m    470\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreparameterized_sampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-e710a4d4c659>\u001b[0m in \u001b[0;36m_distribution\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m    192\u001b[0m                                                            \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                                                            \u001b[0mpolicy_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m                                                            training=self._training)\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpolicy_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPolicyStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tf_agents/networks/network.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_tensor_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_check_trainable_weights_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-e710a4d4c659>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, observations, step_type, network_state, training)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;31m# Take the TanH and shift and scale to fit action spec.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mdistribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformedDistribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistribution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbijector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bijector_chain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/decorator.py:decorator-gen-231>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, distribution, bijector, batch_shape, event_shape, kwargs_split_fn, validate_args, parameters, name)\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tensorflow_probability/python/distributions/distribution.py\u001b[0m in \u001b[0;36mwrapped_init\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    278\u001b[0m       \u001b[0;31m# called, here is the place to do it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m       \u001b[0mself_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m       \u001b[0mdefault_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m       \u001b[0;31m# Note: if we ever want to override things set in `self` by subclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m       \u001b[0;31m# `__init__`, here is the place to do it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tensorflow_probability/python/distributions/transformed_distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, distribution, bijector, batch_shape, event_shape, kwargs_split_fn, validate_args, parameters, name)\u001b[0m\n\u001b[1;32m    239\u001b[0m           \u001b[0mallow_nan_stats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_nan_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m           \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tensorflow_probability/python/distributions/distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dtype, reparameterization_type, validate_args, allow_nan_stats, parameters, graph_parents, name)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_parents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_parents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m     self._initial_parameter_control_dependencies = tuple(\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameter_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m         if d is not None)\n\u001b[1;32m    501\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_parameter_control_dependencies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tensorflow_probability/python/distributions/transformed_distribution.py\u001b[0m in \u001b[0;36m_parameter_control_dependencies\u001b[0;34m(self, is_init)\u001b[0m\n\u001b[1;32m    801\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scalar_event\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m         is_init))\n\u001b[0m\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0massertions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tensorflow_probability/python/distributions/transformed_distribution.py\u001b[0m in \u001b[0;36m_maybe_validate_shape_override\u001b[0;34m(self, override_shape, base_is_scalar_fn, static_base_shape, is_init)\u001b[0m\n\u001b[1;32m    741\u001b[0m     \u001b[0;31m# Check non-negative elements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_init\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverride_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m       \u001b[0moverride_shape_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_static_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverride_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m       \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Shape override must have non-negative elements.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0moverride_shape_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mconstant_value\u001b[0;34m(tensor, partial)\u001b[0m\n\u001b[1;32m    790\u001b[0m   \"\"\"\n\u001b[1;32m    791\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "train = common.function(train)\n",
    "\n",
    "avg_return = compute_avg_return(eval_env, collect_policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "print(\"avg_return={}; max_return={}\".format(avg_return, np.amax(returns)))\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    collect_op.run()\n",
    "    \n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, _ = next(iterator)\n",
    "    train_loss = train(experience)\n",
    "\n",
    "    step = env_steps.result().numpy()\n",
    "    episodes = num_episodes.result().numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, collect_policy, num_eval_episodes)\n",
    "        print('step = {0}: episodes={1}: Average Return = {2}'.format(step, episodes, avg_return))\n",
    "        returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([-55.089905], shape=(1,), dtype=float32)\n",
      "tf.Tensor([-20.39988], shape=(1,), dtype=float32)\n",
      "tf.Tensor([-65.65068], shape=(1,), dtype=float32)\n",
      "tf.Tensor([-29.178307], shape=(1,), dtype=float32)\n",
      "tf.Tensor([-61.93116], shape=(1,), dtype=float32)\n",
      "tf.Tensor([-22.067902], shape=(1,), dtype=float32)\n",
      "tf.Tensor([-74.69744], shape=(1,), dtype=float32)\n",
      "tf.Tensor([-39.224876], shape=(1,), dtype=float32)\n",
      "tf.Tensor([-26.235672], shape=(1,), dtype=float32)\n",
      "tf.Tensor([-71.9828], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    rewards = 0.0\n",
    "    time_step = eval_env.reset()\n",
    "    while not time_step.is_last():\n",
    "        action_step = collect_policy.action(time_step)\n",
    "        time_step = eval_env.step(action_step.action)\n",
    "        rewards += time_step.reward\n",
    "        eval_py_env.render()\n",
    "    print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
