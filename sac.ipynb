{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.specs import distribution_spec\n",
    "#from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import gym_wrapper\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.trajectories import policy_step\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.networks import network\n",
    "from tf_agents.policies import tf_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.agents import tf_agent\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.utils import nest_utils\n",
    "from tf_agents.utils import common\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"Pendulum-v0\"\n",
    "#env_name = \"LunarLanderContinuous-v2\" \n",
    "\n",
    "num_iterations = 40000#100000 \n",
    "\n",
    "initial_collect_steps = 1000#10000  \n",
    "collect_steps_per_iteration = 1 \n",
    "replay_buffer_max_length = num_iterations \n",
    "\n",
    "batch_size = 256 \n",
    "\n",
    "value_learning_rate = 3e-4 \n",
    "softq_learning_rate = 3e-4 \n",
    "policy_learning_rate = 3e-4 \n",
    "target_update_tau = 0.01 \n",
    "target_update_period = 1 \n",
    "gamma = 0.99 \n",
    "\n",
    "value_fc_layer_params = (256, 256)\n",
    "softq_fc_layer_params = (256, 256)\n",
    "policy_fc_layer_params = (256, 256)\n",
    "\n",
    "log_interval = 200#5000 \n",
    "\n",
    "num_eval_episodes = 10 \n",
    "eval_interval = 1000#10000 \n",
    "#max_episode_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environments\n",
    "import gym\n",
    "class NormalizedActions(gym.ActionWrapper):\n",
    "    def _action(self, action):\n",
    "        low  = self.action_space.low\n",
    "        high = self.action_space.high\n",
    "        \n",
    "        action = low + (action + 1.0) * 0.5 * (high - low)\n",
    "        action = np.clip(action, low, high)\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def _reverse_action(self, action):\n",
    "        low  = self.action_space.low\n",
    "        high = self.action_space.high\n",
    "        \n",
    "        action = 2 * (action - low) / (high - low) - 1\n",
    "        action = np.clip(action, low, high)\n",
    "        \n",
    "        return actions\n",
    "\n",
    "\n",
    "train_py_env = gym_wrapper.GymWrapper(NormalizedActions(gym.make(env_name)))\n",
    "eval_py_env = gym_wrapper.GymWrapper(NormalizedActions(gym.make(env_name)))\n",
    "    \n",
    "#train_py_env = suite_gym.load(env_name)#, max_episode_steps=max_episode_steps)\n",
    "#eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "\n",
    "time_step_spec = train_env.time_step_spec()\n",
    "observation_spec = train_env.observation_spec()\n",
    "action_spec = train_env.action_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions and Networks\n",
    "class ValueNetwork(network.Network):\n",
    "    def __init__(self,\n",
    "                 observation_spec,\n",
    "                 fc_layer_params,\n",
    "                 init_w = 3e-3,\n",
    "                 name='ValueNetwork'):\n",
    "        \n",
    "        super(ValueNetwork, self).__init__(\n",
    "            input_tensor_spec=observation_spec,\n",
    "            state_spec=(),\n",
    "            name=name)\n",
    "          \n",
    "        self._encoding_layers = []\n",
    "        for num_units in fc_layer_params:\n",
    "            self._encoding_layers.append(tf.keras.layers.Dense(\n",
    "                num_units,\n",
    "                activation=tf.keras.activations.relu,\n",
    "                kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform(),\n",
    "                name='%s/dense' % name))\n",
    "        \n",
    "        self._value = tf.keras.layers.Dense(\n",
    "            1,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform(),\n",
    "            name='value')\n",
    "\n",
    "\n",
    "    def call(self, observations, step_type, network_state, training=False):\n",
    "        encoding = observations\n",
    "        \n",
    "        for layer in self._encoding_layers:\n",
    "            encoding = layer(encoding, training=training)\n",
    "\n",
    "        value = self._value(encoding, training=training)\n",
    "        return tf.reshape(value, [-1]), network_state\n",
    "\n",
    "    \n",
    "class SoftQNetwork(network.Network):\n",
    "    def __init__(self,\n",
    "                 observation_and_action_spec,\n",
    "                 fc_layer_params,\n",
    "                 init_w = 3e-3,\n",
    "                 name='SoftQNetwork'):\n",
    "        \n",
    "        super(SoftQNetwork, self).__init__(\n",
    "            input_tensor_spec=observation_and_action_spec,\n",
    "            state_spec=(),\n",
    "            name=name)\n",
    "          \n",
    "        self._encoding_layers = []\n",
    "        for num_units in fc_layer_params:\n",
    "            self._encoding_layers.append(tf.keras.layers.Dense(\n",
    "                num_units,\n",
    "                activation=tf.keras.activations.relu,\n",
    "                kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform(),\n",
    "                name='%s/dense' % name))\n",
    "        \n",
    "        self._value = tf.keras.layers.Dense(\n",
    "            1,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform(),\n",
    "            name='value')\n",
    "\n",
    "\n",
    "    def call(self, observations_and_actions, step_type, network_state, training=False):\n",
    "        observations, actions = observations_and_actions\n",
    "        encoding = tf.concat([observations, actions], 1)\n",
    "        \n",
    "        for layer in self._encoding_layers:\n",
    "            encoding = layer(encoding, training=training)\n",
    "\n",
    "        value = self._value(encoding, training=training)\n",
    "        return tf.reshape(value, [-1]), network_state\n",
    "\n",
    "def spec_means_and_magnitudes(action_spec):\n",
    "    action_means = tf.nest.map_structure(\n",
    "        lambda spec: (spec.maximum + spec.minimum) / 2.0, action_spec)\n",
    "    action_magnitudes = tf.nest.map_structure(\n",
    "        lambda spec: (spec.maximum - spec.minimum) / 2.0, action_spec)\n",
    "    return tf.cast(action_means, dtype=tf.float32), tf.cast(action_magnitudes, dtype=tf.float32) \n",
    "    \n",
    "class PolicyNetwork(network.Network):\n",
    "    def __init__(self,\n",
    "                 observation_spec,\n",
    "                 action_spec,\n",
    "                 fc_layer_params,\n",
    "                 init_w=3e-3, \n",
    "                 log_std_min=-20, \n",
    "                 log_std_max=2,\n",
    "                 name=\"ActorNormalDistributionNetwork\"):\n",
    "\n",
    "        super(PolicyNetwork, self).__init__(\n",
    "            input_tensor_spec=observation_spec,\n",
    "            state_spec=(),\n",
    "            name=name)\n",
    "        \n",
    "        self._log_std_min = log_std_min\n",
    "        self._log_std_max = log_std_max\n",
    "\n",
    "        num_actions = action_spec.shape.num_elements()\n",
    "          \n",
    "        self._encoding_layers = []\n",
    "        for num_units in fc_layer_params:\n",
    "            self._encoding_layers.append(tf.keras.layers.Dense(\n",
    "                num_units,\n",
    "                activation=tf.keras.activations.relu,\n",
    "                kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform(),\n",
    "                name='%s/dense' % name))\n",
    "        \n",
    "        init_means_output_factor = 0.1\n",
    "        std_bias_initializer_value = 0.0\n",
    "\n",
    "        # mean layer for distribution\n",
    "        self._means_linear = tf.keras.layers.Dense(\n",
    "            num_actions,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n",
    "                scale=init_means_output_factor),\n",
    "            bias_initializer=tf.keras.initializers.Zeros(),\n",
    "            name='means_linear')\n",
    "\n",
    "        # standard dev layer for distribution\n",
    "        self._log_std_linear = tf.keras.layers.Dense(\n",
    "            num_actions,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n",
    "                scale=init_means_output_factor),\n",
    "            bias_initializer=tf.keras.initializers.Constant(\n",
    "                value=std_bias_initializer_value),\n",
    "            name='log_std_linear')\n",
    "        \n",
    "    def call(self, observations, step_type, network_state, training=False):  \n",
    "        # Feed through fc layers.\n",
    "        encoding = observations\n",
    "        \n",
    "        for layer in self._encoding_layers:\n",
    "            encoding = layer(encoding, training=training)\n",
    "        \n",
    "        # Compute means.\n",
    "        means = self._means_linear(encoding, training=training)\n",
    "\n",
    "        # Compute stds:  (take log of std, clip, and exponentiate to get std.)\n",
    "        log_stds = self._log_std_linear(encoding, training=training)\n",
    "        log_stds = tf.clip_by_value(log_stds, self._log_std_min, self._log_std_max)\n",
    "        \n",
    "        return (means, log_stds), network_state\n",
    "    \n",
    "class ActorPolicy(tf_policy.Base):\n",
    "    def __init__(self,\n",
    "        time_step_spec,\n",
    "        action_spec,\n",
    "        policy_network,\n",
    "        training=False):\n",
    "\n",
    "        policy_network.create_variables()\n",
    "        self._policy_network = policy_network\n",
    "        self._training = training\n",
    "\n",
    "        super(ActorPolicy, self).__init__(\n",
    "            time_step_spec=time_step_spec,\n",
    "            action_spec=action_spec,\n",
    "            policy_state_spec=policy_network.state_spec)\n",
    "\n",
    "    def _variables(self):\n",
    "        return self._policy_network.variables\n",
    "\n",
    "    def _action(self, time_step, policy_state, seed):\n",
    "        means_and_log_std, policy_state = self._policy_network(time_step.observation,\n",
    "                                                           time_step.step_type,\n",
    "                                                           policy_state,\n",
    "                                                           training=self._training)\n",
    "        \n",
    "        \n",
    "        mean, log_std = means_and_log_std\n",
    "        std = tf.exp(log_std)\n",
    "        \n",
    "        normal = tfp.distributions.Normal(loc=mean, scale=std)\n",
    "        z = normal.sample()\n",
    "        action = tf.math.tanh(z)\n",
    "                \n",
    "        return policy_step.PolicyStep(action, policy_state)\n",
    "    \n",
    "def actions_and_logprobs(policy_network, time_steps, training, epsilon=1e-6):\n",
    "    mean_and_log_std, _ = policy_network(time_steps.observation,\n",
    "                                         time_steps.step_type,\n",
    "                                         network_state=(),\n",
    "                                         training=training)\n",
    "    \n",
    "    \n",
    "    mean, log_std = mean_and_log_std\n",
    "    std = tf.exp(log_std)\n",
    "\n",
    "    normal = tfp.distributions.Normal(loc=mean, scale=std)\n",
    "    z = normal.sample()\n",
    "    action = tf.math.tanh(z)\n",
    "\n",
    "    log_prob = normal.log_prob(z) - tf.math.log(1 - tf.math.pow(action, 2) + epsilon)\n",
    "    log_prob = tf.reduce_sum(input_tensor=log_prob, axis=1)\n",
    "\n",
    "    return action, log_prob, z, mean, log_std\n",
    "\n",
    "\n",
    "def experience_to_transitions(experience):\n",
    "    transitions = trajectory.to_transition(experience)\n",
    "    time_steps, policy_steps, next_time_steps = transitions\n",
    "    actions = policy_steps.action\n",
    "    time_steps, actions, next_time_steps = tf.nest.map_structure(\n",
    "        lambda t: tf.squeeze(t, axis=1),\n",
    "        (time_steps, actions, next_time_steps))\n",
    "    return time_steps, actions, next_time_steps\n",
    "\n",
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jwatts/TensorflowProjects/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: <class '__main__.NormalizedActions'> doesn't implement 'action' method. Maybe it implements deprecated '_action' method.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "# Create networks\n",
    "value_network = ValueNetwork(observation_spec, value_fc_layer_params)\n",
    "value_network.create_variables()\n",
    "\n",
    "target_value_network = value_network.copy(name='TargetValueNetwork')\n",
    "target_value_network.create_variables()\n",
    "\n",
    "softq_network = SoftQNetwork((observation_spec, action_spec), softq_fc_layer_params)\n",
    "softq_network.create_variables()\n",
    "\n",
    "policy_network = PolicyNetwork(observation_spec, action_spec, policy_fc_layer_params)\n",
    "policy_network.create_variables()\n",
    "\n",
    "collect_policy = ActorPolicy(time_step_spec, action_spec, policy_network, training=False)\n",
    "\n",
    "# Full copy of network variables.\n",
    "common.soft_variables_update(\n",
    "    value_network.variables,\n",
    "    target_value_network.variables,\n",
    "    tau=1.0)\n",
    "\n",
    "# Optimizers\n",
    "value_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=value_learning_rate)\n",
    "softq_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=softq_learning_rate)\n",
    "policy_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=policy_learning_rate)\n",
    "\n",
    "# Loss Objects\n",
    "value_loss_fn  = tf.compat.v1.losses.mean_squared_error\n",
    "softq_loss_fn = tf.compat.v1.losses.mean_squared_error\n",
    "\n",
    "# Create the replay buffer for training\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=collect_policy.trajectory_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "\n",
    "# Collect some random samples to start.\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())\n",
    "\n",
    "dynamic_step_driver.DynamicStepDriver(\n",
    "    train_env, \n",
    "    random_policy,\n",
    "    observers=[replay_buffer.add_batch],\n",
    "    num_steps=initial_collect_steps).run()\n",
    "\n",
    "# Create collection driver\n",
    "num_episodes = tf_metrics.NumberOfEpisodes()\n",
    "env_steps = tf_metrics.EnvironmentSteps()\n",
    "observers = [num_episodes, env_steps, replay_buffer.add_batch]\n",
    "\n",
    "collect_op = dynamic_step_driver.DynamicStepDriver(\n",
    "    train_env, \n",
    "    collect_policy,\n",
    "    observers=observers,\n",
    "    num_steps=collect_steps_per_iteration)\n",
    "\n",
    "# Create a data set for the training loop\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Functions\n",
    "def compute_softq_loss(time_steps, \n",
    "                       actions, \n",
    "                       next_time_steps,\n",
    "                       gamma=0.99):\n",
    "    # Forward pass on Q network\n",
    "    expected_q_value,_ = softq_network((time_steps.observation, actions),\n",
    "                                       time_steps.step_type,\n",
    "                                       network_state=(),\n",
    "                                       training=True)    \n",
    "    \n",
    "    # Compute Targets\n",
    "    target_value,_ = target_value_network(next_time_steps.observation,\n",
    "                                          next_time_steps.step_type,\n",
    "                                          network_state=(),\n",
    "                                          training=False)\n",
    "    next_q_value = next_time_steps.reward + tf.where(next_time_steps.is_last(), 0.0, 1.0) * gamma * target_value\n",
    "    next_q_value = tf.stop_gradient(next_q_value)\n",
    "    softq_loss = softq_loss_fn(expected_q_value, next_q_value)\n",
    "\n",
    "    return softq_loss\n",
    "\n",
    "def compute_value_loss(time_steps):\n",
    "    # Forward pass on value network\n",
    "    expected_value,_ = value_network(time_steps.observation,\n",
    "                                     time_steps.step_type,\n",
    "                                     network_state=(),\n",
    "                                     training=True)\n",
    "    \n",
    "    # Compute Targets\n",
    "    action, log_prob, z, mean, log_std = actions_and_logprobs(policy_network, time_steps, training=False)\n",
    "    expected_new_q_value,_ = softq_network((time_steps.observation, action),\n",
    "                                           time_steps.step_type,\n",
    "                                           network_state=(),\n",
    "                                           training=False)    \n",
    "    \n",
    "    next_value = expected_new_q_value - log_prob\n",
    "    next_value = tf.stop_gradient(next_value)\n",
    "    value_loss = value_loss_fn(expected_value, next_value)\n",
    "    return value_loss\n",
    "\n",
    "def compute_policy_loss(time_steps,\n",
    "                        mean_lambda=1e-3,\n",
    "                        std_lambda=1e-3,\n",
    "                        z_lambda=0.0):\n",
    "        \n",
    "    # Forward pass on policy network\n",
    "    action, log_prob, z, mean, log_std = actions_and_logprobs(policy_network, time_steps, training=True)\n",
    "\n",
    "    # Compute Targets\n",
    "    expected_value,_ = value_network(time_steps.observation,\n",
    "                                     time_steps.step_type,\n",
    "                                     network_state=(),\n",
    "                                     training=False)\n",
    "    \n",
    "    expected_new_q_value,_ = softq_network((time_steps.observation, action),\n",
    "                                           time_steps.step_type,\n",
    "                                           network_state=(),\n",
    "                                           training=False)    \n",
    "\n",
    "    log_prob_target = expected_new_q_value - expected_value\n",
    "    policy_loss = (log_prob * (log_prob - log_prob_target))\n",
    "    policy_loss = tf.reduce_mean(policy_loss)\n",
    "    \n",
    "    mean_loss = mean_lambda * tf.reduce_mean(tf.math.pow(mean, 2))\n",
    "    std_loss = std_lambda * tf.reduce_mean(tf.math.pow(log_std, 2))\n",
    "    z_loss = z_lambda * tf.reduce_mean(tf.math.pow(z, 2))\n",
    "\n",
    "    return policy_loss + mean_loss + std_loss + z_loss\n",
    "    \n",
    "def train(experience):\n",
    "    time_steps, actions, next_time_steps = experience_to_transitions(experience)    \n",
    "    \n",
    "    # Soft Q Network Update\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        tape.watch(softq_network.trainable_variables)\n",
    "        softq_loss = compute_softq_loss(time_steps, actions, next_time_steps)\n",
    "\n",
    "    tf.debugging.check_numerics(softq_loss, 'softq_loss is inf or nan.')\n",
    "    softq_grads = tape.gradient(softq_loss, softq_network.trainable_variables)\n",
    "    softq_optimizer.apply_gradients(list(zip(softq_grads, softq_network.trainable_variables)))\n",
    "    \n",
    "    # Value Network Update\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        tape.watch(value_network.trainable_variables)\n",
    "        value_loss = compute_value_loss(time_steps)\n",
    "\n",
    "    tf.debugging.check_numerics(value_loss, 'value_loss is inf or nan.')\n",
    "    value_grads = tape.gradient(value_loss, value_network.trainable_variables)\n",
    "    value_optimizer.apply_gradients(list(zip(value_grads, value_network.trainable_variables)))\n",
    "    \n",
    "    # Policy Network Update\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        tape.watch(policy_network.trainable_variables)\n",
    "        policy_loss = compute_policy_loss(time_steps)\n",
    "\n",
    "    tf.debugging.check_numerics(policy_loss, 'value_loss is inf or nan.')\n",
    "    policy_grads = tape.gradient(policy_loss, policy_network.trainable_variables)\n",
    "    policy_optimizer.apply_gradients(list(zip(policy_grads, policy_network.trainable_variables)))\n",
    "    \n",
    "    loss = softq_loss + value_loss + policy_loss\n",
    "    \n",
    "    common.soft_variables_update(\n",
    "        value_network.variables,\n",
    "        target_value_network.variables,\n",
    "        tau=target_update_tau)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_return=-1195.135498046875; max_return=-1195.135498046875\n",
      "step = 200: loss = 1.1680642366409302\n",
      "step = 400: loss = 4.698259353637695\n",
      "step = 600: loss = 1.518215537071228\n",
      "step = 800: loss = 19.684005737304688\n",
      "step = 1000: loss = 41.48929214477539\n",
      "step = 1000: episodes=5: Average Return = -1279.041015625\n",
      "step = 1200: loss = 43.52537155151367\n",
      "step = 1400: loss = 5.897590637207031\n",
      "step = 1600: loss = 45.54631805419922\n",
      "step = 1800: loss = 29.014049530029297\n",
      "step = 2000: loss = 60.02605438232422\n",
      "step = 2000: episodes=10: Average Return = -1058.091796875\n",
      "step = 2200: loss = 44.53179931640625\n",
      "step = 2400: loss = 87.1839828491211\n",
      "step = 2600: loss = 33.8336296081543\n",
      "step = 2800: loss = 150.0789337158203\n",
      "step = 3000: loss = 85.1163558959961\n",
      "step = 3000: episodes=15: Average Return = -1164.81640625\n",
      "step = 3200: loss = 63.95691680908203\n",
      "step = 3400: loss = 9.742003440856934\n",
      "step = 3600: loss = 51.67510986328125\n",
      "step = 3800: loss = 52.92380905151367\n",
      "step = 4000: loss = 278.74298095703125\n",
      "step = 4000: episodes=20: Average Return = -1040.628173828125\n",
      "step = 4200: loss = 40.88816833496094\n",
      "step = 4400: loss = 213.713134765625\n",
      "step = 4600: loss = 13.568135261535645\n",
      "step = 4800: loss = 68.86567687988281\n",
      "step = 5000: loss = 113.02555847167969\n",
      "step = 5000: episodes=25: Average Return = -845.2828979492188\n",
      "step = 5200: loss = 211.38064575195312\n",
      "step = 5400: loss = 108.41610717773438\n",
      "step = 5600: loss = 174.907470703125\n",
      "step = 5800: loss = 186.7353973388672\n",
      "step = 6000: loss = 244.63278198242188\n",
      "step = 6000: episodes=30: Average Return = -836.0427856445312\n",
      "step = 6200: loss = 139.09637451171875\n",
      "step = 6400: loss = 335.9934387207031\n",
      "step = 6600: loss = 11.639891624450684\n",
      "step = 6800: loss = 19.2663516998291\n",
      "step = 7000: loss = 145.63548278808594\n",
      "step = 7000: episodes=35: Average Return = -627.8455810546875\n",
      "step = 7200: loss = 245.81082153320312\n",
      "step = 7400: loss = 187.40187072753906\n",
      "step = 7600: loss = 14.672904014587402\n",
      "step = 7800: loss = 413.41009521484375\n",
      "step = 8000: loss = 175.99615478515625\n",
      "step = 8000: episodes=40: Average Return = -719.7960205078125\n",
      "step = 8200: loss = 327.77777099609375\n",
      "step = 8400: loss = 20.142240524291992\n",
      "step = 8600: loss = 596.66943359375\n",
      "step = 8800: loss = 124.0420913696289\n",
      "step = 9000: loss = 423.3937683105469\n",
      "step = 9000: episodes=45: Average Return = -671.4093627929688\n",
      "step = 9200: loss = 53.539833068847656\n",
      "step = 9400: loss = 117.37776184082031\n",
      "step = 9600: loss = 136.2361297607422\n",
      "step = 9800: loss = 191.33306884765625\n",
      "step = 10000: loss = 20.922958374023438\n",
      "step = 10000: episodes=50: Average Return = -517.1549682617188\n",
      "step = 10200: loss = 16.262130737304688\n",
      "step = 10400: loss = 32.158321380615234\n",
      "step = 10600: loss = 351.7173767089844\n",
      "step = 10800: loss = 538.2891845703125\n",
      "step = 11000: loss = 485.06988525390625\n",
      "step = 11000: episodes=55: Average Return = -502.49139404296875\n",
      "step = 11200: loss = 470.1310119628906\n",
      "step = 11400: loss = 206.1956787109375\n",
      "step = 11600: loss = 330.26531982421875\n",
      "step = 11800: loss = 337.3240966796875\n",
      "step = 12000: loss = 414.7188415527344\n",
      "step = 12000: episodes=60: Average Return = -438.18328857421875\n",
      "step = 12200: loss = 194.5086212158203\n",
      "step = 12400: loss = 93.86456298828125\n",
      "step = 12600: loss = 295.0919189453125\n",
      "step = 12800: loss = 228.68414306640625\n",
      "step = 13000: loss = 43.3048095703125\n",
      "step = 13000: episodes=65: Average Return = -373.21356201171875\n",
      "step = 13200: loss = 68.42562866210938\n",
      "step = 13400: loss = 170.4775390625\n",
      "step = 13600: loss = 179.29849243164062\n",
      "step = 13800: loss = 277.9442443847656\n",
      "step = 14000: loss = 306.4091491699219\n",
      "step = 14000: episodes=70: Average Return = -443.0975036621094\n",
      "step = 14200: loss = 46.62004470825195\n",
      "step = 14400: loss = 656.2533569335938\n",
      "step = 14600: loss = 660.6507568359375\n",
      "step = 14800: loss = 228.57342529296875\n",
      "step = 15000: loss = 68.5636215209961\n",
      "step = 15000: episodes=75: Average Return = -446.57403564453125\n",
      "step = 15200: loss = 219.90138244628906\n",
      "step = 15400: loss = 328.06915283203125\n",
      "step = 15600: loss = 643.1898193359375\n",
      "step = 15800: loss = 579.0053100585938\n",
      "step = 16000: loss = 571.36083984375\n",
      "step = 16000: episodes=80: Average Return = -399.5721435546875\n",
      "step = 16200: loss = 153.31072998046875\n",
      "step = 16400: loss = 301.3030090332031\n",
      "step = 16600: loss = 308.736328125\n",
      "step = 16800: loss = 454.4306945800781\n",
      "step = 17000: loss = 73.00157165527344\n",
      "step = 17000: episodes=85: Average Return = -319.6966857910156\n",
      "step = 17200: loss = 189.72109985351562\n",
      "step = 17400: loss = 266.139404296875\n",
      "step = 17600: loss = 90.01837921142578\n",
      "step = 17800: loss = 749.5917358398438\n",
      "step = 18000: loss = 872.2813110351562\n",
      "step = 18000: episodes=90: Average Return = -460.67218017578125\n",
      "step = 18200: loss = 25.633920669555664\n",
      "step = 18400: loss = 299.1705627441406\n",
      "step = 18600: loss = 197.2364501953125\n",
      "step = 18800: loss = 124.40331268310547\n",
      "step = 19000: loss = 200.97946166992188\n",
      "step = 19000: episodes=95: Average Return = -664.0469970703125\n",
      "step = 19200: loss = 342.2987060546875\n",
      "step = 19400: loss = 328.4664001464844\n",
      "step = 19600: loss = 230.3229217529297\n",
      "step = 19800: loss = 124.71129608154297\n",
      "step = 20000: loss = 207.29165649414062\n",
      "step = 20000: episodes=100: Average Return = -292.50311279296875\n",
      "step = 20200: loss = 234.2347869873047\n",
      "step = 20400: loss = 340.34814453125\n",
      "step = 20600: loss = 72.54701232910156\n",
      "step = 20800: loss = 409.4755554199219\n",
      "step = 21000: loss = 278.3032531738281\n",
      "step = 21000: episodes=105: Average Return = -206.7891845703125\n",
      "step = 21200: loss = 256.3544616699219\n",
      "step = 21400: loss = 533.6534423828125\n",
      "step = 21600: loss = 140.26815795898438\n",
      "step = 21800: loss = 340.92205810546875\n",
      "step = 22000: loss = 254.2052001953125\n",
      "step = 22000: episodes=110: Average Return = -418.03485107421875\n",
      "step = 22200: loss = 329.65875244140625\n",
      "step = 22400: loss = 164.3905487060547\n",
      "step = 22600: loss = 211.81585693359375\n",
      "step = 22800: loss = 349.26580810546875\n",
      "step = 23000: loss = 30.4492130279541\n",
      "step = 23000: episodes=115: Average Return = -280.42401123046875\n",
      "step = 23200: loss = 528.0969848632812\n",
      "step = 23400: loss = 391.1823425292969\n",
      "step = 23600: loss = 255.1602325439453\n",
      "step = 23800: loss = 532.0294799804688\n",
      "step = 24000: loss = 93.77131652832031\n",
      "step = 24000: episodes=120: Average Return = -225.3423614501953\n",
      "step = 24200: loss = 454.8445129394531\n",
      "step = 24400: loss = 541.16455078125\n",
      "step = 24600: loss = 431.8940734863281\n",
      "step = 24800: loss = 105.65769958496094\n",
      "step = 25000: loss = 344.4269714355469\n",
      "step = 25000: episodes=125: Average Return = -317.76287841796875\n",
      "step = 25200: loss = 563.624755859375\n",
      "step = 25400: loss = 365.4009094238281\n",
      "step = 25600: loss = 745.9669799804688\n",
      "step = 25800: loss = 858.0391845703125\n",
      "step = 26000: loss = 243.10061645507812\n",
      "step = 26000: episodes=130: Average Return = -322.04156494140625\n",
      "step = 26200: loss = 129.6057586669922\n",
      "step = 26400: loss = 30.688533782958984\n",
      "step = 26600: loss = 527.756103515625\n",
      "step = 26800: loss = 403.25262451171875\n",
      "step = 27000: loss = 104.17681121826172\n",
      "step = 27000: episodes=135: Average Return = -419.9697265625\n",
      "step = 27200: loss = 218.98150634765625\n",
      "step = 27400: loss = 147.30087280273438\n",
      "step = 27600: loss = 46.99494934082031\n",
      "step = 27800: loss = 45.34080505371094\n",
      "step = 28000: loss = 260.0748596191406\n",
      "step = 28000: episodes=140: Average Return = -377.98162841796875\n",
      "step = 28200: loss = 761.7930908203125\n",
      "step = 28400: loss = 223.94473266601562\n",
      "step = 28600: loss = 21.226104736328125\n",
      "step = 28800: loss = 37.563106536865234\n",
      "step = 29000: loss = 368.89849853515625\n",
      "step = 29000: episodes=145: Average Return = -274.97479248046875\n",
      "step = 29200: loss = 66.39473724365234\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4363e98234d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Sample a batch of data from the buffer and update the agent's network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mexperience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "train = common.function(train)\n",
    "\n",
    "avg_return = compute_avg_return(eval_env, collect_policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "print(\"avg_return={}; max_return={}\".format(avg_return, np.amax(returns)))\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    collect_op.run()\n",
    "    \n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, _ = next(iterator)\n",
    "    train_loss = train(experience)\n",
    "\n",
    "    step = env_steps.result().numpy()\n",
    "    episodes = num_episodes.result().numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, collect_policy, num_eval_episodes)\n",
    "        print('step = {0}: episodes={1}: Average Return = {2}'.format(step, episodes, avg_return))\n",
    "        returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([-1040.5687], shape=(1,), dtype=float32)\n",
      "tf.Tensor([-583.11], shape=(1,), dtype=float32)\n",
      "tf.Tensor([-2.2975605], shape=(1,), dtype=float32)\n",
      "tf.Tensor([-358.5087], shape=(1,), dtype=float32)\n",
      "tf.Tensor([-669.21893], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    rewards = 0.0\n",
    "    time_step = eval_env.reset()\n",
    "    while not time_step.is_last():\n",
    "        action_step = collect_policy.action(time_step)\n",
    "        time_step = eval_env.step(action_step.action)\n",
    "        rewards += time_step.reward\n",
    "        eval_py_env.render()\n",
    "    print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
