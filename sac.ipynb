{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.specs import distribution_spec\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.trajectories import policy_step\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.networks import network\n",
    "from tf_agents.policies import tf_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.agents import tf_agent\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.utils import nest_utils\n",
    "from tf_agents.utils import common\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_name = \"Pendulum-v0\"\n",
    "env_name = \"LunarLanderContinuous-v2\" \n",
    "\n",
    "num_iterations = 100000 \n",
    "\n",
    "initial_collect_steps = 10000  \n",
    "collect_steps_per_iteration = 1 \n",
    "replay_buffer_max_length = num_iterations \n",
    "\n",
    "batch_size = 256 \n",
    "\n",
    "value_learning_rate = 3e-4 \n",
    "softq_learning_rate = 3e-4 \n",
    "policy_learning_rate = 3e-4 \n",
    "target_update_tau = 0.005 \n",
    "target_update_period = 1 \n",
    "gamma = 0.99 \n",
    "\n",
    "value_fc_layer_params = (256, 256)\n",
    "softq_fc_layer_params = (256, 256)\n",
    "policy_fc_layer_params = (256, 256)\n",
    "\n",
    "log_interval = 1000#5000 \n",
    "\n",
    "num_eval_episodes = 10 \n",
    "eval_interval = 5000#10000 \n",
    "max_episode_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environments\n",
    "train_py_env = suite_gym.load(env_name)#, max_episode_steps=max_episode_steps)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "\n",
    "time_step_spec = train_env.time_step_spec()\n",
    "observation_spec = train_env.observation_spec()\n",
    "action_spec = train_env.action_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions and Networks\n",
    "class ValueNetwork(network.Network):\n",
    "    def __init__(self,\n",
    "                 observation_spec,\n",
    "                 fc_layer_params,\n",
    "                 init_w = 3e-3,\n",
    "                 name='ValueNetwork'):\n",
    "        \n",
    "        super(ValueNetwork, self).__init__(\n",
    "            input_tensor_spec=observation_spec,\n",
    "            state_spec=(),\n",
    "            name=name)\n",
    "          \n",
    "        self._encoding_layers = []\n",
    "        for num_units in fc_layer_params:\n",
    "            self._encoding_layers.append(tf.keras.layers.Dense(\n",
    "                num_units,\n",
    "                activation=tf.keras.activations.relu,\n",
    "                kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_w, maxval=init_w),\n",
    "                name='%s/dense' % name))\n",
    "        \n",
    "        self._value = tf.keras.layers.Dense(\n",
    "            1,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_w, maxval=init_w),\n",
    "            name='value')\n",
    "\n",
    "\n",
    "    def call(self, inputs, step_type=(), network_state=(), training=False):\n",
    "        encoding = inputs\n",
    "        \n",
    "        for layer in self._encoding_layers:\n",
    "            encoding = layer(encoding, training=training)\n",
    "\n",
    "        value = self._value(encoding, training=training)\n",
    "        return tf.reshape(value, [-1]), network_state\n",
    "\n",
    "class SoftQNetwork(network.Network):\n",
    "    def __init__(self,\n",
    "                 observation_and_action_spec,\n",
    "                 fc_layer_params,\n",
    "                 init_w = 3e-3,\n",
    "                 name='SoftQNetwork'):\n",
    "        \n",
    "        super(SoftQNetwork, self).__init__(\n",
    "            input_tensor_spec=observation_and_action_spec,\n",
    "            state_spec=(),\n",
    "            name=name)\n",
    "          \n",
    "        self._encoding_layers = []\n",
    "        for num_units in fc_layer_params:\n",
    "            self._encoding_layers.append(tf.keras.layers.Dense(\n",
    "                num_units,\n",
    "                activation=tf.keras.activations.relu,\n",
    "                kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_w, maxval=init_w),\n",
    "                name='%s/dense' % name))\n",
    "        \n",
    "        self._value = tf.keras.layers.Dense(\n",
    "            1,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_w, maxval=init_w),\n",
    "            name='value')\n",
    "\n",
    "\n",
    "    def call(self, inputs, step_type=(), network_state=(), training=False):\n",
    "        observations, actions = inputs\n",
    "        encoding = tf.concat([observations, actions], 1)\n",
    "        \n",
    "        for layer in self._encoding_layers:\n",
    "            encoding = layer(encoding, training=training)\n",
    "\n",
    "        value = self._value(encoding, training=training)\n",
    "        return tf.reshape(value, [-1]), network_state\n",
    "\n",
    "def spec_means_and_magnitudes(action_spec):\n",
    "    action_means = tf.nest.map_structure(\n",
    "        lambda spec: (spec.maximum + spec.minimum) / 2.0, action_spec)\n",
    "    action_magnitudes = tf.nest.map_structure(\n",
    "        lambda spec: (spec.maximum - spec.minimum) / 2.0, action_spec)\n",
    "    return tf.cast(action_means, dtype=tf.float32), tf.cast(action_magnitudes, dtype=tf.float32) \n",
    "    \n",
    "class PolicyNetwork(network.Network):\n",
    "    def __init__(self,\n",
    "                 observation_spec,\n",
    "                 action_spec,\n",
    "                 fc_layer_params,\n",
    "                 init_w=3e-3, \n",
    "                 log_std_min=-20, \n",
    "                 log_std_max=2,\n",
    "                 name=\"ActorNormalDistributionNetwork\"):\n",
    "        \n",
    "        #action_dist_spec = self._build_distribution_spec(action_spec, name) \n",
    "        \n",
    "        super(PolicyNetwork, self).__init__(\n",
    "            input_tensor_spec=observation_spec,\n",
    "            state_spec=(),\n",
    "            name=name)\n",
    "        \n",
    "        self._log_std_min = log_std_min\n",
    "        self._log_std_max = log_std_max\n",
    "\n",
    "        num_actions = action_spec.shape.num_elements()\n",
    "          \n",
    "        self._encoding_layers = []\n",
    "        for num_units in fc_layer_params:\n",
    "            self._encoding_layers.append(tf.keras.layers.Dense(\n",
    "                num_units,\n",
    "                activation=tf.keras.activations.relu,\n",
    "                kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_w, maxval=init_w),\n",
    "                name='%s/dense' % name))\n",
    "        \n",
    "        self._means_linear = tf.keras.layers.Dense(\n",
    "            num_actions,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_w, maxval=init_w),\n",
    "            name='means_linear')\n",
    "\n",
    "        # standard dev layer for distribution\n",
    "        self._log_std_linear = tf.keras.layers.Dense(\n",
    "            num_actions,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.keras.initializers.RandomUniform(minval=-init_w, maxval=init_w),\n",
    "            name='log_std_linear')\n",
    "        \n",
    "        #action_means, action_magnitudes = spec_means_and_magnitudes(action_spec)\n",
    "        #bijectors = [tfp.bijectors.Shift(action_means),\n",
    "        #             tfp.bijectors.Scale(action_magnitudes),\n",
    "        #             tfp.bijectors.Tanh()]\n",
    "\n",
    "        #self._bijector_chain = tfp.bijectors.Chain(bijectors)\n",
    "        \n",
    "        \n",
    "    #def _build_distribution_spec(self, sample_spec, network_name):\n",
    "    #    input_param_shapes = tfp.distributions.Normal.param_static_shapes(sample_spec.shape)\n",
    "\n",
    "    #    input_param_spec = {\n",
    "    #        name: tensor_spec.TensorSpec(  \n",
    "    #            shape=shape,\n",
    "    #            dtype=sample_spec.dtype,\n",
    "    #            name=network_name + '_' + name)\n",
    "    #        for name, shape in input_param_shapes.items()\n",
    "    #    }\n",
    "\n",
    "     #   return distribution_spec.DistributionSpec(None, input_param_spec, sample_spec=sample_spec)\n",
    "\n",
    "    \n",
    "    def call(self, observations, step_type, network_state, training=False):  \n",
    "        # Feed through fc layers.\n",
    "        encoding = observations\n",
    "        \n",
    "        for layer in self._encoding_layers:\n",
    "            encoding = layer(encoding, training=training)\n",
    "        \n",
    "        # Compute means.\n",
    "        means = self._means_linear(encoding, training=training)\n",
    "\n",
    "        # Compute stds:  (take log of std, clip, and exponentiate to get std.)\n",
    "        log_stds = self._log_std_linear(encoding, training=training)\n",
    "        log_stds = tf.clip_by_value(log_stds, self._log_std_min, self._log_std_max)\n",
    "        #stds = tf.exp(log_stds)\n",
    "        \n",
    "        # Build a distribution using the means and stds.\n",
    "        #distribution = tfp.distributions.Normal(loc=means, scale=stds)\n",
    "        \n",
    "        # Take the TanH and shift and scale to fit action spec.\n",
    "        #distribution = tfp.distributions.TransformedDistribution(distribution=distribution, bijector=self._bijector_chain)\n",
    "        \n",
    "        return (means, log_stds), network_state\n",
    "    \n",
    "class ActorPolicy(tf_policy.Base):\n",
    "    def __init__(self,\n",
    "        time_step_spec,\n",
    "        action_spec,\n",
    "        policy_network,\n",
    "        training=False):\n",
    "\n",
    "        policy_network.create_variables()\n",
    "        self._policy_network = policy_network\n",
    "        self._training = training\n",
    "\n",
    "        super(ActorPolicy, self).__init__(\n",
    "            time_step_spec=time_step_spec,\n",
    "            action_spec=action_spec,\n",
    "            policy_state_spec=policy_network.state_spec)\n",
    "\n",
    "    def _variables(self):\n",
    "        return self._policy_network.variables\n",
    "\n",
    "    def _action(self, time_step, policy_state, seed):\n",
    "        means_and_log_std, policy_state = self._policy_network(time_step.observation,\n",
    "                                                           time_step.step_type,\n",
    "                                                           policy_state,\n",
    "                                                           training=self._training)\n",
    "        \n",
    "        mean, log_std = means_and_log_std\n",
    "        std = tf.exp(log_std)\n",
    "        normal = tfp.distributions.Normal(loc=mean, scale=std)\n",
    "        z = normal.sample()\n",
    "        action = tf.math.tanh(z)\n",
    "        \n",
    "        return policy_step.PolicyStep(action, policy_state)\n",
    "    \n",
    "def actions_and_logprobs(policy_network, time_steps, epsilon=1e-6):\n",
    "    means_and_log_stds, _ = policy_network(time_steps.observation,\n",
    "                                           time_steps.step_type,\n",
    "                                           network_state=(),\n",
    "                                           training=True)\n",
    "    \n",
    "    means, log_stds = means_and_log_stds\n",
    "    stds = tf.exp(log_stds)\n",
    "    normal = tfp.distributions.Normal(loc=means, scale=stds)\n",
    "    samples = normal.sample()\n",
    "    actions = tf.math.tanh(samples)\n",
    "    \n",
    "    log_probs = normal.log_prob(samples) - tf.math.log(1-tf.math.pow(actions, 2) + epsilon)\n",
    "    log_probs = tf.reduce_sum(input_tensor=log_probs, axis=1)\n",
    "    return actions, log_probs, means, log_stds\n",
    "\n",
    "def experience_to_transitions(experience):\n",
    "    transitions = trajectory.to_transition(experience)\n",
    "    time_steps, policy_steps, next_time_steps = transitions\n",
    "    actions = policy_steps.action\n",
    "    time_steps, actions, next_time_steps = tf.nest.map_structure(\n",
    "        lambda t: tf.squeeze(t, axis=1),\n",
    "        (time_steps, actions, next_time_steps))\n",
    "    return time_steps, actions, next_time_steps\n",
    "\n",
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create networks\n",
    "value_network = ValueNetwork(observation_spec, value_fc_layer_params)\n",
    "value_network.create_variables()\n",
    "\n",
    "target_value_network = value_network.copy(name='TargetValueNetwork')\n",
    "target_value_network.create_variables()\n",
    "\n",
    "softq_network = SoftQNetwork((observation_spec, action_spec), softq_fc_layer_params)\n",
    "softq_network.create_variables()\n",
    "\n",
    "policy_network = PolicyNetwork(observation_spec, action_spec, policy_fc_layer_params)\n",
    "policy_network.create_variables()\n",
    "\n",
    "collect_policy = ActorPolicy(time_step_spec, action_spec, policy_network, training=False)\n",
    "\n",
    "# Full copy of network variables.\n",
    "common.soft_variables_update(\n",
    "    value_network.variables,\n",
    "    target_value_network.variables,\n",
    "    tau=1.0)\n",
    "\n",
    "# Optimizers\n",
    "value_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=value_learning_rate)\n",
    "softq_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=softq_learning_rate)\n",
    "policy_optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=policy_learning_rate)\n",
    "\n",
    "# Loss Objects\n",
    "value_loss_fn  = tf.compat.v1.losses.mean_squared_error\n",
    "softq_loss_fn = tf.compat.v1.losses.mean_squared_error\n",
    "\n",
    "# Create the replay buffer for training\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=collect_policy.trajectory_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "\n",
    "# Collect some random samples to start.\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())\n",
    "\n",
    "dynamic_step_driver.DynamicStepDriver(\n",
    "    train_env, \n",
    "    random_policy,\n",
    "    observers=[replay_buffer.add_batch],\n",
    "    num_steps=initial_collect_steps).run()\n",
    "\n",
    "# Create collection driver\n",
    "num_episodes = tf_metrics.NumberOfEpisodes()\n",
    "env_steps = tf_metrics.EnvironmentSteps()\n",
    "observers = [num_episodes, env_steps, replay_buffer.add_batch]\n",
    "\n",
    "collect_op = dynamic_step_driver.DynamicStepDriver(\n",
    "    train_env, \n",
    "    collect_policy,\n",
    "    observers=observers,\n",
    "    num_steps=collect_steps_per_iteration)\n",
    "\n",
    "# Create a data set for the training loop\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Functions\n",
    "def compute_softq_loss(time_steps, \n",
    "                       actions, \n",
    "                       next_time_steps,\n",
    "                       gamma=0.99):\n",
    "    expected_q_values,_ = softq_network((time_steps.observation, actions))    \n",
    "    expected_values,_ = value_network(time_steps.observation)\n",
    "    target_values,_ = target_value_network(next_time_steps.observation)\n",
    "    next_q_values = tf.stop_gradient(next_time_steps.reward + \n",
    "                                     tf.where(next_time_steps.is_last(), 0.0, gamma) * target_values)\n",
    "    softq_loss = softq_loss_fn(expected_q_values, next_q_values)\n",
    "    return softq_loss\n",
    "\n",
    "\n",
    "\n",
    "def compute_value_loss(time_steps):    \n",
    "    expected_values,_ = value_network(time_steps.observation)    \n",
    "    new_actions, log_probs,_,_ = actions_and_logprobs(policy_network, time_steps)    \n",
    "    expected_new_q_values,_ = softq_network((time_steps.observation, new_actions))    \n",
    "    next_values = tf.stop_gradient(expected_new_q_values - log_probs)    \n",
    "    value_loss = value_loss_fn(expected_values, next_values)\n",
    "    \n",
    "    return value_loss\n",
    "\n",
    "def compute_policy_loss(time_steps,\n",
    "                        mean_lambda=1e-3,\n",
    "                        std_lambda=1e-3):\n",
    "    \n",
    "    new_actions, log_probs, means, log_stds = actions_and_logprobs(policy_network, time_steps)\n",
    "    expected_new_q_values,_ = softq_network((time_steps.observation, new_actions))\n",
    "    expected_values,_ = value_network(time_steps.observation)\n",
    "    \n",
    "    log_prob_targets = expected_new_q_values - expected_values\n",
    "    policy_loss = tf.reduce_mean(log_probs * (log_probs - log_prob_targets))\n",
    "    \n",
    "    mean_loss = tf.reduce_mean(mean_lambda * tf.math.pow(means, 2))\n",
    "    std_loss = tf.reduce_mean(std_lambda * tf.math.pow(log_stds, 2))\n",
    "    \n",
    "    policy_loss = policy_loss + mean_loss + std_loss\n",
    "    \n",
    "    return policy_loss\n",
    "    \n",
    "def train(experience):\n",
    "    time_steps, actions, next_time_steps = experience_to_transitions(experience)    \n",
    "    \n",
    "    # Soft Q Network Update\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        tape.watch(softq_network.trainable_variables)\n",
    "        softq_loss = compute_softq_loss(time_steps, actions, next_time_steps)\n",
    "\n",
    "    tf.debugging.check_numerics(softq_loss, 'softq_loss is inf or nan.')\n",
    "    softq_grads = tape.gradient(softq_loss, softq_network.trainable_variables)\n",
    "    softq_optimizer.apply_gradients(list(zip(softq_grads, softq_network.trainable_variables)))\n",
    "    \n",
    "    # Value Network Update\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        tape.watch(value_network.trainable_variables)\n",
    "        value_loss = compute_value_loss(time_steps)\n",
    "\n",
    "    tf.debugging.check_numerics(value_loss, 'value_loss is inf or nan.')\n",
    "    value_grads = tape.gradient(value_loss, value_network.trainable_variables)\n",
    "    value_optimizer.apply_gradients(list(zip(value_grads, value_network.trainable_variables)))\n",
    "    \n",
    "    # Policy Network Update\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        tape.watch(policy_network.trainable_variables)\n",
    "        policy_loss = compute_policy_loss(time_steps)\n",
    "\n",
    "    tf.debugging.check_numerics(policy_loss, 'value_loss is inf or nan.')\n",
    "    policy_grads = tape.gradient(policy_loss, policy_network.trainable_variables)\n",
    "    policy_optimizer.apply_gradients(list(zip(policy_grads, policy_network.trainable_variables)))\n",
    "    \n",
    "    loss = softq_loss + value_loss + policy_loss\n",
    "    \n",
    "    common.soft_variables_update(\n",
    "        value_network.variables,\n",
    "        target_value_network.variables,\n",
    "        tau=target_update_tau)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "train = common.function(train)\n",
    "\n",
    "avg_return = compute_avg_return(eval_env, collect_policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "print(\"avg_return={}; max_return={}\".format(avg_return, np.amax(returns)))\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    collect_op.run()\n",
    "    \n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, _ = next(iterator)\n",
    "    train_loss = train(experience)\n",
    "\n",
    "    step = env_steps.result().numpy()\n",
    "    episodes = num_episodes.result().numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, collect_policy, num_eval_episodes)\n",
    "        print('step = {0}: episodes={1}: Average Return = {2}'.format(step, episodes, avg_return))\n",
    "        returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    rewards = 0.0\n",
    "    time_step = eval_env.reset()\n",
    "    while not time_step.is_last():\n",
    "        action_step = collect_policy.action(time_step)\n",
    "        time_step = eval_env.step(action_step.action)\n",
    "        rewards += time_step.reward\n",
    "        eval_py_env.render()\n",
    "    print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
