{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import itertools\n",
    "import matplotlib\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "from collections import deque\n",
    "\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-alpha0\n",
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent and Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, observation_size, action_size, one_hot):\n",
    "        self.observation_size = observation_size\n",
    "        self.action_size = action_size\n",
    "        self.hidden_units=64\n",
    "        self.batch_size = 32\n",
    "        self.one_hot = one_hot\n",
    "        \n",
    "        self.gamma = 1.0\n",
    "        self.epsilon = 0.1\n",
    "        self.learning_rate = 0.001\n",
    "        \n",
    "        self.memory = deque(maxlen=2000)\n",
    "            \n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            layers.Dense(self.hidden_units, activation='relu', input_shape=(self.observation_size,)),\n",
    "            layers.Dense(self.hidden_units, activation='relu'),\n",
    "            layers.Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "                      loss='mse',\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "            \n",
    "    def _act_egreedy(self, best_action):\n",
    "        action_probs = np.ones(self.action_size, dtype=float) * self.epsilon / self.action_size\n",
    "        action_probs[best_action] += (1.0 - self.epsilon)\n",
    "        return np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "    \n",
    "    def _shape_observation(self, observation):\n",
    "        if (self.one_hot == True):\n",
    "            return tf.keras.utils.to_categorical(observation, self.observation_size)[np.newaxis, :]\n",
    "        else:\n",
    "            return np.reshape(observation, [1, self.observation_size])        \n",
    "    \n",
    "    def remember(self, observation, action, reward, next_observation, done):\n",
    "        observation = self._shape_observation(observation)\n",
    "        next_observation = self._shape_observation(next_observation)\n",
    "        \n",
    "        self.memory.append((observation, action, reward, next_observation, done))\n",
    "\n",
    "    def act(self, observation, train=True):\n",
    "        observation = self._shape_observation(observation)\n",
    "        q_values = self.model.predict(observation)[0]\n",
    "        action = np.argmax(q_values)\n",
    "        \n",
    "        if (train==True):\n",
    "            # if we are training.  choose e-greedy.\n",
    "            action = self._act_egreedy(action)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def replay(self):\n",
    "        if (len(self.memory) < self.batch_size):\n",
    "            return\n",
    "        \n",
    "        # Select a batch or replays from memory   \n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "        # Compute the target q for each observation.\n",
    "        # Set target_q = reward if the episode ends at s+1, otherwise set target_q = r + gamma*maxQ(s', a')\n",
    "        for observation, action, reward, next_observation, done in minibatch:\n",
    "            target = reward\n",
    "            \n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_observation)[0])\n",
    "                \n",
    "            target_q = self.model.predict(observation)\n",
    "            \n",
    "            target_q[0][action] = target\n",
    "            \n",
    "            self.model.fit(observation, target_q, epochs=1, verbose=0)\n",
    "        \n",
    "            \n",
    "def train_agent (env, agent, num_episodes):\n",
    "    for episodeIx in range(num_episodes):\n",
    "        # reset state in the beginning of each game\n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(observation)\n",
    "            next_observation, reward, done, _ = env.step(action)\n",
    "            agent.remember(observation, action, reward, next_observation, done)\n",
    "            observation = next_observation\n",
    "\n",
    "        print(\"\\repisode: {}/{}\".format(episodeIx+1, num_episodes), end=\"\")\n",
    "\n",
    "        # train the agent with the experience of the episode\n",
    "        agent.replay()\n",
    "        \n",
    "def average_episodes (env, agent, num_episodes):\n",
    "    rewards = 0\n",
    "    for episodeIx in range(num_episodes):\n",
    "        # reset state in the beginning of each game\n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(observation, train=False)\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "\n",
    "        rewards += reward\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blackjack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "envBlackjack = gym.make('Blackjack-v0')\n",
    "\n",
    "PLAYER_STATE = 0\n",
    "DEALER_STATE = 1\n",
    "USABLE_ACE = 2\n",
    "\n",
    "PLAYER_STATE_COUNT = envBlackjack.observation_space.spaces[PLAYER_STATE].n\n",
    "DEALER_STATE_COUNT = envBlackjack.observation_space.spaces[DEALER_STATE].n\n",
    "USABLE_ACE_COUNT = envBlackjack.observation_space.spaces[USABLE_ACE].n\n",
    "\n",
    "ENVIRONMENT_SPACE = PLAYER_STATE_COUNT * DEALER_STATE_COUNT * USABLE_ACE_COUNT\n",
    "\n",
    "\n",
    "def get_state_index(state):\n",
    "    index = state[PLAYER_STATE] * (DEALER_STATE_COUNT * USABLE_ACE_COUNT)\n",
    "    \n",
    "    index += state[DEALER_STATE] * USABLE_ACE_COUNT\n",
    "    \n",
    "    if (state[USABLE_ACE] == True):\n",
    "        index += 1\n",
    "        \n",
    "    return index\n",
    "\n",
    "correct_policy = np.ones([PLAYER_STATE_COUNT, DEALER_STATE_COUNT, USABLE_ACE_COUNT]).astype(int)\n",
    "correct_policy[12, 4:7, 0] = 0\n",
    "correct_policy[13, 2:7, 0] = 0\n",
    "correct_policy[14, 2:7, 0] = 0\n",
    "correct_policy[15, 2:7, 0] = 0\n",
    "correct_policy[16, 2:7, 0] = 0\n",
    "correct_policy[17:22, :, 0] = 0\n",
    "correct_policy[18, 2:9, 1] = 0\n",
    "correct_policy[19:22, :, 1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5000/5000"
     ]
    }
   ],
   "source": [
    "observation_size = len(envBlackjack.observation_space.spaces)\n",
    "action_size = envBlackjack.action_space.n\n",
    "\n",
    "agent = DQNAgent(observation_size, action_size, one_hot=False)\n",
    "\n",
    "train_agent(envBlackjack, agent, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06565656565656566\n",
      "-549.0\n"
     ]
    }
   ],
   "source": [
    "state = np.zeros((1,3))\n",
    "view_learned_policy = np.zeros([PLAYER_STATE_COUNT, DEALER_STATE_COUNT, USABLE_ACE_COUNT]).astype(int)\n",
    "for playerIx in range(PLAYER_STATE_COUNT):\n",
    "    for dealerIx in range(DEALER_STATE_COUNT):\n",
    "        for usableAceIx in range(USABLE_ACE_COUNT):\n",
    "            state[0,PLAYER_STATE] = playerIx\n",
    "            state[0,DEALER_STATE] = dealerIx\n",
    "            state[0,USABLE_ACE] = usableAceIx\n",
    "            view_learned_policy[playerIx, dealerIx, usableAceIx] = agent.act(state, train=False)\n",
    "\n",
    "view_correct_policy = correct_policy.reshape(PLAYER_STATE_COUNT, DEALER_STATE_COUNT, USABLE_ACE_COUNT)\n",
    "error_rate = np.mean((np.bitwise_xor(view_learned_policy[4:22, :, :], view_correct_policy[4:22, :, :])))\n",
    "print(error_rate)\n",
    "\n",
    "rewards = average_episodes(envBlackjack, agent, 10000)\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
