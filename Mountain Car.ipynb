{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(q_values):\n",
    "    top = float(\"-inf\")\n",
    "    ties = []\n",
    "\n",
    "    for i in range(len(q_values)):\n",
    "        if q_values[i] > top:\n",
    "            top = q_values[i]\n",
    "            ties = []\n",
    "\n",
    "        if q_values[i] == top:\n",
    "            ties.append(i)\n",
    "\n",
    "    return np.random.choice(ties)\n",
    "\n",
    "def train_episode(env, agent, max_steps=15000):\n",
    "    done = False\n",
    "    steps = 0\n",
    "    episode_return = 0.0\n",
    "    \n",
    "    state = env.reset()\n",
    "    action = agent.agent_start(state)\n",
    "\n",
    "    while not done and steps<max_steps:\n",
    "        steps += 1\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        if (done==True):\n",
    "            agent.agent_end(reward)\n",
    "        else:\n",
    "            action = agent.agent_step(reward, next_state)\n",
    "            state = next_state\n",
    "\n",
    "    return steps, episode_return, done\n",
    "\n",
    "def train_episodes (env, agent, episodes):\n",
    "    steps_per_episode = []\n",
    "    episode_returns = []\n",
    "    for episode in range(episodes):\n",
    "        steps, episode_return, done = train_episode(env, agent)\n",
    "        steps_per_episode.append(steps)\n",
    "        episode_returns.append(episode_return)\n",
    "    \n",
    "    return steps_per_episode, episode_returns  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tile Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tools.tiles3 as tc\n",
    "\n",
    "class MountainCarTileCoder:\n",
    "    def __init__(self, iht_size=4096, num_tilings=8, num_tiles=8, position_range=1.8, velocity_range=.14):\n",
    "        self.iht = tc.IHT(iht_size)\n",
    "        self.num_tilings = num_tilings\n",
    "        self.num_tiles = num_tiles\n",
    "        self.position_scale = self.num_tiles / position_range\n",
    "        self.velocity_scale = self.num_tiles / velocity_range\n",
    "        \n",
    "    def get_tiles(self, position, velocity):\n",
    "        tiles = tc.tiles(self.iht, self.num_tilings, [position * self.position_scale, \n",
    "                                                      velocity * self.velocity_scale])\n",
    "        \n",
    "        return np.array(tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaAgent:\n",
    "    \"\"\"\n",
    "    Initialization of Sarsa Agent. All values are set to None so they can\n",
    "    be initialized in the agent_init method.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.last_action = None\n",
    "        self.last_state = None\n",
    "        self.epsilon = None\n",
    "        self.gamma = None\n",
    "        self.iht_size = None\n",
    "        self.w = None\n",
    "        self.alpha = None\n",
    "        self.num_tilings = None\n",
    "        self.num_tiles = None\n",
    "        self.mctc = None\n",
    "        self.initial_weights = None\n",
    "        self.num_actions = None\n",
    "        self.previous_tiles = None\n",
    "\n",
    "    def agent_init(self, agent_info={}):\n",
    "        \"\"\"Setup for the agent called when the experiment first starts.\"\"\"\n",
    "        self.num_tilings = agent_info.get(\"num_tilings\", 8)\n",
    "        self.num_tiles = agent_info.get(\"num_tiles\", 8)\n",
    "        self.iht_size = agent_info.get(\"iht_size\", 4096)\n",
    "        self.epsilon = agent_info.get(\"epsilon\", 0.1)\n",
    "        self.gamma = agent_info.get(\"gamma\", 1.0)\n",
    "        self.alpha = agent_info.get(\"alpha\", 0.5) / self.num_tilings\n",
    "        self.initial_weights = agent_info.get(\"initial_weights\", 0.0)\n",
    "        self.num_actions = agent_info.get(\"num_actions\", 3)\n",
    "        \n",
    "        # We initialize self.w to three times the iht_size. Recall this is because\n",
    "        # we need to have one set of weights for each action.\n",
    "        self.w = np.ones((self.num_actions, self.iht_size)) * self.initial_weights\n",
    "        \n",
    "        # We initialize self.mctc to the mountaincar verions of the \n",
    "        # tile coder that we created\n",
    "        self.tc = MountainCarTileCoder(iht_size=self.iht_size, \n",
    "                                         num_tilings=self.num_tilings, \n",
    "                                         num_tiles=self.num_tiles)\n",
    "\n",
    "    def select_action(self, tiles):\n",
    "        \"\"\"\n",
    "        Selects an action using epsilon greedy\n",
    "        Args:\n",
    "        tiles - np.array, an array of active tiles\n",
    "        Returns:\n",
    "        (chosen_action, action_value) - (int, float), tuple of the chosen action\n",
    "                                        and it's value\n",
    "        \"\"\"\n",
    "        action_values = []\n",
    "        chosen_action = None\n",
    "        \n",
    "        # First loop through the weights of each action and populate action_values\n",
    "        # with the action value for each action and tiles instance\n",
    "        \n",
    "        # Use np.random.random to decide if an exploritory action should be taken\n",
    "        # and set chosen_action to a random action if it is\n",
    "        # Otherwise choose the greedy action using the given argmax \n",
    "        # function and the action values (don't use numpy's armax)\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        action_values = np.zeros(self.num_actions)\n",
    "        for actionIx in range(self.num_actions):\n",
    "            action_values[actionIx] = np.sum(self.w[actionIx][tiles])\n",
    "        \n",
    "        chosen_action = argmax(action_values)\n",
    "        probs = np.ones(self.num_actions) * (self.epsilon / self.num_actions)\n",
    "        probs [chosen_action] += 1-self.epsilon\n",
    "        \n",
    "        chosen_action = np.random.choice(self.num_actions, p=probs)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        return chosen_action, action_values[chosen_action]\n",
    "    \n",
    "    def agent_start(self, state):\n",
    "        \"\"\"The first method called when the experiment starts, called after\n",
    "        the environment starts.\n",
    "        Args:\n",
    "            state (Numpy array): the state observation from the\n",
    "                environment's evn_start function.\n",
    "        Returns:\n",
    "            The first action the agent takes.\n",
    "        \"\"\"\n",
    "        position, velocity = state\n",
    "        \n",
    "        # Use self.tc to set active_tiles using position and velocity\n",
    "        # set current_action to the epsilon greedy chosen action using\n",
    "        # the select_action function above with the active tiles\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        active_tiles = self.tc.get_tiles(position, velocity)\n",
    "        current_action, _ = self.select_action(active_tiles)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        self.last_action = current_action\n",
    "        self.previous_tiles = np.copy(active_tiles)\n",
    "        return self.last_action\n",
    "\n",
    "    def agent_step(self, reward, state):\n",
    "        \"\"\"A step taken by the agent.\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (Numpy array): the state observation from the\n",
    "                environment's step based, where the agent ended up after the\n",
    "                last step\n",
    "        Returns:\n",
    "            The action the agent is taking.\n",
    "        \"\"\"\n",
    "        # choose the action here\n",
    "        position, velocity = state\n",
    "        \n",
    "        # Use self.tc to set active_tiles using position and velocity\n",
    "        # set current_action and action_value to the epsilon greedy chosen action using\n",
    "        # the select_action function above with the active tiles\n",
    "        \n",
    "        # Update self.w at self.previous_tiles and self.previous action\n",
    "        # using the reward, action_value, self.gamma, self.w,\n",
    "        # self.alpha, and the Sarsa update from the textbook\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        active_tiles = self.tc.get_tiles(position, velocity)\n",
    "        current_action, current_action_value = self.select_action(active_tiles)\n",
    "        \n",
    "        td_error = reward + self.gamma * current_action_value - np.sum(self.w[self.last_action][self.previous_tiles])\n",
    "        self.w[self.last_action][self.previous_tiles] += self.alpha * td_error\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        self.last_action = current_action\n",
    "        self.previous_tiles = np.copy(active_tiles)\n",
    "        return self.last_action\n",
    "\n",
    "    def agent_end(self, reward):\n",
    "        \"\"\"Run when the agent terminates.\n",
    "        Args:\n",
    "            reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "        # Update self.w at self.previous_tiles and self.previous action\n",
    "        # using the reward, self.gamma, self.w,\n",
    "        # self.alpha, and the Sarsa update from the textbook\n",
    "        # Hint - there is no action_value used here because this is the end\n",
    "        # of the episode.\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        td_error = reward - np.sum(self.w[self.last_action][self.previous_tiles])\n",
    "        self.w[self.last_action][self.previous_tiles] += self.alpha * td_error        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    def agent_cleanup(self):\n",
    "        \"\"\"Cleanup done after the agent ends.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def agent_message(self, message):\n",
    "        \"\"\"A function used to pass information from the agent to the experiment.\n",
    "        Args:\n",
    "            message: The message passed to the agent.\n",
    "        Returns:\n",
    "            The response (or answer) to the message.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def agent_policy(self, state):\n",
    "        tiles = self.tc.get_tiles(position, velocity)\n",
    "        action_values = np.zeros(self.num_actions)\n",
    "        for actionIx in range(self.num_actions):\n",
    "            action_values[actionIx] = np.sum(self.w[actionIx][tiles])\n",
    "        \n",
    "        chosen_action = argmax(action_values)\n",
    "        return chosen_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 5\n",
    "num_episodes = 500\n",
    "env_info = {}\n",
    "agent_info = {}\n",
    "all_steps = []\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "agent = SarsaAgent()\n",
    "\n",
    "for run in range(num_runs):\n",
    "    agent.agent_init()\n",
    "    steps_per_episode, episode_returns = train_episodes (env, agent, num_episodes)\n",
    "    all_steps.append(np.array(steps_per_episode))\n",
    "\n",
    "plt.plot(np.mean(np.array(all_steps), axis=0))\n",
    "np.save(\"sarsa_test\", np.array(all_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Tile Coder\n",
    "env = gym.make('MountainCar-v0')\n",
    "done = False\n",
    "observation = env.reset()\n",
    "episode_return = 0.0\n",
    "\n",
    "while not done:\n",
    "    position, velocity = observation\n",
    "    action = agent.agent_policy(observation)\n",
    "    next_observation, reward, done, _ = env.step(action)\n",
    "    observation = next_observation\n",
    "    episode_return += reward\n",
    "    env.render()\n",
    "\n",
    "print(episode_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple policy (which is close to optimial) - just accelerate in the direction we were already moving.\n",
    "done = False\n",
    "observation = env.reset()\n",
    "episode_return = 0.0\n",
    "\n",
    "while not done:\n",
    "    position, velocity = observation\n",
    "    action = 0 if velocity<0 else 2\n",
    "    next_observation, reward, done, _ = env.step(action)\n",
    "    observation = next_observation\n",
    "    episode_return += reward\n",
    "    env.render()\n",
    "\n",
    "print(episode_return)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
