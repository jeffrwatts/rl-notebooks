{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from collections import deque, defaultdict, namedtuple\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v0')\n",
    "\n",
    "PLAYER_STATE = 0\n",
    "DEALER_STATE = 1\n",
    "USABLE_ACE = 2\n",
    "\n",
    "PLAYER_STATE_COUNT = env.observation_space.spaces[PLAYER_STATE].n\n",
    "DEALER_STATE_COUNT = env.observation_space.spaces[DEALER_STATE].n\n",
    "USABLE_ACE_COUNT = env.observation_space.spaces[USABLE_ACE].n\n",
    "\n",
    "ENVIRONMENT_SPACE = PLAYER_STATE_COUNT * DEALER_STATE_COUNT * USABLE_ACE_COUNT\n",
    "ACTION_SPACE = env.action_space.n\n",
    "\n",
    "def get_state_index(state):\n",
    "    index = state[0,PLAYER_STATE] * (DEALER_STATE_COUNT * USABLE_ACE_COUNT)\n",
    "    \n",
    "    index += state[0,DEALER_STATE] * USABLE_ACE_COUNT\n",
    "    \n",
    "    if (state[0,USABLE_ACE] == True):\n",
    "        index += 1\n",
    "        \n",
    "    return index\n",
    "\n",
    "correct_policy = np.ones([PLAYER_STATE_COUNT, DEALER_STATE_COUNT, USABLE_ACE_COUNT]).astype(int)\n",
    "correct_policy[12, 4:7, 0] = 0\n",
    "correct_policy[13, 2:7, 0] = 0\n",
    "correct_policy[14, 2:7, 0] = 0\n",
    "correct_policy[15, 2:7, 0] = 0\n",
    "correct_policy[16, 2:7, 0] = 0\n",
    "correct_policy[17:22, :, 0] = 0\n",
    "correct_policy[18, 2:9, 1] = 0\n",
    "correct_policy[19:22, :, 1] = 0\n",
    "correct_policy = correct_policy.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Deep Q-learning Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 1.0    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(layers.Dense(128, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(layers.Dense(128, activation='relu'))\n",
    "        model.add(layers.Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=tf.optimizers.Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def act(self, state, train=True):\n",
    "        if (train is True):\n",
    "            if np.random.rand() <= self.epsilon:\n",
    "                return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        if (len(self.memory) < batch_size):\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * \\\n",
    "                       np.amax(self.model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = len(env.observation_space.spaces)\n",
    "action_size = env.action_space.n\n",
    "\n",
    "agent = DQNAgent(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/5000\n",
      "episode: 500/5000\n",
      "episode: 1000/5000\n",
      "episode: 1500/5000\n",
      "episode: 2000/5000\n",
      "episode: 2500/5000\n",
      "episode: 3000/5000\n",
      "episode: 3500/5000\n",
      "episode: 4000/5000\n",
      "episode: 4500/5000\n"
     ]
    }
   ],
   "source": [
    "episodes = 5000\n",
    "# Iterate the game\n",
    "for episode in range(episodes):\n",
    "    # reset state in the beginning of each game\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    # time_t represents each frame of the game\n",
    "    # Our goal is to keep the pole upright as long as possible until score of 500\n",
    "    # the more time_t the more score\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        #action = correct_policy[get_state_index(state)]\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "    if (episode % 500 == 0):\n",
    "        print(\"episode: {}/{}\".format(episode, episodes))\n",
    "            \n",
    "    # train the agent with the experience of the episode\n",
    "    agent.replay(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13131313131313133\n"
     ]
    }
   ],
   "source": [
    "state = np.zeros((1,3))\n",
    "view_learned_policy = np.zeros([PLAYER_STATE_COUNT, DEALER_STATE_COUNT, USABLE_ACE_COUNT]).astype(int)\n",
    "for playerIx in range(PLAYER_STATE_COUNT):\n",
    "    for dealerIx in range(DEALER_STATE_COUNT):\n",
    "        for usableAceIx in range(USABLE_ACE_COUNT):\n",
    "            state[0,PLAYER_STATE] = playerIx\n",
    "            state[0,DEALER_STATE] = dealerIx\n",
    "            state[0,USABLE_ACE] = usableAceIx\n",
    "            view_learned_policy[playerIx, dealerIx, usableAceIx] = agent.act(state, train=False)\n",
    "\n",
    "view_correct_policy = correct_policy.reshape(PLAYER_STATE_COUNT, DEALER_STATE_COUNT, USABLE_ACE_COUNT)\n",
    "error_rate = np.mean((np.bitwise_xor(view_learned_policy[4:22, :, :], view_correct_policy[4:22, :, :])))\n",
    "print(error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dealer Shows: 1\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0]\n",
      "Dealer Shows: 2\n",
      "[0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0]\n",
      "Dealer Shows: 3\n",
      "[0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0]\n",
      "Dealer Shows: 4\n",
      "[0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0]\n",
      "Dealer Shows: 5\n",
      "[0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0]\n",
      "Dealer Shows: 6\n",
      "[0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0]\n",
      "Dealer Shows: 7\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      "Dealer Shows: 8\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Dealer Shows: 9\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Dealer Shows: 10\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "for dealerIx in range(1, DEALER_STATE_COUNT):\n",
    "    print(\"Dealer Shows: \" + str(dealerIx))\n",
    "    print(np.bitwise_xor(view_learned_policy[4:22, dealerIx, 0], view_correct_policy[4:22, dealerIx, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-760.0\n"
     ]
    }
   ],
   "source": [
    "rewards = 0\n",
    "\n",
    "for episode in range(10000):\n",
    "\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(np.reshape(state, [1, state_size]), train=False)\n",
    "        #print(state, action)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "    \n",
    "        rewards += reward\n",
    "\n",
    "    #print(reward)\n",
    "    #print(\"========\")\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
