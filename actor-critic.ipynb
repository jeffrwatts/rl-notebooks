{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import gym\n",
    "import six\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Shape=(3,)\n",
      "Observation Range (Low)=[-1. -1. -8.]\n",
      "Observation Range (High)=[1. 1. 8.]\n",
      "\r\n",
      "Action Shape=(1,)\n",
      "Action Range (Low)=[-2.]\n",
      "Action Range (High)=[2.]\n"
     ]
    }
   ],
   "source": [
    "env_name = 'Pendulum-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# Observation Spec\n",
    "print(\"Observation Shape={}\".format(env.observation_space.shape))\n",
    "print(\"Observation Range (Low)={}\".format(env.observation_space.low))\n",
    "print(\"Observation Range (High)={}\".format(env.observation_space.high))\n",
    "print(\"\\r\")\n",
    "\n",
    "# Action Spec\n",
    "print(\"Action Shape={}\".format(env.action_space.shape))\n",
    "print(\"Action Range (Low)={}\".format(env.action_space.low))\n",
    "print(\"Action Range (High)={}\".format(env.action_space.high))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation Spec\n",
    "(3,)\n",
    "[-1. -1. -8.]\n",
    "[1. 1. 8.]\n",
    "\n",
    "Action Spec\n",
    "(1,)\n",
    "[-2.]\n",
    "[2.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(tf.keras.Model):\n",
    "    def __init__(self, observation_spec, action_spec, name=\"Actor\"):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        \n",
    "        self._observation_spec = observation_spec # shape, low, high\n",
    "        self._action_spec = action_spec # shape, low, high\n",
    "        \n",
    "        hidden_units = 64\n",
    "        \n",
    "        # Layers\n",
    "        self._layer1 = tf.keras.layers.Dense(hidden_units, activation='relu')\n",
    "        self._layer2 = tf.keras.layers.Dense(hidden_units, activation='relu')\n",
    "        self._output = tf.keras.layers.Dense(action_spec.shape[0])\n",
    "\n",
    "    def call(self, observation):\n",
    "        output = self._layer1(observation)\n",
    "        output = self._layer2(output)\n",
    "        output = self._output(output)\n",
    "        return output\n",
    "    \n",
    "    def copy(self, name):\n",
    "        # Create a new copy of this network with the initialization params that were\n",
    "        # passed to this one.\n",
    "        return type(self)(self._observation_spec, self._action_spec, name)\n",
    "    \n",
    "    def _build(self):\n",
    "        if not self.built and self._observation_spec is not None:\n",
    "            # Generate a single random observation to build the network if it hasn't\n",
    "            # been built already.\n",
    "            random_observation = tf.random.uniform((1,self._observation_spec.shape[0]), \n",
    "                                             self._observation_spec.low, \n",
    "                                             self._observation_spec.high)\n",
    "            action = self.__call__(random_observation)\n",
    "            print(action)\n",
    "\n",
    "    @property\n",
    "    def variables(self):\n",
    "        try:\n",
    "            self._build()\n",
    "        except ValueError as e:\n",
    "            traceback = sys.exc_info()[2]\n",
    "            six.reraise(\n",
    "                ValueError, \"Failed to call build on the network when accessing \"\n",
    "                \"variables. Message: {!r}.\".format(e), traceback)\n",
    "        return self.weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-0.46198443]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "actor_network = ActorNetwork(env.observation_space, env.action_space)\n",
    "actorVariables = actor_network.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(tf.keras.Model):\n",
    "    def __init__(self, observation_spec, action_spec, name=\"Critic\"):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        \n",
    "        self._observation_spec = observation_spec # shape, low, high\n",
    "        self._action_spec = action_spec # shape, low, high\n",
    "        \n",
    "        hidden_units = 64\n",
    "        \n",
    "        # Observation Layers\n",
    "        self._observation_layer1 = tf.keras.layers.Dense(hidden_units, activation='relu')\n",
    "        self._observation_layer2 = tf.keras.layers.Dense(hidden_units, activation='relu')\n",
    "        \n",
    "        # Action Layers\n",
    "        self._action_layer1 = tf.keras.layers.Dense(hidden_units, activation='relu')\n",
    "        \n",
    "        # Merged Layers\n",
    "        self._merge_layer1 = tf.keras.layers.Add()\n",
    "        self._merge_layer2 = tf.keras.layers.Dense(hidden_units, activation='relu')\n",
    "        \n",
    "        # Output Layer: Q-value for the action taken based on this observation.\n",
    "        self._output = tf.keras.layers.Dense(1, activation='relu')\n",
    "\n",
    "    def call(self, observation, action):\n",
    "        obs_output = self._observation_layer1(observation)\n",
    "        obs_output = self._observation_layer2(obs_output)\n",
    "        \n",
    "        action_output = self._action_layer1(action)\n",
    "        \n",
    "        output = self._merge_layer1([obs_output, action_output])\n",
    "        output = self._merge_layer2(output)\n",
    "        q = self._output(output)\n",
    "        return q\n",
    "    \n",
    "    def copy(self, name):\n",
    "        # Create a new copy of this network with the initialization params that were\n",
    "        # passed to this one.\n",
    "        return type(self)(self._observation_spec, self._action_spec, name)\n",
    "    \n",
    "    def _build(self):\n",
    "        if not self.built and self._observation_spec is not None:\n",
    "            # Generate a single random observation and action to build the network if it hasn't\n",
    "            # been built already.\n",
    "            random_observation = tf.random.uniform((1, self._observation_spec.shape[0]), \n",
    "                                             self._observation_spec.low, \n",
    "                                             self._observation_spec.high)\n",
    "            random_action = tf.random.uniform((1, self._action_spec.shape[0]), \n",
    "                                              self._action_spec.low, \n",
    "                                              self._action_spec.high)\n",
    "\n",
    "            q = self.__call__(random_observation, random_action)\n",
    "            print(q)\n",
    "\n",
    "    @property\n",
    "    def variables(self):\n",
    "        try:\n",
    "            self._build()\n",
    "        except ValueError as e:\n",
    "            traceback = sys.exc_info()[2]\n",
    "            six.reraise(\n",
    "                ValueError, \"Failed to call build on the network when accessing \"\n",
    "                \"variables. Message: {!r}.\".format(e), traceback)\n",
    "        return self.weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "critic_network = CriticNetwork(env.observation_space, env.action_space)\n",
    "criticVariables = critic_network.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticAgent:\n",
    "    def __init__(self, \n",
    "                 observation_spec, \n",
    "                 action_spec, \n",
    "                 actor_network,\n",
    "                 critic_network, \n",
    "                 actor_learning_rate = 3e-4, \n",
    "                 critic_learning_rate = 3e-4, \n",
    "                 gamma = 0.99): \n",
    "        \n",
    "        self._observation_spec = observation_spec # shape, low, high\n",
    "        self._action_spec = action_spec # shape, low, high\n",
    "        \n",
    "        self._actor_optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=actor_learning_rate)\n",
    "        \n",
    "        self._actor_network = actor_network\n",
    "        self._target_actor_network = actor_network.copy(\"actor-target\")\n",
    "        \n",
    "        self._critic_optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=critic_learning_rate)\n",
    "        self._critic_network = critic_network\n",
    "        self._target_critic_network = critic_network.copy(\"critic-target\")\n",
    "        \n",
    "        self._gamma = gamma\n",
    "        \n",
    "    def to_tensors(batch):\n",
    "        no_state = np.zeros(self.observation_size)\n",
    "\n",
    "        observations = np.array([ x[0] for x in batch ])\n",
    "        actions = np.array([ x[1] for x in batch ])\n",
    "        rewards = np.array([ x[2] for x in batch ])\n",
    "        next_observations = np.array([ (no_state if x[4] is True else x[3]) for x in batch ])\n",
    "\n",
    "    def train_test(self):\n",
    "        batch_size = 32\n",
    "        \n",
    "        observations = tf.random.uniform((batch_size, self._observation_spec.shape[0]), \n",
    "                                         self._observation_spec.low, \n",
    "                                         self._observation_spec.high)\n",
    "        \n",
    "        actions = tf.random.uniform((batch_size, self._action_spec.shape[0]), \n",
    "                                    self._action_spec.low, \n",
    "                                    self._action_spec.high)\n",
    "        \n",
    "        next_observations = tf.random.uniform((batch_size, self._observation_spec.shape[0]), \n",
    "                                              self._observation_spec.low, \n",
    "                                              self._observation_spec.high)\n",
    "        \n",
    "        rewards = tf.constant(1, shape=(batch_size,1), dtype=tf.float32)\n",
    "        \n",
    "        loss = self.train_batch(observations, actions, rewards, next_observations)\n",
    "        \n",
    "        print(loss)\n",
    "        \n",
    "    def train_batch(self, observations, actions, rewards, next_observations): \n",
    "        \n",
    "        # Compute Critic Loss and apply gradients.\n",
    "        critic_variables = self._critic_network.variables\n",
    "        with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "            tape.watch(critic_variables)\n",
    "            critic_loss = self.critic_loss(observations, actions, rewards, next_observations)\n",
    "        \n",
    "        critic_grads = tape.gradient(critic_loss, critic_variables)\n",
    "        critic_grads_and_vars = tuple(zip(critic_grads, critic_variables))\n",
    "        self._critic_optimizer.apply_gradients(critic_grads_and_vars)\n",
    "        \n",
    "        # Compute Actor Loss and apply gradients\n",
    "        actor_variables = self._actor_network.variables\n",
    "        with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "            tape.watch(actor_variables)\n",
    "            actor_loss = self.actor_loss(observations)\n",
    "            \n",
    "        actor_grads = tape.gradient(actor_loss, actor_variables)\n",
    "        actor_grads_and_vars = tuple(zip(actor_grads, actor_variables))\n",
    "        self._actor_optimizer.apply_gradients(actor_grads_and_vars)\n",
    "\n",
    "        # Increment counter and conditionally target networks.\n",
    "        #self.train_step_counter.assign_add(1)\n",
    "        #self._update_target()\n",
    "        \n",
    "        total_loss = actor_loss + critic_loss\n",
    "        return total_loss\n",
    "        \n",
    "\n",
    "    def critic_loss(self, observations, actions, rewards, next_observations):        \n",
    "        with tf.name_scope('critic_loss'):\n",
    "            target_actions = self._target_actor_network(observations)\n",
    "\n",
    "            target_q_values = self._target_critic_network(next_observations, target_actions)\n",
    "\n",
    "            td_targets = tf.stop_gradient(rewards + self._gamma * target_q_values)\n",
    "\n",
    "            q_values = self._critic_network(observations, actions)\n",
    "\n",
    "            critic_loss = tf.compat.v1.losses.mean_squared_error(td_targets, q_values)\n",
    "\n",
    "            critic_loss = tf.reduce_mean(critic_loss)\n",
    "\n",
    "            return critic_loss\n",
    "                \n",
    "    def actor_loss(self, observations):\n",
    "        with tf.name_scope('actor_loss'):\n",
    "            actions = self._actor_network(observations)\n",
    "            \n",
    "            with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "                tape.watch(actions)\n",
    "                q_values = self._critic_network(observations, actions)\n",
    "\n",
    "            dqdas = tape.gradient([q_values], actions)\n",
    "\n",
    "            actor_losses = []\n",
    "            \n",
    "            for dqda, action in zip(dqdas, actions):\n",
    "                loss = tf.compat.v1.losses.mean_squared_error(tf.stop_gradient(dqda + action), \n",
    "                                                              action,\n",
    "                                                             reduction=tf.compat.v1.losses.Reduction.NONE)\n",
    "                loss = tf.reduce_mean(loss)\n",
    "                actor_losses.append(loss)\n",
    "\n",
    "            actor_loss = tf.add_n(actor_losses)\n",
    "            \n",
    "            return actor_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ActorCriticAgent(env.observation_space, \n",
    "                         env.action_space, \n",
    "                         actor_network, \n",
    "                         critic_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.7272506, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "agent.train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
