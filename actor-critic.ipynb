{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import gym\n",
    "import six\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Shape=(3,)\n",
      "Observation Range (Low)=[-1. -1. -8.]\n",
      "Observation Range (High)=[1. 1. 8.]\n",
      "\r\n",
      "Action Shape=(1,)\n",
      "Action Range (Low)=[-2.]\n",
      "Action Range (High)=[2.]\n"
     ]
    }
   ],
   "source": [
    "env_name = 'Pendulum-v0'\n",
    "\n",
    "eval_env = gym.make(env_name)\n",
    "train_env = gym.make(env_name)\n",
    "\n",
    "# Observation Spec\n",
    "print(\"Observation Shape={}\".format(train_env.observation_space.shape))\n",
    "print(\"Observation Range (Low)={}\".format(train_env.observation_space.low))\n",
    "print(\"Observation Range (High)={}\".format(train_env.observation_space.high))\n",
    "print(\"\\r\")\n",
    "\n",
    "# Action Spec\n",
    "print(\"Action Shape={}\".format(train_env.action_space.shape))\n",
    "print(\"Action Range (Low)={}\".format(train_env.action_space.low))\n",
    "print(\"Action Range (High)={}\".format(train_env.action_space.high))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:   # stored as ( s, a, r, s_, d )\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, sample):\n",
    "        self.memory.append(sample)        \n",
    "\n",
    "    def sample(self, n):\n",
    "        n = min(n, len(self.memory))\n",
    "        return random.sample(self.memory, n)\n",
    "\n",
    "    def isFull(self):\n",
    "        return len(self.memory) >= self.memory.maxlen\n",
    "    \n",
    "    def update(self, idx, p):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_capacity = 50000\n",
    "memory = Memory(memory_capacity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(tf.keras.Model):\n",
    "    def __init__(self, observation_spec, action_spec, name=\"Actor\"):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        \n",
    "        self._observation_spec = observation_spec # shape, low, high\n",
    "        self._action_spec = action_spec # shape, low, high\n",
    "        \n",
    "        hidden_units = 64\n",
    "        \n",
    "        # Layers\n",
    "        self._layer1 = tf.keras.layers.Dense(hidden_units, activation='relu')\n",
    "        self._layer2 = tf.keras.layers.Dense(hidden_units, activation='relu')\n",
    "        self._output = tf.keras.layers.Dense(action_spec.shape[0])\n",
    "\n",
    "    def call(self, observation):\n",
    "        output = self._layer1(observation)\n",
    "        output = self._layer2(output)\n",
    "        output = self._output(output)\n",
    "        return output\n",
    "    \n",
    "    def copy(self, name):\n",
    "        # Create a new copy of this network with the initialization params that were\n",
    "        # passed to this one.\n",
    "        return type(self)(self._observation_spec, self._action_spec, name)\n",
    "    \n",
    "    def _build(self):\n",
    "        if not self.built and self._observation_spec is not None:\n",
    "            # Generate a single random observation to build the network if it hasn't\n",
    "            # been built already.\n",
    "            random_observation = tf.random.uniform((1,self._observation_spec.shape[0]), \n",
    "                                             self._observation_spec.low, \n",
    "                                             self._observation_spec.high)\n",
    "            action = self.__call__(random_observation)\n",
    "            print(action)\n",
    "\n",
    "    @property\n",
    "    def variables(self):\n",
    "        try:\n",
    "            self._build()\n",
    "        except ValueError as e:\n",
    "            traceback = sys.exc_info()[2]\n",
    "            six.reraise(\n",
    "                ValueError, \"Failed to call build on the network when accessing \"\n",
    "                \"variables. Message: {!r}.\".format(e), traceback)\n",
    "        return self.weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.01904323]], shape=(1, 1), dtype=float32)\n",
      "Model: \"actor_network\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  256       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  65        \n",
      "=================================================================\n",
      "Total params: 4,481\n",
      "Trainable params: 4,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "actor_network = ActorNetwork(train_env.observation_space, train_env.action_space)\n",
    "actor_variables = actor_network.variables\n",
    "actor_network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(tf.keras.Model):\n",
    "    def __init__(self, observation_spec, action_spec, name=\"Critic\"):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        \n",
    "        self._observation_spec = observation_spec # shape, low, high\n",
    "        self._action_spec = action_spec # shape, low, high\n",
    "        \n",
    "        hidden_units = 64\n",
    "        \n",
    "        # Observation Layers\n",
    "        self._observation_layer1 = tf.keras.layers.Dense(hidden_units, activation='relu')\n",
    "        self._observation_layer2 = tf.keras.layers.Dense(hidden_units, activation='relu')\n",
    "        \n",
    "        # Action Layers\n",
    "        self._action_layer1 = tf.keras.layers.Dense(hidden_units, activation='relu')\n",
    "        \n",
    "        # Merged Layers\n",
    "        self._merge_layer1 = tf.keras.layers.Add()\n",
    "        self._merge_layer2 = tf.keras.layers.Dense(hidden_units, activation='relu')\n",
    "        \n",
    "        # Output Layer: Q-value for the action taken based on this observation.\n",
    "        self._output = tf.keras.layers.Dense(1, activation='relu')\n",
    "\n",
    "    def call(self, observation, action):\n",
    "        obs_output = self._observation_layer1(observation)\n",
    "        obs_output = self._observation_layer2(obs_output)\n",
    "        \n",
    "        action_output = self._action_layer1(action)\n",
    "        \n",
    "        output = self._merge_layer1([obs_output, action_output])\n",
    "        output = self._merge_layer2(output)\n",
    "        q = self._output(output)\n",
    "        return q\n",
    "    \n",
    "    def copy(self, name):\n",
    "        # Create a new copy of this network with the initialization params that were\n",
    "        # passed to this one.\n",
    "        return type(self)(self._observation_spec, self._action_spec, name)\n",
    "    \n",
    "    def _build(self):\n",
    "        if not self.built and self._observation_spec is not None:\n",
    "            # Generate a single random observation and action to build the network if it hasn't\n",
    "            # been built already.\n",
    "            random_observation = tf.random.uniform((1, self._observation_spec.shape[0]), \n",
    "                                             self._observation_spec.low, \n",
    "                                             self._observation_spec.high)\n",
    "            random_action = tf.random.uniform((1, self._action_spec.shape[0]), \n",
    "                                              self._action_spec.low, \n",
    "                                              self._action_spec.high)\n",
    "\n",
    "            q = self.__call__(random_observation, random_action)\n",
    "            print(q)\n",
    "\n",
    "    @property\n",
    "    def variables(self):\n",
    "        try:\n",
    "            self._build()\n",
    "        except ValueError as e:\n",
    "            traceback = sys.exc_info()[2]\n",
    "            six.reraise(\n",
    "                ValueError, \"Failed to call build on the network when accessing \"\n",
    "                \"variables. Message: {!r}.\".format(e), traceback)\n",
    "        return self.weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n",
      "Model: \"critic_network\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              multiple                  256       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  128       \n",
      "_________________________________________________________________\n",
      "add (Add)                    multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              multiple                  65        \n",
      "=================================================================\n",
      "Total params: 8,769\n",
      "Trainable params: 8,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "critic_network = CriticNetwork(train_env.observation_space, train_env.action_space)\n",
    "critic_variables = critic_network.variables\n",
    "critic_network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticAgent:\n",
    "    def __init__(self, \n",
    "                 observation_spec, \n",
    "                 action_spec, \n",
    "                 actor_network,\n",
    "                 critic_network, \n",
    "                 actor_learning_rate = 3e-4, \n",
    "                 critic_learning_rate = 3e-4, \n",
    "                 gamma = 0.99): \n",
    "        \n",
    "        self._train_step_counter = tf.compat.v2.Variable(0)\n",
    "        self._observation_spec = observation_spec # shape, low, high\n",
    "        self._action_spec = action_spec # shape, low, high\n",
    "        \n",
    "        self._actor_optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=actor_learning_rate)\n",
    "        \n",
    "        self._actor_network = actor_network\n",
    "        self._target_actor_network = actor_network.copy(\"actor-target\")\n",
    "        \n",
    "        self._critic_optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=critic_learning_rate)\n",
    "        self._critic_network = critic_network\n",
    "        self._target_critic_network = critic_network.copy(\"critic-target\")\n",
    "        \n",
    "        self._gamma = gamma\n",
    "\n",
    "    def train(self, batch): \n",
    "        no_state = np.zeros(self._observation_spec.shape[0])\n",
    "        observations = tf.constant([ x[0] for x in batch ], dtype=tf.float32)\n",
    "        actions = tf.constant([ x[1] for x in batch ], dtype=tf.float32)\n",
    "        rewards = tf.expand_dims(tf.constant([ x[2] for x in batch ], dtype=tf.float32),axis=-1)\n",
    "        next_observations = tf.constant([ (no_state if x[4] is True else x[3]) for x in batch ], dtype=tf.float32)\n",
    "        \n",
    "        # Compute Critic Loss and apply gradients.\n",
    "        critic_variables = self._critic_network.variables\n",
    "        with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "            tape.watch(critic_variables)\n",
    "            critic_loss = self.critic_loss(observations, actions, rewards, next_observations)\n",
    "        \n",
    "        critic_grads = tape.gradient(critic_loss, critic_variables)\n",
    "        critic_grads_and_vars = tuple(zip(critic_grads, critic_variables))\n",
    "        self._critic_optimizer.apply_gradients(critic_grads_and_vars)\n",
    "        \n",
    "        # Compute Actor Loss and apply gradients\n",
    "        actor_variables = self._actor_network.variables\n",
    "        with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "            tape.watch(actor_variables)\n",
    "            actor_loss = self.actor_loss(observations)\n",
    "            \n",
    "        actor_grads = tape.gradient(actor_loss, actor_variables)\n",
    "        actor_grads_and_vars = tuple(zip(actor_grads, actor_variables))\n",
    "        self._actor_optimizer.apply_gradients(actor_grads_and_vars)\n",
    "\n",
    "        # Increment counter and conditionally target networks.\n",
    "        self._train_step_counter.assign_add(1)\n",
    "        #self._update_target()\n",
    "        \n",
    "        total_loss = actor_loss + critic_loss\n",
    "        return total_loss\n",
    "        \n",
    "\n",
    "    def critic_loss(self, observations, actions, rewards, next_observations):        \n",
    "        with tf.name_scope('critic_loss'):\n",
    "            target_actions = self._target_actor_network(observations)\n",
    "\n",
    "            target_q_values = self._target_critic_network(next_observations, target_actions)\n",
    "\n",
    "            td_targets = tf.stop_gradient(rewards + self._gamma * target_q_values)\n",
    "\n",
    "            q_values = self._critic_network(observations, actions)\n",
    "\n",
    "            critic_loss = tf.compat.v1.losses.mean_squared_error(td_targets, q_values)\n",
    "\n",
    "            critic_loss = tf.reduce_mean(critic_loss)\n",
    "\n",
    "            return critic_loss\n",
    "                \n",
    "    def actor_loss(self, observations):\n",
    "        with tf.name_scope('actor_loss'):\n",
    "            actions = self._actor_network(observations)\n",
    "            \n",
    "            with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "                tape.watch(actions)\n",
    "                q_values = self._critic_network(observations, actions)\n",
    "\n",
    "            dqdas = tape.gradient([q_values], actions)\n",
    "\n",
    "            actor_losses = []\n",
    "            \n",
    "            for dqda, action in zip(dqdas, actions):\n",
    "                loss = tf.compat.v1.losses.mean_squared_error(tf.stop_gradient(dqda + action), \n",
    "                                                              action,\n",
    "                                                             reduction=tf.compat.v1.losses.Reduction.NONE)\n",
    "                loss = tf.reduce_mean(loss)\n",
    "                actor_losses.append(loss)\n",
    "\n",
    "            actor_loss = tf.add_n(actor_losses)\n",
    "            \n",
    "            return actor_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ActorCriticAgent(train_env.observation_space, \n",
    "                         train_env.action_space, \n",
    "                         actor_network, \n",
    "                         critic_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1521.9634213417144\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "observation = train_env.reset()\n",
    "episode_rewards = 0\n",
    "while not done:\n",
    "    action = train_env.action_space.sample()\n",
    "    next_observation, reward, done, _ = train_env.step(action)\n",
    "    memory.add((observation, action, reward, next_observation, done))\n",
    "    observation = next_observation\n",
    "    episode_rewards += reward\n",
    "\n",
    "print(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(env, policy, num_episodes=10):\n",
    "\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "        episode_return = 0.0\n",
    "        \n",
    "        while not done:\n",
    "            action = env.action_space.sample() # TODO: replace with policy\n",
    "            next_observation, reward, done, _ = env.step(action)\n",
    "            memory.add((observation, action, reward, next_observation, done))\n",
    "            observation = next_observation\n",
    "            episode_return += reward\n",
    "\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 100: loss = 28.36805534362793\n",
      "step = 200: loss = 51.639686584472656\n",
      "step = 300: loss = 45.86397933959961\n",
      "step = 400: loss = 39.15007019042969\n",
      "step = 500: loss = 50.80495071411133\n",
      "step = 600: loss = 51.993736267089844\n",
      "step = 700: loss = 60.535552978515625\n",
      "step = 800: loss = 46.92090606689453\n",
      "step = 900: loss = 33.07298278808594\n",
      "step = 1000: loss = 42.93412780761719\n",
      "step = 1000: Average Return = -1411.4911204163868\n",
      "step = 1100: loss = 56.757286071777344\n",
      "step = 1200: loss = 41.279640197753906\n",
      "step = 1300: loss = 58.4522590637207\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-aedc4c10d2bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Sample a batch of data from the buffer and update the agent's network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_step_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-c0238e15ddcc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwatch_accessed_variables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mactor_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mactor_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-c0238e15ddcc>\u001b[0m in \u001b[0;36mactor_loss\u001b[0;34m(self, observations)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mactor_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mdqda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdqdas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                 loss = tf.compat.v1.losses.mean_squared_error(tf.stop_gradient(dqda + action), \n\u001b[1;32m     91\u001b[0m                                                               \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    494\u001b[0m           \"Cannot iterate over a tensor with unknown first dimension.\")\n\u001b[1;32m    495\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m       \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_shape_as_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0mvar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[1;32m    844\u001b[0m       \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m       shrink_axis_mask=shrink_axis_mask)\n\u001b[0m\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m   \u001b[0mparent_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[1;32m   9944\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9945\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9946\u001b[0;31m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   9947\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9948\u001b[0m         \u001b[0;34m\"StridedSlice\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbegin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_iterations = 2000\n",
    "log_interval = 100\n",
    "eval_interval = 1000\n",
    "num_eval_episodes = 5\n",
    "\n",
    "# Reset the train step\n",
    "agent._train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "eval_policy = None\n",
    "avg_return = compute_avg_return(eval_env, eval_policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    #for _ in range(collect_steps_per_iteration):\n",
    "    #    collect_driver.run()\n",
    "\n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    batch = memory.sample(32)\n",
    "    train_loss = agent.train(batch)\n",
    "\n",
    "    step = agent._train_step_counter.numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, eval_policy, num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (5):\n",
    "    batch = memory.sample(32)\n",
    "    loss = agent.train(batch)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
