{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import gym\n",
    "import six\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Shape=(3,)\n",
      "Observation Range (Low)=[-1. -1. -8.]\n",
      "Observation Range (High)=[1. 1. 8.]\n",
      "\r\n",
      "Action Shape=(1,)\n",
      "Action Range (Low)=[-2.]\n",
      "Action Range (High)=[2.]\n"
     ]
    }
   ],
   "source": [
    "env_name = 'Pendulum-v0'\n",
    "\n",
    "eval_env = gym.make(env_name)\n",
    "train_env = gym.make(env_name)\n",
    "\n",
    "# Observation Spec\n",
    "print(\"Observation Shape={}\".format(train_env.observation_space.shape))\n",
    "print(\"Observation Range (Low)={}\".format(train_env.observation_space.low))\n",
    "print(\"Observation Range (High)={}\".format(train_env.observation_space.high))\n",
    "print(\"\\r\")\n",
    "\n",
    "# Action Spec\n",
    "print(\"Action Shape={}\".format(train_env.action_space.shape))\n",
    "print(\"Action Range (Low)={}\".format(train_env.action_space.low))\n",
    "print(\"Action Range (High)={}\".format(train_env.action_space.high))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:   # stored as ( s, a, r, s_, d )\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, sample):\n",
    "        self.memory.append(sample)        \n",
    "\n",
    "    def sample(self, n):\n",
    "        n = min(n, len(self.memory))\n",
    "        return random.sample(self.memory, n)\n",
    "\n",
    "    def isFull(self):\n",
    "        return len(self.memory) >= self.memory.maxlen\n",
    "    \n",
    "    def update(self, idx, p):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_capacity = 50000\n",
    "memory = Memory(memory_capacity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(tf.keras.Model):\n",
    "    def __init__(self, observation_spec, action_spec, name=\"Actor\"):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        \n",
    "        self._observation_spec = observation_spec # shape, low, high\n",
    "        self._action_spec = action_spec # shape, low, high\n",
    "        self._action_mean = tf.constant((action_spec.high + action_spec.low) / 2)\n",
    "        self._action_scale = tf.constant((action_spec.high - action_spec.low) / 2)\n",
    "        \n",
    "        hidden_units = 128\n",
    "        \n",
    "        # Layers\n",
    "        self._layer1 = tf.keras.layers.Dense(hidden_units, \n",
    "                                             activation='relu', \n",
    "                                             kernel_initializer='he_uniform')\n",
    "        self._layer2 = tf.keras.layers.Dense(hidden_units, \n",
    "                                             activation='relu', \n",
    "                                             kernel_initializer='he_uniform')\n",
    "        self._output = tf.keras.layers.Dense(action_spec.shape[0], \n",
    "                                             activation='tanh', \n",
    "                                             kernel_initializer='he_uniform')\n",
    "\n",
    "    def call(self, observation):\n",
    "        output = self._layer1(observation)\n",
    "        output = self._layer2(output)\n",
    "        output = self._output(output)\n",
    "        return output * self._action_scale + self._action_mean\n",
    "    \n",
    "    def copy(self, name):\n",
    "        # Create a new copy of this network with the initialization params that were\n",
    "        # passed to this one.\n",
    "        return type(self)(self._observation_spec, self._action_spec, name)\n",
    "    \n",
    "    def _build(self):\n",
    "        if not self.built and self._observation_spec is not None:\n",
    "            # Generate a single random observation to build the network if it hasn't\n",
    "            # been built already.\n",
    "            random_observation = tf.random.uniform((1,self._observation_spec.shape[0]), \n",
    "                                             self._observation_spec.low, \n",
    "                                             self._observation_spec.high)\n",
    "            action = self.__call__(random_observation)\n",
    "\n",
    "    @property\n",
    "    def variables(self):\n",
    "        try:\n",
    "            self._build()\n",
    "        except ValueError as e:\n",
    "            traceback = sys.exc_info()[2]\n",
    "            six.reraise(\n",
    "                ValueError, \"Failed to call build on the network when accessing \"\n",
    "                \"variables. Message: {!r}.\".format(e), traceback)\n",
    "        return self.weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"actor_network_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              multiple                  512       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  16512     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  129       \n",
      "=================================================================\n",
      "Total params: 17,153\n",
      "Trainable params: 17,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "actor_network = ActorNetwork(train_env.observation_space, train_env.action_space)\n",
    "actor_variables = actor_network.variables\n",
    "actor_network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(tf.keras.Model):\n",
    "    def __init__(self, observation_spec, action_spec, name=\"Critic\"):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        \n",
    "        self._observation_spec = observation_spec # shape, low, high\n",
    "        self._action_spec = action_spec # shape, low, high\n",
    "        \n",
    "        hidden_units = 128\n",
    "        \n",
    "        # Observation Layers\n",
    "        self._observation_layer1 = tf.keras.layers.Dense(hidden_units, \n",
    "                                                         activation='relu',\n",
    "                                                         kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n",
    "            scale=1. / 3., mode='fan_in', distribution='uniform'))\n",
    "        #self._observation_layer2 = tf.keras.layers.Dense(hidden_units, \n",
    "        #                                                 activation='relu',\n",
    "        #                                                 kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n",
    "        #    scale=1. / 3., mode='fan_in', distribution='uniform'))\n",
    "        \n",
    "        # Action Layers\n",
    "        self._action_layer1 = tf.keras.layers.Dense(hidden_units, \n",
    "                                                    activation='relu', \n",
    "                                                    kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n",
    "            scale=1. / 3., mode='fan_in', distribution='uniform'))\n",
    "        \n",
    "        # Merged Layers\n",
    "        self._merge_layer1 = tf.keras.layers.Add()\n",
    "        self._merge_layer2 = tf.keras.layers.Dense(hidden_units, \n",
    "                                                   activation='relu', \n",
    "                                                   kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n",
    "            scale=1. / 3., mode='fan_in', distribution='uniform'))\n",
    "        \n",
    "        # Output Layer: Q-value for the action taken based on this observation.\n",
    "        self._output_layer = tf.keras.layers.Dense(1, \n",
    "                                                   activation='relu', \n",
    "                                                   kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "                minval=-0.003, maxval=0.003))\n",
    "\n",
    "    def call(self, observation, action):\n",
    "        print(observation)\n",
    "        print(action)\n",
    "        obs_output = self._observation_layer1(observation)\n",
    "        print(\"obs_output={}\".format(obs_output))\n",
    "        #obs_output = self._observation_layer2(obs_output)\n",
    "        #print(\"obs_output={}\".format(obs_output))\n",
    "        \n",
    "        action_output = self._action_layer1(action)\n",
    "        print(\"action_output={}\".format(action_output))\n",
    "        \n",
    "        output = self._merge_layer1([obs_output, action_output])\n",
    "        print(\"output={}\".format(output))\n",
    "\n",
    "        output = self._merge_layer2(output)\n",
    "        print(\"output={}\".format(output))\n",
    "\n",
    "        output = self._output_layer(output)\n",
    "        print(\"output={}\".format(output))\n",
    "        return output\n",
    "    \n",
    "    def copy(self, name):\n",
    "        # Create a new copy of this network with the initialization params that were\n",
    "        # passed to this one.\n",
    "        return type(self)(self._observation_spec, self._action_spec, name)\n",
    "    \n",
    "    def _build(self):\n",
    "        if not self.built and self._observation_spec is not None:\n",
    "            # Generate a single random observation and action to build the network if it hasn't\n",
    "            # been built already.\n",
    "            random_observation = tf.random.uniform((1, self._observation_spec.shape[0]), \n",
    "                                             [-0.1, -0.1, -0.1], \n",
    "                                             [0.1, 0.1, 0.1])\n",
    "            random_action = tf.random.uniform((1, self._action_spec.shape[0]), \n",
    "                                              [-0.1], \n",
    "                                              [0.1])\n",
    "\n",
    "            q = self.__call__(random_observation, random_action)\n",
    "\n",
    "    @property\n",
    "    def variables(self):\n",
    "        try:\n",
    "            self._build()\n",
    "        except ValueError as e:\n",
    "            traceback = sys.exc_info()[2]\n",
    "            six.reraise(\n",
    "                ValueError, \"Failed to call build on the network when accessing \"\n",
    "                \"variables. Message: {!r}.\".format(e), traceback)\n",
    "        return self.weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-0.05010579  0.05722728 -0.07889152]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor([[0.0462544]], shape=(1, 1), dtype=float32)\n",
      "obs_output=[[0.         0.         0.         0.06373786 0.         0.06809693\n",
      "  0.01212819 0.00752896 0.         0.         0.03520053 0.07635716\n",
      "  0.         0.         0.         0.03842733 0.         0.05193348\n",
      "  0.02520209 0.         0.04383183 0.05485588 0.         0.\n",
      "  0.0192854  0.         0.0137463  0.         0.01956771 0.04075774\n",
      "  0.         0.         0.         0.05153321 0.01403232 0.\n",
      "  0.         0.         0.         0.00986948 0.         0.\n",
      "  0.         0.         0.         0.         0.00081735 0.\n",
      "  0.         0.07648408 0.02781786 0.         0.02492818 0.\n",
      "  0.         0.         0.         0.03180475 0.00222764 0.0276683\n",
      "  0.         0.03869043 0.00172387 0.         0.         0.\n",
      "  0.06178748 0.02292705 0.00437802 0.         0.         0.03413288\n",
      "  0.03820348 0.         0.00146915 0.         0.         0.\n",
      "  0.0410388  0.00354084 0.         0.         0.0086085  0.04845211\n",
      "  0.01119615 0.0305857  0.05578833 0.04502035 0.         0.\n",
      "  0.04470813 0.         0.00725798 0.         0.         0.01560978\n",
      "  0.         0.         0.04123745 0.         0.         0.\n",
      "  0.01447621 0.02088086 0.05447111 0.         0.         0.\n",
      "  0.         0.03498008 0.01319095 0.0431616  0.         0.03363298\n",
      "  0.         0.02977381 0.03956994 0.02349268 0.03397588 0.\n",
      "  0.         0.         0.03078404 0.         0.         0.\n",
      "  0.         0.01801412]]\n",
      "action_output=[[2.07324701e-05 7.58688198e-03 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.16711454e-02 3.35394009e-03\n",
      "  1.76257864e-02 0.00000000e+00 0.00000000e+00 3.89506742e-02\n",
      "  2.26699077e-02 0.00000000e+00 0.00000000e+00 2.61792634e-02\n",
      "  2.69696452e-02 0.00000000e+00 7.48684746e-04 0.00000000e+00\n",
      "  0.00000000e+00 3.43523659e-02 2.30134055e-02 2.16426570e-02\n",
      "  0.00000000e+00 0.00000000e+00 4.22251374e-02 2.11786479e-02\n",
      "  2.44720001e-02 3.64211462e-02 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 4.05473523e-02 1.93297956e-02 1.35872224e-02\n",
      "  0.00000000e+00 1.74346399e-02 4.03234176e-02 3.46513130e-02\n",
      "  1.50070880e-02 4.13788892e-02 0.00000000e+00 3.43231224e-02\n",
      "  0.00000000e+00 4.27380651e-02 4.49018963e-02 1.54355438e-02\n",
      "  1.17907980e-02 4.49140482e-02 9.79115162e-03 2.95310766e-02\n",
      "  3.10926829e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  2.10561827e-02 0.00000000e+00 5.59984008e-03 4.49228510e-02\n",
      "  0.00000000e+00 2.02248548e-03 7.65148317e-03 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  2.97953375e-02 1.29494891e-02 3.35545875e-02 0.00000000e+00\n",
      "  2.40149926e-02 0.00000000e+00 2.17684861e-02 0.00000000e+00\n",
      "  2.49884594e-02 3.24263200e-02 0.00000000e+00 0.00000000e+00\n",
      "  2.19014939e-02 0.00000000e+00 4.10250239e-02 3.93882059e-02\n",
      "  0.00000000e+00 4.45850659e-03 0.00000000e+00 1.39643215e-02\n",
      "  3.13397758e-02 0.00000000e+00 3.85402143e-02 3.78483012e-02\n",
      "  4.17614356e-02 0.00000000e+00 4.46483195e-02 3.72965448e-02\n",
      "  0.00000000e+00 0.00000000e+00 4.47788276e-03 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.02865633e-02\n",
      "  1.82613581e-02 0.00000000e+00 1.46807935e-02 1.89821962e-02\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.11057868e-02\n",
      "  4.34593149e-02 0.00000000e+00 3.95132974e-02 0.00000000e+00\n",
      "  2.16831863e-02 2.79897265e-02 0.00000000e+00 4.34395857e-02\n",
      "  2.02534385e-02 0.00000000e+00 3.76994582e-03 0.00000000e+00]]\n",
      "output=[[2.0732470e-05 7.5868820e-03 0.0000000e+00 6.3737862e-02 0.0000000e+00\n",
      "  6.8096928e-02 2.3799336e-02 1.0882897e-02 1.7625786e-02 0.0000000e+00\n",
      "  3.5200529e-02 1.1530783e-01 2.2669908e-02 0.0000000e+00 0.0000000e+00\n",
      "  6.4606592e-02 2.6969645e-02 5.1933482e-02 2.5950776e-02 0.0000000e+00\n",
      "  4.3831825e-02 8.9208245e-02 2.3013406e-02 2.1642657e-02 1.9285401e-02\n",
      "  0.0000000e+00 5.5971436e-02 2.1178648e-02 4.4039711e-02 7.7178881e-02\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 5.1533207e-02 1.4032319e-02\n",
      "  0.0000000e+00 0.0000000e+00 4.0547352e-02 1.9329796e-02 2.3456704e-02\n",
      "  0.0000000e+00 1.7434640e-02 4.0323418e-02 3.4651313e-02 1.5007088e-02\n",
      "  4.1378889e-02 8.1735291e-04 3.4323122e-02 0.0000000e+00 1.1922215e-01\n",
      "  7.2719753e-02 1.5435544e-02 3.6718983e-02 4.4914048e-02 9.7911516e-03\n",
      "  2.9531077e-02 3.1092683e-02 3.1804752e-02 2.2276379e-03 2.7668299e-02\n",
      "  2.1056183e-02 3.8690429e-02 7.3237107e-03 4.4922851e-02 0.0000000e+00\n",
      "  2.0224855e-03 6.9438964e-02 2.2927046e-02 4.3780198e-03 0.0000000e+00\n",
      "  0.0000000e+00 3.4132876e-02 6.7998812e-02 1.2949489e-02 3.5023741e-02\n",
      "  0.0000000e+00 2.4014993e-02 0.0000000e+00 6.2807284e-02 3.5408437e-03\n",
      "  2.4988459e-02 3.2426320e-02 8.6084967e-03 4.8452113e-02 3.3097647e-02\n",
      "  3.0585699e-02 9.6813351e-02 8.4408551e-02 0.0000000e+00 4.4585066e-03\n",
      "  4.4708129e-02 1.3964321e-02 3.8597751e-02 0.0000000e+00 3.8540214e-02\n",
      "  5.3458076e-02 4.1761436e-02 0.0000000e+00 8.5885771e-02 3.7296545e-02\n",
      "  0.0000000e+00 0.0000000e+00 1.8954098e-02 2.0880857e-02 5.4471113e-02\n",
      "  0.0000000e+00 0.0000000e+00 4.0286563e-02 1.8261358e-02 3.4980077e-02\n",
      "  2.7871743e-02 6.2143795e-02 0.0000000e+00 3.3632983e-02 0.0000000e+00\n",
      "  5.0879598e-02 8.3029255e-02 2.3492683e-02 7.3489174e-02 0.0000000e+00\n",
      "  2.1683186e-02 2.7989727e-02 3.0784037e-02 4.3439586e-02 2.0253439e-02\n",
      "  0.0000000e+00 3.7699458e-03 1.8014122e-02]]\n",
      "output=[[0.03282408 0.00356164 0.00098218 0.01384399 0.03080065 0.\n",
      "  0.         0.00178706 0.034654   0.         0.         0.\n",
      "  0.         0.04545458 0.         0.00808268 0.03368669 0.\n",
      "  0.         0.         0.02388042 0.         0.01817425 0.\n",
      "  0.         0.         0.01523444 0.         0.         0.\n",
      "  0.         0.02156173 0.         0.00870703 0.         0.01097574\n",
      "  0.         0.         0.         0.01683856 0.00355623 0.00413892\n",
      "  0.02432169 0.00904554 0.         0.01939238 0.00723896 0.\n",
      "  0.01670327 0.0252993  0.02681135 0.02520682 0.         0.00680594\n",
      "  0.02071398 0.         0.01962563 0.         0.         0.\n",
      "  0.         0.03068499 0.         0.00341744 0.         0.\n",
      "  0.01106433 0.00916806 0.00566519 0.01288728 0.01719298 0.02022725\n",
      "  0.         0.         0.         0.00257173 0.         0.01233615\n",
      "  0.         0.         0.         0.00667548 0.         0.\n",
      "  0.00481837 0.         0.         0.         0.         0.0024563\n",
      "  0.00326023 0.         0.         0.         0.0278781  0.013304\n",
      "  0.         0.01164538 0.02388892 0.         0.         0.02827665\n",
      "  0.00183912 0.03935152 0.         0.         0.01934826 0.02235677\n",
      "  0.         0.         0.         0.         0.03280051 0.00576707\n",
      "  0.         0.         0.         0.01467637 0.01045449 0.\n",
      "  0.         0.01014172 0.02896849 0.00766068 0.00103424 0.\n",
      "  0.02680893 0.01113657]]\n",
      "output=[[1.8887549e-06]]\n",
      "Model: \"critic_network_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             multiple                  512       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             multiple                  256       \n",
      "_________________________________________________________________\n",
      "add_1 (Add)                  multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             multiple                  16512     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             multiple                  129       \n",
      "=================================================================\n",
      "Total params: 17,409\n",
      "Trainable params: 17,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "critic_network = CriticNetwork(train_env.observation_space, train_env.action_space)\n",
    "critic_variables = critic_network.variables\n",
    "critic_network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticAgent:\n",
    "    def __init__(self, \n",
    "                 observation_spec, \n",
    "                 action_spec, \n",
    "                 actor_network,\n",
    "                 critic_network, \n",
    "                 actor_learning_rate = 3e-4, \n",
    "                 critic_learning_rate = 3e-4, \n",
    "                 gamma = 0.99): \n",
    "        \n",
    "        self._train_step_counter = tf.compat.v2.Variable(0)\n",
    "        self._observation_spec = observation_spec # shape, low, high\n",
    "        self._action_spec = action_spec # shape, low, high\n",
    "        \n",
    "        self._actor_optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=actor_learning_rate)\n",
    "        \n",
    "        self._actor_network = actor_network\n",
    "        self._target_actor_network = actor_network.copy(\"actor-target\")\n",
    "        \n",
    "        self._critic_optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=critic_learning_rate)\n",
    "        self._critic_network = critic_network\n",
    "        self._target_critic_network = critic_network.copy(\"critic-target\")\n",
    "        \n",
    "        self._gamma = gamma\n",
    "        \n",
    "        self._initial_value = 0\n",
    "        self._damping = 0.15\n",
    "        self._stddev = 0.2\n",
    "        self._x = tf.compat.v2.Variable(initial_value=self._initial_value, \n",
    "                                        dtype=tf.float32,\n",
    "                                        trainable=False)\n",
    "        \n",
    "        self._update_target()\n",
    "        \n",
    "    def collect_policy(self, observation):\n",
    "        observation = tf.expand_dims(tf.constant(observation), axis=0)\n",
    "        action = self._actor_network(observation)\n",
    "        \n",
    "        noise = tf.random.normal(shape=self._x.shape,\n",
    "                                 stddev=self._stddev,\n",
    "                                 dtype=self._x.dtype)\n",
    "        \n",
    "        self._x.assign((1. - self._damping) * self._x + noise)\n",
    "        \n",
    "        return self._x + action\n",
    "\n",
    "    def eval_policy(self, observation):\n",
    "        observation = tf.expand_dims(tf.constant(observation), axis=0)\n",
    "        action = self._actor_network(observation)\n",
    "        return action\n",
    "    \n",
    "    def _update_network(self, source_variables, target_variables):\n",
    "        updates = []\n",
    "        for (v_s, v_t) in zip(source_variables, target_variables):\n",
    "            update = v_t.assign(v_s)\n",
    "            updates.append(update)\n",
    "        return tf.group(*updates)\n",
    "\n",
    "    def _update_target(self):\n",
    "        self._update_network(self._actor_network.variables, self._target_actor_network.variables)\n",
    "        self._update_network(self._critic_network.variables, self._target_critic_network.variables)\n",
    "\n",
    "    def train(self, batch): \n",
    "        no_state = np.zeros(self._observation_spec.shape[0])\n",
    "        observations = tf.constant([ x[0] for x in batch ], dtype=tf.float32)\n",
    "        actions = tf.constant([ x[1] for x in batch ], dtype=tf.float32)\n",
    "        rewards = tf.expand_dims(tf.constant([ x[2] for x in batch ], dtype=tf.float32),axis=-1)\n",
    "        next_observations = tf.constant([ (no_state if x[4] is True else x[3]) for x in batch ], dtype=tf.float32)\n",
    "        \n",
    "        # Compute Critic Loss and apply gradients.\n",
    "        critic_variables = self._critic_network.variables\n",
    "        with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "            tape.watch(critic_variables)\n",
    "            critic_loss = self.critic_loss(observations, actions, rewards, next_observations)\n",
    "        \n",
    "        critic_grads = tape.gradient(critic_loss, critic_variables)\n",
    "        critic_grads_and_vars = tuple(zip(critic_grads, critic_variables))\n",
    "        self._critic_optimizer.apply_gradients(critic_grads_and_vars)\n",
    "        \n",
    "        # Compute Actor Loss and apply gradients\n",
    "        actor_variables = self._actor_network.variables\n",
    "        with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "            tape.watch(actor_variables)\n",
    "            actor_loss = self.actor_loss(observations)\n",
    "            \n",
    "        actor_grads = tape.gradient(actor_loss, actor_variables)\n",
    "        actor_grads_and_vars = tuple(zip(actor_grads, actor_variables))\n",
    "        self._actor_optimizer.apply_gradients(actor_grads_and_vars)\n",
    "\n",
    "        # Increment counter and conditionally target networks.\n",
    "        self._train_step_counter.assign_add(1)\n",
    "        self._update_target()\n",
    "        \n",
    "        total_loss = actor_loss + critic_loss\n",
    "        return total_loss\n",
    "        \n",
    "\n",
    "    def critic_loss(self, observations, actions, rewards, next_observations):        \n",
    "        with tf.name_scope('critic_loss'):\n",
    "            target_actions = self._target_actor_network(observations)\n",
    "\n",
    "            target_q_values = self._target_critic_network(next_observations, target_actions)\n",
    "\n",
    "            td_targets = tf.stop_gradient(rewards + self._gamma * target_q_values)\n",
    "\n",
    "            q_values = self._critic_network(observations, actions)\n",
    "\n",
    "            critic_loss = tf.compat.v1.losses.mean_squared_error(td_targets, q_values)\n",
    "\n",
    "            critic_loss = tf.reduce_mean(critic_loss)\n",
    "\n",
    "            return critic_loss\n",
    "                \n",
    "    def actor_loss(self, observations):\n",
    "        with tf.name_scope('actor_loss'):\n",
    "            actions = self._actor_network(observations)\n",
    "            with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "                tape.watch(actions)\n",
    "                q_values = self._critic_network(observations, actions)\n",
    "                actions = tf.nest.flatten(actions)\n",
    "\n",
    "            dqdas = tape.gradient([q_values], actions)\n",
    "\n",
    "            actor_losses = []\n",
    "            \n",
    "            for dqda, action in zip(dqdas, actions):\n",
    "                loss = tf.compat.v1.losses.mean_squared_error(tf.stop_gradient(dqda + action), \n",
    "                                                              action,\n",
    "                                                             reduction=tf.compat.v1.losses.Reduction.NONE)\n",
    "                loss = tf.reduce_mean(loss)\n",
    "                actor_losses.append(loss)\n",
    "\n",
    "            actor_loss = tf.add_n(actor_losses)\n",
    "            \n",
    "            return actor_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ActorCriticAgent(train_env.observation_space, \n",
    "                         train_env.action_space, \n",
    "                         actor_network, \n",
    "                         critic_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(env, agent, num_episodes=10):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "        episode_return = 0.0\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.eval_policy(observation)[0].numpy()\n",
    "            next_observation, reward, done, _ = env.step(action)\n",
    "            memory.add((observation, action, reward, next_observation, done))\n",
    "            observation = next_observation\n",
    "            episode_return += reward\n",
    "\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_collect_steps = 1000\n",
    "done = True\n",
    "for _ in range(initial_collect_steps):\n",
    "    if (done):\n",
    "        observation = train_env.reset()\n",
    "        done = False\n",
    "    else:\n",
    "        observation = next_observation\n",
    "        \n",
    "    action = tf.random.uniform(train_env.action_space.shape, \n",
    "                               minval=train_env.action_space.low,\n",
    "                               maxval=train_env.action_space.high,\n",
    "                               dtype=tf.dtypes.float32).numpy()\n",
    "    \n",
    "    next_observation, reward, done, _ = train_env.step(action)\n",
    "    \n",
    "    memory.add((observation, action, reward, next_observation, done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 10000\n",
    "log_interval = 100\n",
    "eval_interval = 1000\n",
    "num_eval_episodes = 5\n",
    "\n",
    "# Reset the train step\n",
    "agent._train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "done = True\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "    if (done):\n",
    "        observation = train_env.reset()\n",
    "        done = False\n",
    "    else:\n",
    "        observation = next_observation\n",
    "        \n",
    "    action = agent.collect_policy(observation)[0].numpy()\n",
    "    next_observation, reward, done, _ = train_env.step(action)\n",
    "    memory.add((observation, action, reward, next_observation, done))\n",
    "\n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    batch = memory.sample(32)\n",
    "    train_loss = agent.train(batch)\n",
    "\n",
    "    step = agent._train_step_counter.numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, agent, num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "observation = eval_env.reset()\n",
    "episode_return = 0.0\n",
    "\n",
    "while not done:\n",
    "    action = agent.eval_policy(observation)[0].numpy()\n",
    "    next_observation, reward, done, _ = eval_env.step(action)\n",
    "    observation = next_observation\n",
    "    episode_return += reward\n",
    "    eval_env.render()\n",
    "\n",
    "print(episode_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = memory.sample(32)\n",
    "observations = tf.constant([ x[0] for x in batch ], dtype=tf.float32)\n",
    "\n",
    "actions = agent._actor_network(observations)\n",
    "print(actions)\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "    tape.watch(actions)\n",
    "    q_values = agent._critic_network(observations, actions)\n",
    "    print(q_values)\n",
    "    actions = tf.nest.flatten(actions)\n",
    "\n",
    "dqdas = tape.gradient([q_values], actions)\n",
    "print(dqdas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
