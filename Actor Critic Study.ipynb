{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from tf_agents.policies import actor_policy\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.agents.ddpg import critic_network\n",
    "from tf_agents.agents.sac import sac_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.policies import greedy_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"Pendulum-v0\" \n",
    "\n",
    "num_iterations = 20000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 1000 # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1 # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 25000 # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 256 # @param {type:\"integer\"}\n",
    "\n",
    "critic_learning_rate = 1e-3 # @param {type:\"number\"}\n",
    "actor_learning_rate = 1e-3 # @param {type:\"number\"}\n",
    "alpha_learning_rate = 1e-3 # @param {type:\"number\"}\n",
    "target_update_tau = 0.05 # @param {type:\"number\"}\n",
    "target_update_period = 1 # @param {type:\"number\"}\n",
    "gamma = 0.99 # @param {type:\"number\"}\n",
    "reward_scale_factor = 1.0 # @param {type:\"number\"}\n",
    "gradient_clipping = None # @param\n",
    "\n",
    "actor_fc_layer_params = (256,)\n",
    "critic_joint_fc_layer_params = (256,)\n",
    "\n",
    "log_interval = 200 # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10 # @param {type:\"integer\"}\n",
    "eval_interval = 2000 # @param {type:\"integer\"}\n",
    "max_episode_steps = 1000\n",
    "\n",
    "load_weights = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "BoundedTensorSpec(shape=(3,), dtype=tf.float32, name='observation', minimum=array([-1., -1., -8.], dtype=float32), maximum=array([1., 1., 8.], dtype=float32))\n",
      "Action Spec:\n",
      "BoundedTensorSpec(shape=(1,), dtype=tf.float32, name='action', minimum=array(-2., dtype=float32), maximum=array(2., dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "\n",
    "observation_spec = train_env.observation_spec()\n",
    "action_spec = train_env.action_spec()\n",
    "\n",
    "print('Observation Spec:')\n",
    "print(observation_spec)\n",
    "print('Action Spec:')\n",
    "print(action_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic\n",
    "critic_net = critic_network.CriticNetwork(\n",
    "    (observation_spec, action_spec),\n",
    "    observation_fc_layer_params=None,\n",
    "    action_fc_layer_params=None,\n",
    "    joint_fc_layer_params=critic_joint_fc_layer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.networks import categorical_projection_network\n",
    "from tf_agents.networks import encoding_network\n",
    "from tf_agents.networks import network\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.utils import nest_utils\n",
    "\n",
    "from tf_agents.distributions import utils as distribution_utils\n",
    "from tf_agents.networks import bias_layer\n",
    "from tf_agents.networks import utils as network_utils\n",
    "from tf_agents.specs import distribution_spec\n",
    "from tf_agents.specs import tensor_spec\n",
    "\n",
    "\n",
    "def tanh_squash_to_spec(inputs, spec):\n",
    "    \"\"\"Maps inputs with arbitrary range to range defined by spec using `tanh`.\"\"\"\n",
    "    means = (spec.maximum + spec.minimum) / 2.0\n",
    "    magnitudes = (spec.maximum - spec.minimum) / 2.0\n",
    "\n",
    "    return means + magnitudes * tf.tanh(inputs)\n",
    "\n",
    "\n",
    "class NormalProjectionNetwork(network.DistributionNetwork):\n",
    "    def __init__(self,\n",
    "        sample_spec,\n",
    "        activation_fn=None,\n",
    "        init_means_output_factor=0.1,\n",
    "        std_bias_initializer_value=0.0,\n",
    "        mean_transform=tanh_squash_to_spec,\n",
    "        std_transform=tf.nn.softplus,\n",
    "        state_dependent_std=False,\n",
    "        scale_distribution=False,\n",
    "        name='NormalProjectionNetwork'):\n",
    "        print(\"Using Normal\")\n",
    "        if len(tf.nest.flatten(sample_spec)) != 1:\n",
    "            raise ValueError('Normal Projection network only supports single spec '\n",
    "                       'samples.')\n",
    "        self._scale_distribution = scale_distribution\n",
    "        output_spec = self._output_distribution_spec(sample_spec, name)\n",
    "        super(NormalProjectionNetwork, self).__init__(\n",
    "            # We don't need these, but base class requires them.\n",
    "            input_tensor_spec=None,\n",
    "            state_spec=(),\n",
    "            output_spec=output_spec,\n",
    "            name=name)\n",
    "\n",
    "        self._sample_spec = sample_spec\n",
    "        self._mean_transform = mean_transform\n",
    "        self._std_transform = std_transform\n",
    "        self._state_dependent_std = state_dependent_std\n",
    "\n",
    "        self._means_projection_layer = tf.keras.layers.Dense(\n",
    "            sample_spec.shape.num_elements(),\n",
    "            activation=activation_fn,\n",
    "            kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n",
    "                scale=init_means_output_factor),\n",
    "            bias_initializer=tf.keras.initializers.Zeros(),\n",
    "            name='means_projection_layer')\n",
    "\n",
    "        self._stddev_projection_layer = None\n",
    "        if self._state_dependent_std:\n",
    "            self._stddev_projection_layer = tf.keras.layers.Dense(\n",
    "                sample_spec.shape.num_elements(),\n",
    "                activation=activation_fn,\n",
    "                kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n",
    "                  scale=init_means_output_factor),\n",
    "                bias_initializer=tf.keras.initializers.Constant(\n",
    "                  value=std_bias_initializer_value),\n",
    "                name='stddev_projection_layer')\n",
    "        else:\n",
    "            self._bias = bias_layer.BiasLayer(\n",
    "                bias_initializer=tf.keras.initializers.Constant(\n",
    "                    value=std_bias_initializer_value))\n",
    "\n",
    "    def _output_distribution_spec(self, sample_spec, network_name):\n",
    "        input_param_shapes = tfp.distributions.Normal.param_static_shapes(\n",
    "            sample_spec.shape)\n",
    "\n",
    "        input_param_spec = {\n",
    "            name: tensor_spec.TensorSpec(  # pylint: disable=g-complex-comprehension\n",
    "                shape=shape,\n",
    "                dtype=sample_spec.dtype,\n",
    "                name=network_name + '_' + name)\n",
    "            for name, shape in input_param_shapes.items()\n",
    "        }\n",
    "\n",
    "        def distribution_builder(*args, **kwargs):\n",
    "            distribution = tfp.distributions.Normal(*args, **kwargs)\n",
    "            if self._scale_distribution:\n",
    "                return distribution_utils.scale_distribution_to_spec(\n",
    "                    distribution, sample_spec)\n",
    "            return distribution\n",
    "\n",
    "        return distribution_spec.DistributionSpec(\n",
    "            distribution_builder, input_param_spec, sample_spec=sample_spec)\n",
    "\n",
    "    def call(self, inputs, outer_rank, training=False):\n",
    "        if inputs.dtype != self._sample_spec.dtype:\n",
    "            raise ValueError(\n",
    "            'Inputs to NormalProjectionNetwork must match the sample_spec.dtype.')\n",
    "        # outer_rank is needed because the projection is not done on the raw\n",
    "        # observations so getting the outer rank is hard as there is no spec to\n",
    "        # compare to.\n",
    "        batch_squash = network_utils.BatchSquash(outer_rank)\n",
    "        inputs = batch_squash.flatten(inputs)\n",
    "\n",
    "        means = self._means_projection_layer(inputs, training=training)\n",
    "        means = tf.reshape(means, [-1] + self._sample_spec.shape.as_list())\n",
    "\n",
    "        # If scaling the distribution later, use a normalized mean.\n",
    "        if not self._scale_distribution and self._mean_transform is not None:\n",
    "            means = self._mean_transform(means, self._sample_spec)\n",
    "        means = tf.cast(means, self._sample_spec.dtype)\n",
    "\n",
    "        if self._state_dependent_std:\n",
    "            stds = self._stddev_projection_layer(inputs, training=training)\n",
    "        else:\n",
    "            stds = self._bias(tf.zeros_like(means), training=training)\n",
    "            stds = tf.reshape(stds, [-1] + self._sample_spec.shape.as_list())\n",
    "\n",
    "        if self._std_transform is not None:\n",
    "            stds = self._std_transform(stds)\n",
    "        stds = tf.cast(stds, self._sample_spec.dtype)\n",
    "\n",
    "        means = batch_squash.unflatten(means)\n",
    "        stds = batch_squash.unflatten(stds)\n",
    "\n",
    "        return self.output_spec.build_distribution(loc=means, scale=stds)\n",
    "\n",
    "\n",
    "def _categorical_projection_net(action_spec, logits_init_output_factor=0.1):\n",
    "    return categorical_projection_network.CategoricalProjectionNetwork(\n",
    "        action_spec, logits_init_output_factor=logits_init_output_factor)\n",
    "\n",
    "\n",
    "def _normal_projection_net(action_spec,init_means_output_factor=0.1):\n",
    "    return NormalProjectionNetwork(\n",
    "        action_spec,\n",
    "        mean_transform=None,\n",
    "        state_dependent_std=True,\n",
    "        init_means_output_factor=init_means_output_factor,\n",
    "        std_transform=sac_agent.std_clip_transform,\n",
    "        scale_distribution=True)\n",
    "\n",
    "\n",
    "class ActorDistributionNetwork(network.DistributionNetwork):\n",
    "    def __init__(self,\n",
    "        input_tensor_spec,\n",
    "        output_tensor_spec,\n",
    "        preprocessing_layers=None,\n",
    "        preprocessing_combiner=None,\n",
    "        conv_layer_params=None,\n",
    "        fc_layer_params=(200, 100),\n",
    "        dropout_layer_params=None,\n",
    "        activation_fn=tf.keras.activations.relu,\n",
    "        batch_squash=True,\n",
    "        dtype=tf.float32,\n",
    "        discrete_projection_net=_categorical_projection_net,\n",
    "        continuous_projection_net=_normal_projection_net,\n",
    "        name='ActorDistributionNetwork'):\n",
    "    \n",
    "        kernel_initializer = tf.compat.v1.keras.initializers.glorot_uniform()\n",
    "\n",
    "        encoder = encoding_network.EncodingNetwork(\n",
    "            input_tensor_spec,\n",
    "            preprocessing_layers=preprocessing_layers,\n",
    "            preprocessing_combiner=preprocessing_combiner,\n",
    "            conv_layer_params=conv_layer_params,\n",
    "            fc_layer_params=fc_layer_params,\n",
    "            dropout_layer_params=dropout_layer_params,\n",
    "            activation_fn=activation_fn,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            batch_squash=batch_squash,\n",
    "            dtype=dtype)\n",
    "\n",
    "        def map_proj(spec):\n",
    "            if tensor_spec.is_discrete(spec):\n",
    "                return discrete_projection_net(spec)\n",
    "            else:\n",
    "                return continuous_projection_net(spec)\n",
    "\n",
    "        projection_networks = tf.nest.map_structure(map_proj, output_tensor_spec)\n",
    "        output_spec = tf.nest.map_structure(lambda proj_net: proj_net.output_spec,\n",
    "                                            projection_networks)\n",
    "\n",
    "        super(ActorDistributionNetwork, self).__init__(\n",
    "            input_tensor_spec=input_tensor_spec,\n",
    "            state_spec=(),\n",
    "            output_spec=output_spec,\n",
    "            name=name)\n",
    "\n",
    "        self._encoder = encoder\n",
    "        self._projection_networks = projection_networks\n",
    "        self._output_tensor_spec = output_tensor_spec\n",
    "\n",
    "    @property\n",
    "    def output_tensor_spec(self):\n",
    "        return self._output_tensor_spec\n",
    "\n",
    "    def call(self, observations, step_type, network_state, training=False):\n",
    "        state, network_state = self._encoder(\n",
    "            observations,\n",
    "            step_type=step_type,\n",
    "            network_state=network_state,\n",
    "            training=training)\n",
    "        outer_rank = nest_utils.get_outer_rank(observations, self.input_tensor_spec)\n",
    "        output_actions = tf.nest.map_structure(\n",
    "            lambda proj_net: proj_net(state, outer_rank), self._projection_networks)\n",
    "        return output_actions, network_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Normal\n"
     ]
    }
   ],
   "source": [
    "# Actor\n",
    "actor_net = ActorDistributionNetwork(\n",
    "    observation_spec,\n",
    "    action_spec,\n",
    "    fc_layer_params=actor_fc_layer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jwatts/TensorflowProjects/lib/python3.7/site-packages/tf_agents/distributions/utils.py:92: AffineScalar.__init__ (from tensorflow_probability.python.bijectors.affine_scalar) is deprecated and will be removed after 2020-01-01.\n",
      "Instructions for updating:\n",
      "`AffineScalar` bijector is deprecated; please use `tfb.Shift(loc)(tfb.Scale(...))` instead.\n",
      "PolicyStep(action=<tfp.distributions.SquashToSpecNormal 'ActorDistributionNetwork_NormalProjectionNetwork_SquashToSpecNormal' batch_shape=? event_shape=? dtype=float32>, state=(), info=())\n",
      "tfp.distributions.SquashToSpecNormal(\"ActorDistributionNetwork_NormalProjectionNetwork_SquashToSpecNormal\", dtype=float32)\n",
      "tf.Tensor([[1.2909049]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "train_policy = actor_policy.ActorPolicy(\n",
    "    time_step_spec=train_env.time_step_spec(),\n",
    "    action_spec=train_env.action_spec(),\n",
    "    actor_network=actor_net,\n",
    "    training=True)\n",
    "\n",
    "time_step = train_env.reset()\n",
    "policy_step = train_policy.distribution(time_step)\n",
    "print(policy_step)\n",
    "action = policy_step.action\n",
    "print(action)\n",
    "sample = action.sample()\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "agent = sac_agent.SacAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    action_spec,\n",
    "    actor_network=actor_net,\n",
    "    critic_network=critic_net,\n",
    "    actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=actor_learning_rate),\n",
    "    critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=critic_learning_rate),\n",
    "    alpha_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=alpha_learning_rate),\n",
    "    target_update_tau=target_update_tau,\n",
    "    target_update_period=target_update_period,\n",
    "    td_errors_loss_fn=tf.compat.v1.losses.mean_squared_error,\n",
    "    gamma=gamma,\n",
    "    reward_scale_factor=reward_scale_factor,\n",
    "    gradient_clipping=gradient_clipping,\n",
    "    train_step_counter=global_step)\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Weights\n",
    "def load_model_weights():\n",
    "    agent._actor_network.load_weights(\"./{}/actor/saved_actor\".format(env_name))\n",
    "    agent._critic_network_1.load_weights(\"./{}/critic1/saved_critic\".format(env_name))\n",
    "    agent._critic_network_2.load_weights(\"./{}/critic2/saved_critic\".format(env_name))\n",
    "\n",
    "if (load_weights == True):   \n",
    "    print(\"Loaded Weights\")\n",
    "    load_model_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the replay buffer for training\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "\n",
    "# Collect some random samples to start.\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())\n",
    "\n",
    "dynamic_step_driver.DynamicStepDriver(\n",
    "    train_env, \n",
    "    random_policy,\n",
    "    observers=[replay_buffer.add_batch],\n",
    "    num_steps=initial_collect_steps).run()\n",
    "\n",
    "# Create collection driver\n",
    "num_episodes = tf_metrics.NumberOfEpisodes()\n",
    "env_steps = tf_metrics.EnvironmentSteps()\n",
    "observers = [num_episodes, env_steps, replay_buffer.add_batch]\n",
    "\n",
    "collect_op = dynamic_step_driver.DynamicStepDriver(\n",
    "    train_env, \n",
    "    agent.collect_policy,\n",
    "    observers=observers,\n",
    "    num_steps=collect_steps_per_iteration)\n",
    "\n",
    "# Create a data set for the training loop\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_return=-1307.593505859375; max_return=-1307.593505859375\n",
      "step = 200: loss = 67.79924774169922\n",
      "step = 400: loss = 115.33133697509766\n",
      "step = 600: loss = 177.2347412109375\n",
      "step = 800: loss = 225.99684143066406\n",
      "step = 1000: loss = 270.737548828125\n",
      "step = 1200: loss = 293.6075744628906\n",
      "step = 1400: loss = 352.45703125\n",
      "step = 1600: loss = 378.1627197265625\n",
      "step = 1800: loss = 370.14508056640625\n",
      "step = 2000: loss = 399.9150695800781\n",
      "step = 2000: episodes=10: Average Return = -1172.70654296875\n",
      "Save Weights: avg_return=-1172.70654296875; max_return=-1307.593505859375\n",
      "step = 2200: loss = 395.9053649902344\n",
      "step = 2400: loss = 423.9303283691406\n",
      "step = 2600: loss = 419.1785583496094\n",
      "step = 2800: loss = 412.9429931640625\n",
      "step = 3000: loss = 431.1529235839844\n",
      "step = 3200: loss = 424.0841064453125\n",
      "step = 3400: loss = 435.0840148925781\n",
      "step = 3600: loss = 443.4576416015625\n",
      "step = 3800: loss = 434.5008239746094\n",
      "step = 4000: loss = 492.8074645996094\n",
      "step = 4000: episodes=20: Average Return = -212.83645629882812\n",
      "Save Weights: avg_return=-212.83645629882812; max_return=-1172.70654296875\n",
      "step = 4200: loss = 508.6068420410156\n",
      "step = 4400: loss = 441.55340576171875\n",
      "step = 4600: loss = 426.0877990722656\n",
      "step = 4800: loss = 518.458251953125\n",
      "step = 5000: loss = 443.8321533203125\n",
      "step = 5200: loss = 465.6999816894531\n",
      "step = 5400: loss = 437.5052490234375\n",
      "step = 5600: loss = 393.67724609375\n",
      "step = 5800: loss = 496.4437561035156\n",
      "step = 6000: loss = 576.7120361328125\n",
      "step = 6000: episodes=30: Average Return = -322.2911682128906\n",
      "step = 6200: loss = 378.5115966796875\n",
      "step = 6400: loss = 371.29754638671875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-671a134423a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Collect a few steps using collect_policy and save to the replay buffer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mcollect_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# Sample a batch of data from the buffer and update the agent's network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tf_agents/drivers/dynamic_step_driver.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, time_step, policy_state, maximum_iterations)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mtime_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mpolicy_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         maximum_iterations=maximum_iterations)\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[0;31m# TODO(b/113529538): Add tests for policy_state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tf_agents/utils/common.py\u001b[0m in \u001b[0;36mwith_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# autodep-like behavior is already expected of fn.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresource_variables_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMISSING_RESOURCE_VARIABLES_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tf_agents/drivers/dynamic_step_driver.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, time_step, policy_state, maximum_iterations)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mparallel_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         name='driver_loop')\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop_v2\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, maximum_iterations, name)\u001b[0m\n\u001b[1;32m   2476\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m       \u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2478\u001b[0;31m       return_same_structure=True)\n\u001b[0m\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   2712\u001b[0m                                               list(loop_vars))\n\u001b[1;32m   2713\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2714\u001b[0;31m         \u001b[0mloop_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2715\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basetuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2716\u001b[0m           \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tf_agents/drivers/dynamic_step_driver.py\u001b[0m in \u001b[0;36mloop_body\u001b[0;34m(counter, time_step, policy_state)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mloop_vars\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnext\u001b[0m \u001b[0miteration\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhile_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m       \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m       \u001b[0maction_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m       \u001b[0mnext_time_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36maction\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_automatic_state_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_reset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclip_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tf_agents/utils/common.py\u001b[0m in \u001b[0;36mwith_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# autodep-like behavior is already expected of fn.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresource_variables_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMISSING_RESOURCE_VARIABLES_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36m_action\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    466\u001b[0m     \"\"\"\n\u001b[1;32m    467\u001b[0m     \u001b[0mseed_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeedStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msalt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ppo_policy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m     \u001b[0mdistribution_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m     actions = tf.nest.map_structure(\n\u001b[1;32m    470\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreparameterized_sampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tf_agents/policies/actor_policy.py\u001b[0m in \u001b[0;36m_distribution\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m# Actor network outputs nested structure of distributions or actions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     actions_or_distributions, policy_state = self._apply_actor_network(\n\u001b[0;32m--> 101\u001b[0;31m         time_step, policy_state)\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_to_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_or_distribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tf_agents/policies/actor_policy.py\u001b[0m in \u001b[0;36m_apply_actor_network\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observation_normalizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     return self._actor_network(\n\u001b[0;32m---> 89\u001b[0;31m         observation, time_step.step_type, policy_state, training=self._training)\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tf_agents/networks/network.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_tensor_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_check_trainable_weights_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-671d500014e9>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, observations, step_type, network_state, training)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mnetwork_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetwork_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             training=training)\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mouter_rank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_outer_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_tensor_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         output_actions = tf.nest.map_structure(\n\u001b[1;32m    207\u001b[0m             lambda proj_net: proj_net(state, outer_rank), self._projection_networks)\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tf_agents/utils/nest_utils.py\u001b[0m in \u001b[0;36mget_outer_rank\u001b[0;34m(tensors, specs)\u001b[0m\n\u001b[1;32m    432\u001b[0m   is_unbatched = [\n\u001b[1;32m    433\u001b[0m       \u001b[0mspec_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mspec_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_shape\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m   ]\n\u001b[1;32m    436\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_unbatched\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tf_agents/utils/nest_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    432\u001b[0m   is_unbatched = [\n\u001b[1;32m    433\u001b[0m       \u001b[0mspec_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mspec_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_shape\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m   ]\n\u001b[1;32m    436\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_unbatched\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mis_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \"\"\"\n\u001b[0;32m-> 1093\u001b[0;31m     \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TensorflowProjects/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mas_shape\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m   \u001b[0;34m\"\"\"Converts the given object to a TensorShape.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensorShape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def measure_distribution(timestep, policy, num_samples = 1000):\n",
    "    samples = np.zeros(num_samples)\n",
    "    for i in range (num_samples):\n",
    "        samples[i] = policy.action(timestep).action.numpy()\n",
    "    return np.mean(samples), np.std(samples)\n",
    "\n",
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "print(\"avg_return={}; max_return={}\".format(avg_return, np.amax(returns)))\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    collect_op.run()\n",
    "    \n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = agent.train(experience).loss\n",
    "\n",
    "    step = env_steps.result().numpy()\n",
    "    episodes = num_episodes.result().numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: episodes={1}: Average Return = {2}'.format(step, episodes, avg_return))\n",
    "        if (avg_return > max(returns)):\n",
    "            # Save Weights\n",
    "            print(\"Save Weights: avg_return={}; max_return={}\".format(avg_return, np.amax(returns)))\n",
    "            agent._actor_network.save_weights(\"./{}/actor/saved_actor\".format(env_name))\n",
    "            agent._critic_network_1.save_weights(\"./{}/critic1/saved_critic\".format(env_name))\n",
    "            agent._critic_network_2.save_weights(\"./{}/critic2/saved_critic\".format(env_name))\n",
    "        returns.append(avg_return)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([-128.00604], shape=(1,), dtype=float32)\n",
      "tf.Tensor([-1.6130695], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for i in range (10):\n",
    "    rewards = 0.0\n",
    "    time_step = eval_env.reset()\n",
    "    while not time_step.is_last():\n",
    "        action_step = agent.policy.action(time_step)\n",
    "        time_step = eval_env.step(action_step.action)\n",
    "        rewards += time_step.reward\n",
    "        eval_py_env.render()\n",
    "    print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
