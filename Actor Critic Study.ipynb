{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "\n",
    "from tf_agents.networks import network\n",
    "\n",
    "from tf_agents.policies import tf_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "\n",
    "from tf_agents.agents.ddpg import critic_network\n",
    "from tf_agents.agents.sac import sac_agent\n",
    "\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.trajectories import policy_step\n",
    "\n",
    "\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.utils import common\n",
    "\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.specs import distribution_spec\n",
    "from tf_agents.distributions import tanh_bijector_stable\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"Pendulum-v0\" \n",
    "#env_name = \"LunarLanderContinuous-v2\" \n",
    "\n",
    "num_iterations = 20000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 1000 # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1 # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 25000 # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 256 # @param {type:\"integer\"}\n",
    "\n",
    "critic_learning_rate = 1e-3 # @param {type:\"number\"}\n",
    "actor_learning_rate = 1e-3 # @param {type:\"number\"}\n",
    "alpha_learning_rate = 1e-3 # @param {type:\"number\"}\n",
    "target_update_tau = 0.005 # @param {type:\"number\"}\n",
    "target_update_period = 1 # @param {type:\"number\"}\n",
    "gamma = 0.99 # @param {type:\"number\"}\n",
    "reward_scale_factor = 1.0 # @param {type:\"number\"}\n",
    "gradient_clipping = None # @param\n",
    "\n",
    "actor_fc_layer_params = (256,)\n",
    "critic_joint_fc_layer_params = (256,)\n",
    "\n",
    "log_interval = 200 # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10 # @param {type:\"integer\"}\n",
    "eval_interval = 2000 # @param {type:\"integer\"}\n",
    "max_episode_steps = 1000\n",
    "\n",
    "load_weights = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "BoundedTensorSpec(shape=(3,), dtype=tf.float32, name='observation', minimum=array([-1., -1., -8.], dtype=float32), maximum=array([1., 1., 8.], dtype=float32))\n",
      "Action Spec:\n",
      "BoundedTensorSpec(shape=(1,), dtype=tf.float32, name='action', minimum=array(-2., dtype=float32), maximum=array(2., dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "\n",
    "observation_spec = train_env.observation_spec()\n",
    "action_spec = train_env.action_spec()\n",
    "\n",
    "print('Observation Spec:')\n",
    "print(observation_spec)\n",
    "print('Action Spec:')\n",
    "print(action_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic\n",
    "critic_net = critic_network.CriticNetwork(\n",
    "    (observation_spec, action_spec),\n",
    "    observation_fc_layer_params=None,\n",
    "    action_fc_layer_params=None,\n",
    "    joint_fc_layer_params=critic_joint_fc_layer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor\n",
    "#def std_clip_transform(stddevs):\n",
    "#    stddevs = tf.nest.map_structure(lambda t: tf.clip_by_value(t, -20, 2),\n",
    "#                                  stddevs)\n",
    "#    return tf.exp(stddevs)\n",
    "\n",
    "\n",
    "class ActorNormalDistributionNetwork(network.DistributionNetwork):\n",
    "    def __init__(self,\n",
    "                 input_tensor_spec,\n",
    "                 output_tensor_spec,\n",
    "                 hidden_units = 256,\n",
    "                 name=\"ActorNormalDistributionNetwork\"):\n",
    "        \n",
    "        output_spec = self._output_distribution_spec(output_tensor_spec, name) \n",
    "        \n",
    "        super(ActorNormalDistributionNetwork, self).__init__(\n",
    "            input_tensor_spec=input_tensor_spec,\n",
    "            state_spec=(),\n",
    "            output_spec=output_spec,\n",
    "            name=name)\n",
    "\n",
    "        num_actions = output_tensor_spec.shape.num_elements()\n",
    "          \n",
    "        # hidden layer to encode\n",
    "        self._hidden_layer = tf.keras.layers.Dense(\n",
    "            hidden_units,\n",
    "            activation=tf.keras.activations.relu,\n",
    "            kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform(),\n",
    "            input_shape=input_tensor_spec.shape)\n",
    "        \n",
    "        # means layer for distribution\n",
    "        init_means_output_factor = 0.1\n",
    "        std_bias_initializer_value = 0.0\n",
    "        \n",
    "        self._means_projection_layer = tf.keras.layers.Dense(\n",
    "            num_actions,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n",
    "                scale=init_means_output_factor),\n",
    "            bias_initializer=tf.keras.initializers.Zeros(),\n",
    "            name='means_projection_layer')\n",
    "\n",
    "        # standard dev layer for distribution\n",
    "        self._stddev_projection_layer = tf.keras.layers.Dense(\n",
    "            num_actions,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n",
    "                scale=init_means_output_factor),\n",
    "            bias_initializer=tf.keras.initializers.Constant(\n",
    "                value=std_bias_initializer_value),\n",
    "            name='stddev_projection_layer')\n",
    "        \n",
    "        # Scale\n",
    "        action_means, action_magnitudes = common.spec_means_and_magnitudes(output_tensor_spec)\n",
    "        bijectors = [tfp.bijectors.Shift(action_means),\n",
    "                     tfp.bijectors.Scale(action_magnitudes),\n",
    "                     tanh_bijector_stable.Tanh()]\n",
    "        self._bijector_chain = tfp.bijectors.Chain(bijectors)\n",
    "        \n",
    "        \n",
    "    def _output_distribution_spec(self, sample_spec, network_name):\n",
    "        input_param_shapes = tfp.distributions.Normal.param_static_shapes(\n",
    "            sample_spec.shape)\n",
    "\n",
    "        input_param_spec = {\n",
    "            name: tensor_spec.TensorSpec(  \n",
    "                shape=shape,\n",
    "                dtype=sample_spec.dtype,\n",
    "                name=network_name + '_' + name)\n",
    "            for name, shape in input_param_shapes.items()\n",
    "        }\n",
    "\n",
    "        def distribution_builder(*args, **kwargs):            \n",
    "            distribution = tfp.distributions.Normal(*args, **kwargs)\n",
    "            return tfp.distributions.TransformedDistribution(distribution=distribution, bijector=self._bijector_chain)\n",
    "\n",
    "        return distribution_spec.DistributionSpec(distribution_builder, input_param_spec, sample_spec=sample_spec)\n",
    "\n",
    "    \n",
    "    def call(self, observations, step_type, network_state, training=False):\n",
    "        state = self._hidden_layer(observations, training=training)\n",
    "        \n",
    "        means = self._means_projection_layer(state, training=training)\n",
    "\n",
    "        stds = self._stddev_projection_layer(state, training=training)\n",
    "        stds = tf.clip_by_value(stds, -20, 2)\n",
    "        stds = tf.exp(stds)\n",
    "        \n",
    "        return self.output_spec.builder(loc=means, scale=stds), network_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyActorPolicy(tf_policy.Base):\n",
    "    def __init__(self,\n",
    "        time_step_spec,\n",
    "        action_spec,\n",
    "        actor_network,\n",
    "        training=False):\n",
    "\n",
    "        actor_network.create_variables()\n",
    "        self._actor_network = actor_network\n",
    "        self._training = training\n",
    "\n",
    "        super(MyActorPolicy, self).__init__(\n",
    "            time_step_spec=time_step_spec,\n",
    "            action_spec=action_spec,\n",
    "            policy_state_spec=actor_network.state_spec)\n",
    "\n",
    "    def _variables(self):\n",
    "        return self._actor_network.variables\n",
    "\n",
    "    def _distribution(self, time_step, policy_state):\n",
    "        distributions, policy_state = self._actor_network(time_step.observation, \n",
    "                                                          time_step.step_type, \n",
    "                                                          policy_state, \n",
    "                                                          training=self._training)\n",
    "\n",
    "        return policy_step.PolicyStep(distributions, policy_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_net = ActorNormalDistributionNetwork(observation_spec,\n",
    "                                               action_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "agent = sac_agent.SacAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    action_spec,\n",
    "    actor_network=actor_net,\n",
    "    critic_network=critic_net,\n",
    "    actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=actor_learning_rate),\n",
    "    critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=critic_learning_rate),\n",
    "    alpha_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=alpha_learning_rate),\n",
    "    actor_policy_ctor=MyActorPolicy,\n",
    "    target_update_tau=target_update_tau,\n",
    "    target_update_period=target_update_period,\n",
    "    td_errors_loss_fn=tf.compat.v1.losses.mean_squared_error,\n",
    "    gamma=gamma,\n",
    "    reward_scale_factor=reward_scale_factor,\n",
    "    gradient_clipping=gradient_clipping,\n",
    "    train_step_counter=global_step)\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Weights\n",
    "def load_model_weights():\n",
    "    agent._actor_network.load_weights(\"./{}/actor/saved_actor\".format(env_name))\n",
    "    agent._critic_network_1.load_weights(\"./{}/critic1/saved_critic\".format(env_name))\n",
    "    agent._critic_network_2.load_weights(\"./{}/critic2/saved_critic\".format(env_name))\n",
    "\n",
    "if (load_weights == True):   \n",
    "    print(\"Loaded Weights\")\n",
    "    load_model_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the replay buffer for training\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "\n",
    "# Collect some random samples to start.\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())\n",
    "\n",
    "dynamic_step_driver.DynamicStepDriver(\n",
    "    train_env, \n",
    "    random_policy,\n",
    "    observers=[replay_buffer.add_batch],\n",
    "    num_steps=initial_collect_steps).run()\n",
    "\n",
    "# Create collection driver\n",
    "num_episodes = tf_metrics.NumberOfEpisodes()\n",
    "env_steps = tf_metrics.EnvironmentSteps()\n",
    "observers = [num_episodes, env_steps, replay_buffer.add_batch]\n",
    "\n",
    "collect_op = dynamic_step_driver.DynamicStepDriver(\n",
    "    train_env, \n",
    "    agent.collect_policy,\n",
    "    observers=observers,\n",
    "    num_steps=collect_steps_per_iteration)\n",
    "\n",
    "# Create a data set for the training loop\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_return=-1321.633056640625; max_return=-1321.633056640625\n",
      "step = 200: loss = 9.78829288482666\n",
      "step = 400: loss = 11.189778327941895\n",
      "step = 600: loss = 14.761147499084473\n",
      "step = 800: loss = 18.820968627929688\n",
      "step = 1000: loss = 42.47685241699219\n",
      "step = 1200: loss = 29.997093200683594\n",
      "step = 1400: loss = 39.29471206665039\n",
      "step = 1600: loss = 46.759246826171875\n",
      "step = 1800: loss = 51.243595123291016\n",
      "step = 2000: loss = 58.40377426147461\n",
      "step = 2000: episodes=10: Average Return = -1129.9898681640625\n",
      "Save Weights: avg_return=-1129.9898681640625; max_return=-1321.633056640625\n",
      "step = 2200: loss = 67.1185073852539\n",
      "step = 2400: loss = 78.48113250732422\n",
      "step = 2600: loss = 68.61000061035156\n",
      "step = 2800: loss = 97.30880737304688\n",
      "step = 3000: loss = 85.07583618164062\n",
      "step = 3200: loss = 93.72799682617188\n",
      "step = 3400: loss = 98.14154052734375\n",
      "step = 3600: loss = 145.60098266601562\n",
      "step = 3800: loss = 105.47412872314453\n",
      "step = 4000: loss = 106.58920288085938\n",
      "step = 4000: episodes=20: Average Return = -678.9273681640625\n",
      "Save Weights: avg_return=-678.9273681640625; max_return=-1129.9898681640625\n",
      "step = 4200: loss = 115.69261169433594\n",
      "step = 4400: loss = 117.32279968261719\n",
      "step = 4600: loss = 108.87042999267578\n",
      "step = 4800: loss = 103.56827545166016\n",
      "step = 5000: loss = 117.86750793457031\n",
      "step = 5200: loss = 115.70232391357422\n",
      "step = 5400: loss = 120.03543853759766\n",
      "step = 5600: loss = 120.78076934814453\n",
      "step = 5800: loss = 125.04781341552734\n",
      "step = 6000: loss = 260.63800048828125\n",
      "step = 6000: episodes=30: Average Return = -224.50558471679688\n",
      "Save Weights: avg_return=-224.50558471679688; max_return=-678.9273681640625\n",
      "step = 6200: loss = 127.07699584960938\n",
      "step = 6400: loss = 129.5388946533203\n",
      "step = 6600: loss = 276.29766845703125\n",
      "step = 6800: loss = 131.31732177734375\n",
      "step = 7000: loss = 156.39515686035156\n",
      "step = 7200: loss = 128.20619201660156\n",
      "step = 7400: loss = 168.46881103515625\n",
      "step = 7600: loss = 320.2002868652344\n",
      "step = 7800: loss = 163.90606689453125\n",
      "step = 8000: loss = 264.794677734375\n",
      "step = 8000: episodes=40: Average Return = -291.1739501953125\n",
      "step = 8200: loss = 145.41937255859375\n",
      "step = 8400: loss = 137.3736114501953\n",
      "step = 8600: loss = 142.60784912109375\n",
      "step = 8800: loss = 287.4377746582031\n",
      "step = 9000: loss = 227.23458862304688\n",
      "step = 9200: loss = 175.8796844482422\n",
      "step = 9400: loss = 144.98545837402344\n",
      "step = 9600: loss = 149.19114685058594\n",
      "step = 9800: loss = 146.0394744873047\n",
      "step = 10000: loss = 143.0353546142578\n",
      "step = 10000: episodes=50: Average Return = -230.9787139892578\n",
      "step = 10200: loss = 402.9403381347656\n",
      "step = 10400: loss = 142.87303161621094\n",
      "step = 10600: loss = 143.4945831298828\n",
      "step = 10800: loss = 533.7149047851562\n",
      "step = 11000: loss = 304.3874816894531\n",
      "step = 11200: loss = 348.9486083984375\n",
      "step = 11400: loss = 327.82867431640625\n",
      "step = 11600: loss = 167.48101806640625\n",
      "step = 11800: loss = 428.2265319824219\n",
      "step = 12000: loss = 242.13755798339844\n",
      "step = 12000: episodes=60: Average Return = -135.18394470214844\n",
      "Save Weights: avg_return=-135.18394470214844; max_return=-224.50558471679688\n",
      "step = 12200: loss = 408.324951171875\n",
      "step = 12400: loss = 206.7384796142578\n",
      "step = 12600: loss = 318.5137023925781\n",
      "step = 12800: loss = 205.17633056640625\n",
      "step = 13000: loss = 306.2950439453125\n",
      "step = 13200: loss = 274.1610107421875\n",
      "step = 13400: loss = 141.86892700195312\n",
      "step = 13600: loss = 160.74325561523438\n",
      "step = 13800: loss = 143.4983367919922\n",
      "step = 14000: loss = 246.06617736816406\n",
      "step = 14000: episodes=70: Average Return = -218.3550262451172\n",
      "step = 14200: loss = 135.2932891845703\n",
      "step = 14400: loss = 504.3815002441406\n",
      "step = 14600: loss = 614.5431518554688\n",
      "step = 14800: loss = 141.1646728515625\n",
      "step = 15000: loss = 141.09703063964844\n",
      "step = 15200: loss = 519.0517578125\n",
      "step = 15400: loss = 141.335205078125\n",
      "step = 15600: loss = 205.89390563964844\n",
      "step = 15800: loss = 227.31890869140625\n",
      "step = 16000: loss = 135.55296325683594\n",
      "step = 16000: episodes=80: Average Return = -206.4332733154297\n",
      "step = 16200: loss = 321.21209716796875\n",
      "step = 16400: loss = 269.71319580078125\n",
      "step = 16600: loss = 315.5745544433594\n",
      "step = 16800: loss = 152.31451416015625\n",
      "step = 17000: loss = 134.51724243164062\n",
      "step = 17200: loss = 216.4622344970703\n",
      "step = 17400: loss = 268.4670104980469\n",
      "step = 17600: loss = 822.293701171875\n",
      "step = 17800: loss = 585.0900268554688\n",
      "step = 18000: loss = 490.7850646972656\n",
      "step = 18000: episodes=90: Average Return = -172.89871215820312\n",
      "step = 18200: loss = 257.18896484375\n",
      "step = 18400: loss = 696.52685546875\n",
      "step = 18600: loss = 875.9050903320312\n",
      "step = 18800: loss = 338.3263244628906\n",
      "step = 19000: loss = 861.639404296875\n",
      "step = 19200: loss = 307.7531433105469\n",
      "step = 19400: loss = 373.0110778808594\n",
      "step = 19600: loss = 442.60546875\n",
      "step = 19800: loss = 132.65342712402344\n",
      "step = 20000: loss = 253.84178161621094\n",
      "step = 20000: episodes=100: Average Return = -119.40608978271484\n",
      "Save Weights: avg_return=-119.40608978271484; max_return=-135.18394470214844\n"
     ]
    }
   ],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "print(\"avg_return={}; max_return={}\".format(avg_return, np.amax(returns)))\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    collect_op.run()\n",
    "    \n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = agent.train(experience).loss\n",
    "\n",
    "    step = env_steps.result().numpy()\n",
    "    episodes = num_episodes.result().numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: episodes={1}: Average Return = {2}'.format(step, episodes, avg_return))\n",
    "        if (avg_return > max(returns)):\n",
    "            # Save Weights\n",
    "            print(\"Save Weights: avg_return={}; max_return={}\".format(avg_return, np.amax(returns)))\n",
    "            agent._actor_network.save_weights(\"./{}/actor/saved_actor\".format(env_name))\n",
    "            agent._critic_network_1.save_weights(\"./{}/critic1/saved_critic\".format(env_name))\n",
    "            agent._critic_network_2.save_weights(\"./{}/critic2/saved_critic\".format(env_name))\n",
    "        returns.append(avg_return)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([-253.99608], shape=(1,), dtype=float32)\n",
      "tf.Tensor([-0.30610645], shape=(1,), dtype=float32)\n",
      "tf.Tensor([-124.2281], shape=(1,), dtype=float32)\n",
      "tf.Tensor([-122.49755], shape=(1,), dtype=float32)\n",
      "tf.Tensor([-127.02479], shape=(1,), dtype=float32)\n",
      "tf.Tensor([-119.26954], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for i in range (10):\n",
    "    rewards = 0.0\n",
    "    time_step = eval_env.reset()\n",
    "    while not time_step.is_last():\n",
    "        action_step = agent.policy.action(time_step)\n",
    "        time_step = eval_env.step(action_step.action)\n",
    "        rewards += time_step.reward\n",
    "        eval_py_env.render()\n",
    "    print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
